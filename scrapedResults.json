[
{"funcName": "BaseEstimator", "notes": "All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).", "libName": "sklearn.base", "methods": [{"methodName": "__init__()", "methodDesc": "Initialize self.  See help(type(self)) for accurate signature."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "Base class for all estimators in scikit-learn"},
{"allReturnParams": ["data", "(data, target)", "This is a copy of the test set of the UCI ML hand-written digits datasets", "http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "load_digits", "allFuncParams": ["n_class", "return_X_y"], "funcDesc": "Load and return the digits dataset (classification).", "funcParamBody": "n_class : integer, between 0 and 10, optional (default=10) The number of classes to return. return_X_y : boolean, default=False. If True, returns (data,  target) instead of a Bunch object. See below for more information about the data and target object. New in version 0.18.", "funcReturnBody": "data : Bunch Dictionary-like object, the interesting attributes are: data, the data to learn, images, the images corresponding to each sample, target, the classification labels for each sample, target_names, the meaning of the labels, and DESCR, the full description of the dataset. (data, target) : tuple if return_X_y is True New in version 0.18. This is a copy of the test set of the UCI ML hand-written digits datasets : http://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits :"},
{"allReturnParams": ["data", "(data, target)"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "load_diabetes", "allFuncParams": ["return_X_y"], "funcDesc": "Load and return the diabetes dataset (regression).", "funcParamBody": "return_X_y : boolean, default=False. If True, returns (data,  target) instead of a Bunch object. See below for more information about the data and target object. New in version 0.18.", "funcReturnBody": "data : Bunch Dictionary-like object, the interesting attributes are: data, the data to learn and target, the regression target for each sample. (data, target) : tuple if return_X_y is True New in version 0.18."},
{"allReturnParams": ["data", "(data, target)", "The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is", "downloaded from:", "https://goo.gl/U2Uwz2"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "load_breast_cancer", "allFuncParams": ["return_X_y"], "funcDesc": "Load and return the breast cancer wisconsin dataset (classification).", "funcParamBody": "return_X_y : boolean, default=False If True, returns (data,  target) instead of a Bunch object. See below for more information about the data and target object. New in version 0.18.", "funcReturnBody": "data : Bunch Dictionary-like object, the interesting attributes are: data, the data to learn, target, the classification labels, target_names, the meaning of the labels, feature_names, the meaning of the features, and DESCR, the full description of the dataset. (data, target) : tuple if return_X_y is True New in version 0.18. The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is : downloaded from: : https://goo.gl/U2Uwz2 :"},
{"allReturnParams": ["data", "(data, target)"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "load_boston", "allFuncParams": ["return_X_y"], "funcDesc": "Load and return the boston house-prices dataset (regression).", "funcParamBody": "return_X_y : boolean, default=False. If True, returns (data,  target) instead of a Bunch object. See below for more information about the data and target object. New in version 0.18.", "funcReturnBody": "data : Bunch Dictionary-like object, the interesting attributes are: data, the data to learn, target, the regression targets, and DESCR, the full description of the dataset. (data, target) : tuple if return_X_y is True New in version 0.18."},
{"libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "get_data_home", "allFuncParams": ["data_home"], "funcDesc": "Return the path of the scikit-learn data dir.", "funcParamBody": "data_home : str | None The path to scikit-learn data dir."},
{"allReturnParams": ["The data is returned as a Bunch object with the following attributes:", "coverages", "train", "test", "Nx, Ny", "x_left_lower_corner, y_left_lower_corner", "grid_size"], "libName": "sklearn.datasets", "methods": [], "notes": "This dataset represents the geographic distribution of species. The dataset is provided by Phillips et. al. (2006). The two species are: This dataset represents the geographic distribution of species. The dataset is provided by Phillips et. al. (2006). The two species are:", "funcName": "fetch_species_distributions", "allFuncParams": ["data_home", "download_if_missing"], "funcDesc": "Loader for species distribution dataset from Phillips et. al. (2006)", "funcParamBody": "data_home : optional, default: None Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in ~/scikit_learn_data subfolders. download_if_missing : optional, True by default If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.", "funcReturnBody": "The data is returned as a Bunch object with the following attributes: : coverages : array, shape = [14, 1592, 1212] These represent the 14 features measured at each point of the map grid. The latitude/longitude values for the grid are discussed below. Missing data is represented by the value -9999. train : record array, shape = (1623,) The training points for the data.  Each point has three fields: train[species] is the species name train[dd long] is the longitude, in degrees train[dd lat] is the latitude, in degrees test : record array, shape = (619,) The test points for the data.  Same format as the training data. Nx, Ny : integers The number of longitudes (x) and latitudes (y) in the grid x_left_lower_corner, y_left_lower_corner : floats The (x,y) position of the lower-left corner, in degrees grid_size : float The spacing between points of the grid, in degrees"},
{"allReturnParams": ["dataset", "dataset.data", "dataset.target", "dataset.sample_id", "dataset.target_names", "dataset.DESCR"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "fetch_rcv1", "allFuncParams": ["data_home", "subset", "download_if_missing", "random_state", "shuffle"], "funcDesc": "Load the RCV1 multilabel dataset, downloading it if necessary.", "funcParamBody": "data_home : string, optional Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in ~/scikit_learn_data subfolders. subset : string, train, test, or all, default=all Select the dataset to load: train for the training set (23149 samples), test for the test set (781265 samples), all for both, with the training samples first if shuffle is False. This follows the official LYRL2004 chronological split. download_if_missing : boolean, default=True If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site. random_state : int, RandomState instance or None, optional (default=None) Random state for shuffling the dataset. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . shuffle : bool, default=False Whether to shuffle dataset.", "funcReturnBody": "dataset : dict-like object with the following attributes: dataset.data : scipy csr array, dtype np.float64, shape (804414, 47236) The array has 0.16% of non zero values. dataset.target : scipy csr array, dtype np.uint8, shape (804414, 103) Each sample has a value of 1 in its categories, and 0 in others. The array has 3.15% of non zero values. dataset.sample_id : numpy array, dtype np.uint32, shape (804414,) Identification number of each sample, as ordered in dataset.data. dataset.target_names : numpy array, dtype object, length (103) Names of each target (RCV1 topics), as ordered in dataset.target. dataset.DESCR : string Description of the RCV1 dataset."},
{"allReturnParams": ["An object with the following attributes:", "data", "images", "target", "DESCR"], "libName": "sklearn.datasets", "methods": [], "notes": "This dataset consists of 10 pictures each of 40 individuals. The original database was available from (now defunct) The version retrieved here comes in MATLAB format from the personal web page of Sam Roweis: This dataset consists of 10 pictures each of 40 individuals. The original database was available from (now defunct) The version retrieved here comes in MATLAB format from the personal web page of Sam Roweis:", "funcName": "fetch_olivetti_faces", "allFuncParams": ["data_home", "shuffle", "random_state", "download_if_missing"], "funcDesc": "Loader for the Olivetti faces data-set from AT&T.", "funcParamBody": "data_home : optional, default: None Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in ~/scikit_learn_data subfolders. shuffle : boolean, optional If True the order of the dataset is shuffled to avoid having images of the same person grouped. random_state : int, RandomState instance or None, optional (default=0) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . download_if_missing : optional, True by default If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.", "funcReturnBody": "An object with the following attributes: : data : numpy array of shape (400, 4096) Each row corresponds to a ravelled face image of original size 64 x 64 pixels. images : numpy array of shape (400, 64, 64) Each row is a face image corresponding to one of the 40 subjects of the dataset. target : numpy array of shape (400, ) Labels associated to each face image. Those labels are ranging from 0-39 and correspond to the Subject IDs. DESCR : string Description of the modified Olivetti Faces Dataset."},
{"allReturnParams": ["data"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "fetch_mldata", "allFuncParams": ["dataname", "target_name", "data_name", "transpose_data", "data_home"], "funcDesc": "Fetch an mldata.org data set", "funcParamBody": "dataname : str Name of the data set on mldata.org, e.g.: leukemia, Whistler Daily Snowfall, etc. The raw name is automatically converted to a mldata.org URL . target_name : optional, default: label Name or index of the column containing the target values. data_name : optional, default: data Name or index of the column containing the data. transpose_data : optional, default: True If True, transpose the downloaded data array. data_home : optional, default: None Specify another download and cache folder for the data sets. By default all scikit-learn data is stored in ~/scikit_learn_data subfolders.", "funcReturnBody": "data : Bunch Dictionary-like object, the interesting attributes are: data, the data to learn, target, the classification labels, DESCR, the full description of the dataset, and COL_NAMES, the original names of the dataset columns."},
{"allReturnParams": ["dataset", "dataset.data", "dataset.images", "dataset.target", "dataset.DESCR"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "fetch_lfw_people", "allFuncParams": ["data_home", "funneled", "resize", "min_faces_per_person", "color", "slice_", "download_if_missing"], "funcDesc": "Loader for the Labeled Faces in the Wild (LFW) people dataset", "funcParamBody": "data_home : optional, default: None Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in ~/scikit_learn_data subfolders. funneled : boolean, optional, default: True Download and use the funneled variant of the dataset. resize : float, optional, default 0.5 Ratio used to resize the each face picture. min_faces_per_person : int, optional, default None The extracted dataset will only retain pictures of people that have at least min_faces_per_person different pictures. color : boolean, optional, default False Keep the 3 RGB channels instead of averaging them to a single gray level channel. If color is True the shape of the data has one more dimension than the shape with color = False. slice_ : optional Provide a custom 2D slice (height, width) to extract the interesting part of the jpeg files and avoid use statistical correlation from the background download_if_missing : optional, True by default If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.", "funcReturnBody": "dataset : dict-like object with the following attributes: dataset.data : numpy array of shape (13233, 2914) Each row corresponds to a ravelled face image of original size 62 x 47 pixels. Changing the slice_ or resize parameters will change the shape of the output. dataset.images : numpy array of shape (13233, 62, 47) Each row is a face image corresponding to one of the 5749 people in the dataset. Changing the slice_ or resize parameters will change the shape of the output. dataset.target : numpy array of shape (13233,) Labels associated to each face image. Those labels range from 0-5748 and correspond to the person IDs. dataset.DESCR : string Description of the Labeled Faces in the Wild (LFW) dataset."},
{"allReturnParams": ["The data is returned as a Bunch object with the following attributes:", "data", "pairs", "target", "DESCR"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "fetch_lfw_pairs", "allFuncParams": ["subset", "data_home", "funneled", "resize", "color", "slice_", "download_if_missing"], "funcDesc": "Loader for the Labeled Faces in the Wild (LFW) pairs dataset", "funcParamBody": "subset : optional, default: train Select the dataset to load: train for the development training set, test for the development test set, and 10_folds for the official evaluation set that is meant to be used with a 10-folds cross validation. data_home : optional, default: None Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in ~/scikit_learn_data subfolders. funneled : boolean, optional, default: True Download and use the funneled variant of the dataset. resize : float, optional, default 0.5 Ratio used to resize the each face picture. color : boolean, optional, default False Keep the 3 RGB channels instead of averaging them to a single gray level channel. If color is True the shape of the data has one more dimension than the shape with color = False. slice_ : optional Provide a custom 2D slice (height, width) to extract the interesting part of the jpeg files and avoid use statistical correlation from the background download_if_missing : optional, True by default If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.", "funcReturnBody": "The data is returned as a Bunch object with the following attributes: : data : numpy array of shape (2200, 5828). Shape depends on subset . Each row corresponds to 2 raveld face images of original size 62 x 47 pixels. Changing the slice_ , resize or subset parameters will change the shape of the output. pairs : numpy array of shape (2200, 2, 62, 47). Shape depends on subset . Each row has 2 face images corresponding to same or different person from the dataset containing 5749 people. Changing the slice_ , resize or subset parameters will change the shape of the output. target : numpy array of shape (2200,). Shape depends on subset . Labels associated to each pair of images. The two label values being different persons or the same person. DESCR : string Description of the Labeled Faces in the Wild (LFW) dataset."},
{"allReturnParams": ["data"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "fetch_kddcup99", "allFuncParams": ["subset", "data_home", "shuffle", "random_state", "percent10", "download_if_missing"], "funcDesc": "Load and return the kddcup 99 dataset (classification).", "funcParamBody": "subset : None, SA, SF, http, smtp To return the corresponding classical subsets of kddcup 99. If None, return the entire kddcup 99 dataset. data_home : string, optional Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in ~/scikit_learn_data subfolders. .. versionadded:: 0.19 shuffle : bool, default=False Whether to shuffle dataset. random_state : int, RandomState instance or None, optional (default=None) Random state for shuffling the dataset. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . percent10 : bool, default=True Whether to load only 10 percent of the data. download_if_missing : bool, default=True If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.", "funcReturnBody": "data : Bunch Dictionary-like object, the interesting attributes are: data, the data to learn and target, the regression target for each sample."},
{"allReturnParams": ["dataset", "dataset.data", "dataset.target", "dataset.DESCR"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "fetch_covtype", "allFuncParams": ["data_home", "download_if_missing", "random_state", "shuffle"], "funcDesc": "Load the covertype dataset, downloading it if necessary.", "funcParamBody": "data_home : string, optional Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in ~/scikit_learn_data subfolders. download_if_missing : boolean, default=True If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site. random_state : int, RandomState instance or None, optional (default=None) Random state for shuffling the dataset. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . shuffle : bool, default=False Whether to shuffle dataset.", "funcReturnBody": "dataset : dict-like object with the following attributes: dataset.data : numpy array of shape (581012, 54) Each row corresponds to the 54 features in the dataset. dataset.target : numpy array of shape (581012,) Each value corresponds to one of the 7 forest covertypes with values ranging between 1 to 7. dataset.DESCR : string Description of the forest covertype dataset."},
{"allReturnParams": ["dataset", "dataset.data", "dataset.target", "dataset.feature_names", "dataset.DESCR"], "libName": "sklearn.datasets", "methods": [], "notes": "This dataset consists of 20,640 samples and 9 features. This dataset consists of 20,640 samples and 9 features.", "funcName": "fetch_california_housing", "allFuncParams": ["data_home", "download_if_missing"], "funcDesc": "Loader for the California housing dataset from StatLib.", "funcParamBody": "data_home : optional, default: None Specify another download and cache folder for the datasets. By default all scikit-learn data is stored in ~/scikit_learn_data subfolders. download_if_missing : optional, True by default If False, raise a IOError if the data is not locally available instead of trying to download the data from the source site.", "funcReturnBody": "dataset : dict-like object with the following attributes: dataset.data : ndarray, shape [20640, 8] Each row corresponding to the 8 feature values in order. dataset.target : numpy array of shape (20640,) Each value corresponds to the average house value in units of 100,000. dataset.feature_names : array of length 8 Array of ordered feature names used in the dataset. dataset.DESCR : string Description of the California housing dataset."},
{"allReturnParams": ["bunch"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "fetch_20newsgroups_vectorized", "allFuncParams": ["subset", "remove", "data_home", "download_if_missing"], "funcDesc": "Load the 20 newsgroups dataset and transform it into tf-idf vectors.", "funcParamBody": "subset : train or test, all, optional Select the dataset to load: train for the training set, test for the test set, all for both, with shuffled ordering. remove : tuple May contain any subset of (headers, footers, quotes). Each of these are kinds of text that will be detected and removed from the newsgroup posts, preventing classifiers from overfitting on metadata. headers removes newsgroup headers, footers removes blocks at the ends of posts that look like signatures, and quotes removes lines that appear to be quoting another post. data_home : optional, default: None Specify an download and cache folder for the datasets. If None, all scikit-learn data is stored in ~/scikit_learn_data subfolders. download_if_missing : optional, True by default If False, raise an IOError if the data is not locally available instead of trying to download the data from the source site.", "funcReturnBody": "bunch : Bunch object bunch.data: sparse matrix, shape [n_samples, n_features] bunch.target: array, shape [n_samples] bunch.target_names: list, length [n_classes]"},
{"allReturnParams": ["train_scores", "test_scores"], "libName": "sklearn.learning_curve", "methods": [], "notes": "See examples/model_selection/plot_validation_curve.py", "funcName": "validation_curve", "allFuncParams": ["estimator", "X", "y", "param_name", "param_range", "cv", "scoring", "n_jobs", "pre_dispatch", "verbose"], "funcDesc": "Validation curve.", "funcParamBody": "estimator : object type that implements the fit and predict methods An object of that type which is cloned for each validation. X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape (n_samples) or (n_samples, n_features), optional Target relative to X for classification or regression; None for unsupervised learning. param_name : string Name of the parameter that will be varied. param_range : array-like, shape (n_values,) The values of the parameter that will be evaluated. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, sklearn.model_selection.StratifiedKFold is used. In all other cases, sklearn.model_selection.KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. scoring : string, callable or None, optional, default: None A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator,  X,  y) . n_jobs : integer, optional Number of jobs to run in parallel (default 1). pre_dispatch : integer or string, optional Number of predispatched jobs for parallel execution (default is all). The option can reduce the allocated memory. The string can be an expression like 2*n_jobs. verbose : integer, optional Controls the verbosity: the higher, the more messages.", "funcReturnBody": "train_scores : array, shape (n_ticks, n_cv_folds) Scores on training sets. test_scores : array, shape (n_ticks, n_cv_folds) Scores on test set."},
{"allReturnParams": ["train_sizes_abs", "train_scores", "test_scores"], "libName": "sklearn.learning_curve", "methods": [], "notes": "See examples/model_selection/plot_learning_curve.py", "funcName": "learning_curve", "allFuncParams": ["estimator", "X", "y", "train_sizes", "cv", "scoring", "exploit_incremental_learning", "n_jobs", "pre_dispatch", "verbose", "error_score"], "funcDesc": "Learning curve.", "funcParamBody": "estimator : object type that implements the fit and predict methods An object of that type which is cloned for each validation. X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape (n_samples) or (n_samples, n_features), optional Target relative to X for classification or regression; None for unsupervised learning. train_sizes : array-like, shape (n_ticks,), dtype float or int Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within (0, 1]. Otherwise it is interpreted as absolute sizes of the training sets. Note that for classification the number of samples usually have to be big enough to contain at least one sample from each class. (default: np.linspace(0.1, 1.0, 5)) cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, sklearn.model_selection.StratifiedKFold is used. In all other cases, sklearn.model_selection.KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. scoring : string, callable or None, optional, default: None A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator,  X,  y) . exploit_incremental_learning : boolean, optional, default: False If the estimator supports incremental learning, this will be used to speed up fitting for different training set sizes. n_jobs : integer, optional Number of jobs to run in parallel (default 1). pre_dispatch : integer or string, optional Number of predispatched jobs for parallel execution (default is all). The option can reduce the allocated memory. The string can be an expression like 2*n_jobs. verbose : integer, optional Controls the verbosity: the higher, the more messages. error_score : raise (default) or numeric Value to assign to the score if an error occurs in estimator fitting. If set to raise, the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.", "funcReturnBody": "train_sizes_abs : array, shape = (n_unique_ticks,), dtype int Numbers of training examples that has been used to generate the learning curve. Note that the number of ticks might be less than n_ticks because duplicate entries will be removed. train_scores : array, shape (n_ticks, n_cv_folds) Scores on training sets. test_scores : array, shape (n_ticks, n_cv_folds) Scores on test set."},
{"allReturnParams": ["score", "parameters", "n_samples_test"], "libName": "sklearn.grid_search", "methods": [], "notes": "", "funcName": "fit_grid_point", "allFuncParams": ["X", "y", "estimator", "parameters", "train", "test", "scorer", "verbose", "**fit_params", "error_score"], "funcDesc": "Run fit on one set of parameters.", "funcParamBody": "X : array-like, sparse matrix or list Input data. y : array-like or None Targets for input data. estimator : estimator object A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a score function, or scoring must be passed. parameters : dict Parameters to be set on estimator for this grid point. train : ndarray, dtype int or bool Boolean mask or indices for training set. test : ndarray, dtype int or bool Boolean mask or indices for test set. scorer : callable or None. If provided must be a scorer callable object / function with signature scorer(estimator,  X,  y) . verbose : int Verbosity level. **fit_params : kwargs Additional parameter passed to the fit function of the estimator. error_score : raise (default) or numeric Value to assign to the score if an error occurs in estimator fitting. If set to raise, the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.", "funcReturnBody": "score : float Score of this parameter setting on given training / test split. parameters : dict The parameters that have been evaluated. n_samples_test : int Number of test samples in this split."},
{"allReturnParams": ["splitting"], "libName": "sklearn.cross_validation", "methods": [], "notes": "", "funcName": "train_test_split", "allFuncParams": ["*arrays", "test_size", "train_size", "random_state", "stratify"], "funcDesc": "Split arrays or matrices into random train and test subsets", "funcParamBody": "*arrays : sequence of indexables with same length / shape[0] Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes. test_size : float, int, or None (default is None) If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is automatically set to the complement of the train size. If train size is also None, test size is set to 0.25. train_size : float, int, or None (default is None) If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . stratify : array-like or None (default is None) If not None, data is split in a stratified fashion, using this as the labels array. New in version 0.17: stratify splitting", "funcReturnBody": "splitting : list, length = 2 * len(arrays), List containing train-test split of inputs. New in version 0.16: If the input is sparse, the output will be a scipy.sparse.csr_matrix . Else, output type is the same as the input type."},
{"allReturnParams": ["score", "permutation_scores", "pvalue"], "libName": "sklearn.cross_validation", "methods": [], "notes": "This function implements Test 1 in:", "funcName": "permutation_test_score", "allFuncParams": ["estimator", "X", "y", "scoring", "cv", "n_permutations", "n_jobs", "labels", "random_state", "verbose"], "funcDesc": "Evaluate the significance of a cross-validated score with permutations", "funcParamBody": "estimator : estimator object implementing fit The object to use to fit the data. X : array-like of shape at least 2D The data to fit. y : array-like The target variable to try to predict in the case of supervised learning. scoring : string, callable or None, optional, default: None A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator,  X,  y) . cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. n_permutations : integer, optional Number of times to permute y . n_jobs : integer, optional The number of CPUs to use to do the computation. -1 means all CPUs. labels : array-like of shape [n_samples] (optional) Labels constrain the permutation among groups of samples with a same label. random_state : int, RandomState instance or None, optional (default=0) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . verbose : integer, optional The verbosity level.", "funcReturnBody": "score : float The true score without permuting targets. permutation_scores : array, shape (n_permutations,) The scores obtained for each permutations. pvalue : float The p-value, which approximates the probability that the score would be obtained by chance. This is calculated as: (C + 1) / (n_permutations + 1) Where C is the number of permutations whose score >= the true score. The best possible p-value is 1/(n_permutations + 1), the worst is 1.0."},
{"allReturnParams": ["scores"], "libName": "sklearn.cross_validation", "methods": [], "notes": "", "funcName": "cross_val_score", "allFuncParams": ["estimator", "X", "y", "scoring", "cv", "n_jobs", "verbose", "fit_params", "pre_dispatch"], "funcDesc": "Evaluate a score by cross-validation", "funcParamBody": "estimator : estimator object implementing fit The object to use to fit the data. X : array-like The data to fit. Can be, for example a list, or an array at least 2d. y : array-like, optional, default: None The target variable to try to predict in the case of supervised learning. scoring : string, callable or None, optional, default: None A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator,  X,  y) . cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. n_jobs : integer, optional The number of CPUs to use to do the computation. -1 means all CPUs. verbose : integer, optional The verbosity level. fit_params : dict, optional Parameters to pass to the fit method of the estimator. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs", "funcReturnBody": "scores : array of float, shape=(len(list(cv)),) Array of scores of the estimator for each run of the cross validation."},
{"allReturnParams": ["preds"], "libName": "sklearn.cross_validation", "methods": [], "notes": "", "funcName": "cross_val_predict", "allFuncParams": ["estimator", "X", "y", "cv", "n_jobs", "verbose", "fit_params", "pre_dispatch"], "funcDesc": "Generate cross-validated estimates for each input data point", "funcParamBody": "estimator : estimator object implementing fit and predict The object to use to fit the data. X : array-like The data to fit. Can be, for example a list, or an array at least 2d. y : array-like, optional, default: None The target variable to try to predict in the case of supervised learning. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. n_jobs : integer, optional The number of CPUs to use to do the computation. -1 means all CPUs. verbose : integer, optional The verbosity level. fit_params : dict, optional Parameters to pass to the fit method of the estimator. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs", "funcReturnBody": "preds : ndarray This is the result of calling predict"},
{"allReturnParams": ["checked_cv"], "libName": "sklearn.cross_validation", "methods": [], "notes": "", "funcName": "check_cv", "allFuncParams": ["cv", "X", "y", "classifier"], "funcDesc": "Input checker utility for building a CV in a user friendly way.", "funcParamBody": "cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, if classifier is True and y is binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. X : array-like The data the cross-val object will be applied on. y : array-like The target variable for a supervised learning problem. classifier : boolean optional Whether the task is a classification task, in which case stratified KFold will be used.", "funcReturnBody": "checked_cv : a cross-validation generator instance. The return value is guaranteed to be a cv generator instance, whatever the input type."},
{"libName": "sklearn.mixture", "methods": [{"methodName": "__init__(*args, **kwargs)", "methodDesc": "DEPRECATED: The VBGMM class is not working correctly and its better to use sklearn.mixture.BayesianGaussianMixture class with parameter weight_concentration_prior_type=dirichlet_distribution instead. VBGMM is deprecated in 0.18 and will be removed in 0.20."}, {"methodName": "aic(X)", "methodReturnsBody": "aic : float (the lower the better)", "methodParams": [], "methodReturns": [], "methodDesc": "Akaike information criterion for the current model fit and the proposed data.", "methodParamsBody": "X : array of shape(n_samples, n_dimensions)"}, {"methodName": "bic(X)", "methodReturnsBody": "bic : float (the lower the better)", "methodParams": [], "methodReturns": [], "methodDesc": "Bayesian information criterion for the current model fit and the proposed data.", "methodParamsBody": "X : array of shape(n_samples, n_dimensions)"}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Estimate model parameters with the EM algorithm. A initialization step is performed before entering the expectation-maximization (EM) algorithm. If you want to avoid this step, set the keyword argument init_params to the empty string  when creating the GMM object. Likewise, if you would like just to do an initialization, set n_iter=0.", "methodParamsBody": "X : array_like, shape (n, n_features)   List of n_features-dimensional data points.  Each row corresponds to a single data point.  "}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "C : array, shape = (n_samples,) component memberships", "methodParams": [], "methodReturns": [], "methodDesc": "Fit and then predict labels for data. Warning: Due to the final maximization step in the EM algorithm, with low iterations the prediction may not be 100%  accurate.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]"}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "lower_bound(X, z)", "methodDesc": "returns a lower bound on model evidence based on X and membership"}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,) component memberships", "methodParams": [], "methodReturns": [], "methodDesc": "Predict label for data.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]"}, {"methodName": "predict_proba(X)", "methodReturnsBody": "responsibilities : array-like, shape = (n_samples, n_components)   Returns the probability of the sample for each Gaussian (state) in the model.  ", "methodParams": ["X"], "methodReturns": ["responsibilities"], "methodDesc": "Predict posterior probability of data under each Gaussian in the model.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "sample(n_samples=1, random_state=None)", "methodReturnsBody": "X : array_like, shape (n_samples, n_features)   List of samples  ", "methodParams": ["n_samples"], "methodReturns": ["X"], "methodDesc": "Generate random samples from the model.", "methodParamsBody": "n_samples : int, optional   Number of samples to generate. Defaults to 1.  "}, {"methodName": "score(X, y=None)", "methodReturnsBody": "logprob : array_like, shape (n_samples,)   Log probabilities of each data point in X  ", "methodParams": ["X"], "methodReturns": ["logprob"], "methodDesc": "Compute the log probability under the model.", "methodParamsBody": "X : array_like, shape (n_samples, n_features)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "score_samples(X)", "methodReturnsBody": "logprob : array_like, shape (n_samples,)   Log probabilities of each data point in X   responsibilities : array_like, shape (n_samples, n_components)   Posterior probabilities of each mixture component for each observation  ", "methodParams": ["X"], "methodReturns": ["logprob", "responsibilities"], "methodDesc": "Return the likelihood of the data under the model. Compute the bound on log probability of X under the model and return the posterior distribution (responsibilities) of each mixture component for each element of X. This is done by computing the parameters for the mean-field of z for each observation.", "methodParamsBody": "X : array_like, shape (n_samples, n_features)   List of n_features-dimensional data points.  Each row corresponds to a single data point.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["n_components", "covariance_type", "alpha", "tol", "n_iter", "params", "init_params", "verbose"], "notes": "", "funcName": "VBGMM", "allFuncAttributes": ["covariance_type", "n_features", "n_components", "weights_", "means_", "precs_", "converged_"], "funcDesc": "Variational Inference for the Gaussian Mixture Model", "funcParamBody": "n_components : int, default 1 Number of mixture components. covariance_type : string, default diag String describing the type of covariance parameters to use.  Must be one of spherical, tied, diag, full. alpha : float, default 1 Real number representing the concentration parameter of the dirichlet distribution. Intuitively, the higher the value of alpha the more likely the variational mixture of Gaussians model will use all components it can. tol : float, default 1e-3 Convergence threshold. n_iter : int, default 10 Maximum number of iterations to perform before convergence. params : string, default wmc Controls which parameters are updated in the training process.  Can contain any combination of w for weights, m for means, and c for covars. init_params : string, default wmc Controls which parameters are updated in the initialization process.  Can contain any combination of w for weights, m for means, and c for covars.  Defaults to wmc. verbose : int, default 0 Controls output verbosity.", "funcAttrBody": "covariance_type : string String describing the type of covariance parameters used by the DP-GMM.  Must be one of spherical, tied, diag, full. n_features : int Dimensionality of the Gaussians. n_components : int (read-only) Number of mixture components. weights_ : array, shape ( n_components ,) Mixing weights for each mixture component. means_ : array, shape ( n_components , n_features ) Mean parameters for each mixture component. precs_ : array Precision (inverse covariance) parameters for each mixture component.  The shape depends on covariance_type : (`n_components`, 'n_features')                if 'spherical', (`n_features`, `n_features`)                  if 'tied', (`n_components`, `n_features`)                if 'diag', (`n_components`, `n_features`, `n_features`)  if 'full' converged_ : bool True when convergence was reached in fit(), False otherwise."},
{"funcName": "GMM", "notes": "", "libName": "sklearn.mixture", "methods": [{"methodName": "__init__(*args, **kwargs)", "methodDesc": "DEPRECATED: The class GMM is deprecated in 0.18 and will be  removed in 0.20. Use class GaussianMixture instead."}, {"methodName": "aic(X)", "methodReturnsBody": "aic : float (the lower the better)", "methodParams": [], "methodReturns": [], "methodDesc": "Akaike information criterion for the current model fit and the proposed data.", "methodParamsBody": "X : array of shape(n_samples, n_dimensions)"}, {"methodName": "bic(X)", "methodReturnsBody": "bic : float (the lower the better)", "methodParams": [], "methodReturns": [], "methodDesc": "Bayesian information criterion for the current model fit and the proposed data.", "methodParamsBody": "X : array of shape(n_samples, n_dimensions)"}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Estimate model parameters with the EM algorithm. A initialization step is performed before entering the expectation-maximization (EM) algorithm. If you want to avoid this step, set the keyword argument init_params to the empty string  when creating the GMM object. Likewise, if you would like just to do an initialization, set n_iter=0.", "methodParamsBody": "X : array_like, shape (n, n_features)   List of n_features-dimensional data points.  Each row corresponds to a single data point.  "}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "C : array, shape = (n_samples,) component memberships", "methodParams": [], "methodReturns": [], "methodDesc": "Fit and then predict labels for data. Warning: Due to the final maximization step in the EM algorithm, with low iterations the prediction may not be 100%  accurate.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]"}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,) component memberships", "methodParams": [], "methodReturns": [], "methodDesc": "Predict label for data.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]"}, {"methodName": "predict_proba(X)", "methodReturnsBody": "responsibilities : array-like, shape = (n_samples, n_components)   Returns the probability of the sample for each Gaussian (state) in the model.  ", "methodParams": ["X"], "methodReturns": ["responsibilities"], "methodDesc": "Predict posterior probability of data under each Gaussian in the model.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "sample(n_samples=1, random_state=None)", "methodReturnsBody": "X : array_like, shape (n_samples, n_features)   List of samples  ", "methodParams": ["n_samples"], "methodReturns": ["X"], "methodDesc": "Generate random samples from the model.", "methodParamsBody": "n_samples : int, optional   Number of samples to generate. Defaults to 1.  "}, {"methodName": "score(X, y=None)", "methodReturnsBody": "logprob : array_like, shape (n_samples,)   Log probabilities of each data point in X  ", "methodParams": ["X"], "methodReturns": ["logprob"], "methodDesc": "Compute the log probability under the model.", "methodParamsBody": "X : array_like, shape (n_samples, n_features)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "score_samples(X)", "methodReturnsBody": "logprob : array_like, shape (n_samples,)   Log probabilities of each data point in X.   responsibilities : array_like, shape (n_samples, n_components)   Posterior probabilities of each mixture component for each observation  ", "methodParams": ["X"], "methodReturns": ["logprob", "responsibilities"], "methodDesc": "Return the per-sample likelihood of the data under the model. Compute the log probability of X under the model and return the posterior distribution (responsibilities) of each mixture component for each element of X.", "methodParamsBody": "X : array_like, shape (n_samples, n_features)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "Legacy Gaussian Mixture Model"},
{"funcName": "DPGMM", "notes": "", "libName": "sklearn.mixture", "methods": [{"methodName": "__init__(*args, **kwargs)", "methodDesc": "DEPRECATED: The DPGMM class is not working correctly and its better to use sklearn.mixture.BayesianGaussianMixture class with parameter weight_concentration_prior_type=dirichlet_process instead. DPGMM is deprecated in 0.18 and will be removed in 0.20."}, {"methodName": "aic(X)", "methodReturnsBody": "aic : float (the lower the better)", "methodParams": [], "methodReturns": [], "methodDesc": "Akaike information criterion for the current model fit and the proposed data.", "methodParamsBody": "X : array of shape(n_samples, n_dimensions)"}, {"methodName": "bic(X)", "methodReturnsBody": "bic : float (the lower the better)", "methodParams": [], "methodReturns": [], "methodDesc": "Bayesian information criterion for the current model fit and the proposed data.", "methodParamsBody": "X : array of shape(n_samples, n_dimensions)"}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Estimate model parameters with the EM algorithm. A initialization step is performed before entering the expectation-maximization (EM) algorithm. If you want to avoid this step, set the keyword argument init_params to the empty string  when creating the GMM object. Likewise, if you would like just to do an initialization, set n_iter=0.", "methodParamsBody": "X : array_like, shape (n, n_features)   List of n_features-dimensional data points.  Each row corresponds to a single data point.  "}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "C : array, shape = (n_samples,) component memberships", "methodParams": [], "methodReturns": [], "methodDesc": "Fit and then predict labels for data. Warning: Due to the final maximization step in the EM algorithm, with low iterations the prediction may not be 100%  accurate.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]"}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "lower_bound(X, z)", "methodDesc": "returns a lower bound on model evidence based on X and membership"}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,) component memberships", "methodParams": [], "methodReturns": [], "methodDesc": "Predict label for data.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]"}, {"methodName": "predict_proba(X)", "methodReturnsBody": "responsibilities : array-like, shape = (n_samples, n_components)   Returns the probability of the sample for each Gaussian (state) in the model.  ", "methodParams": ["X"], "methodReturns": ["responsibilities"], "methodDesc": "Predict posterior probability of data under each Gaussian in the model.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "sample(n_samples=1, random_state=None)", "methodReturnsBody": "X : array_like, shape (n_samples, n_features)   List of samples  ", "methodParams": ["n_samples"], "methodReturns": ["X"], "methodDesc": "Generate random samples from the model.", "methodParamsBody": "n_samples : int, optional   Number of samples to generate. Defaults to 1.  "}, {"methodName": "score(X, y=None)", "methodReturnsBody": "logprob : array_like, shape (n_samples,)   Log probabilities of each data point in X  ", "methodParams": ["X"], "methodReturns": ["logprob"], "methodDesc": "Compute the log probability under the model.", "methodParamsBody": "X : array_like, shape (n_samples, n_features)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "score_samples(X)", "methodReturnsBody": "logprob : array_like, shape (n_samples,)   Log probabilities of each data point in X   responsibilities : array_like, shape (n_samples, n_components)   Posterior probabilities of each mixture component for each observation  ", "methodParams": ["X"], "methodReturns": ["logprob", "responsibilities"], "methodDesc": "Return the likelihood of the data under the model. Compute the bound on log probability of X under the model and return the posterior distribution (responsibilities) of each mixture component for each element of X. This is done by computing the parameters for the mean-field of z for each observation.", "methodParamsBody": "X : array_like, shape (n_samples, n_features)   List of n_features-dimensional data points.  Each row corresponds to a single data point.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "Dirichlet Process Gaussian Mixture Models"},
{"libName": "sklearn.grid_search", "methods": [{"methodName": "__init__(estimator, param_distributions, n_iter=10, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=2*n_jobs, random_state=None, error_score=raise)", "methodDesc": ""}, {"methodParams": ["X"], "methodName": "decision_function(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call decision_function on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports decision_function ."}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vector, where n_samples in the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples] or [n_samples, n_output], optional   Target relative to X for classification or regression; None for unsupervised learning.  ", "methodDesc": "Run fit on the estimator with randomly drawn parameters."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodParams": ["Xt"], "methodName": "inverse_transform(Xt)", "methodParamsBody": "Xt : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call inverse_transform on the estimator with the best found parameters. Only available if the underlying estimator implements inverse_transform and refit=True ."}, {"methodParams": ["X"], "methodName": "predict(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call predict on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict ."}, {"methodParams": ["X"], "methodName": "predict_log_proba(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call predict_log_proba on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict_log_proba ."}, {"methodParams": ["X"], "methodName": "predict_proba(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call predict_proba on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict_proba ."}, {"methodName": "score(X, y=None)", "methodReturnsBody": "score : float ", "methodParams": ["X", "y"], "methodReturns": ["score"], "methodDesc": "Returns the score on the given data, if the estimator has been refit. This uses the score defined by scoring where provided, and the best_estimator_.score method otherwise. Notes", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Input data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples] or [n_samples, n_output], optional   Target relative to X for classification or regression; None for unsupervised learning.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodParams": ["X"], "methodName": "transform(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call transform on the estimator with the best found parameters. Only available if the underlying estimator supports transform and refit=True ."}], "allFuncParams": ["estimator", "param_distributions", "n_iter", "scoring", "fit_params", "n_jobs: int, default: 1 :", "pre_dispatch", "iid", "cv", "refit", "verbose", "random_state", "error_score"], "notes": "The parameters selected are those that maximize the score of the held-out data, according to the scoring parameter. If n_jobs was set to a value higher than one, the data is copied for each parameter setting(and not n_jobs times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available.  A workaround in this case is to set pre_dispatch . Then, the memory is copied only pre_dispatch many times. A reasonable value for pre_dispatch is 2 * n_jobs .", "funcName": "RandomizedSearchCV", "allFuncAttributes": ["grid_scores_", "best_estimator_", "best_score_", "best_params_"], "funcDesc": "Randomized search on hyper parameters.", "funcParamBody": "estimator : estimator object. A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a score function, or scoring must be passed. param_distributions : dict Dictionary with parameters names (string) as keys and distributions or lists of parameters to try. Distributions must provide a rvs n_iter : int, default=10 Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution. scoring : string, callable or None, default=None A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator,  X,  y) . If None , the score method of the estimator is used. fit_params : dict, optional Parameters to pass to the fit method. n_jobs: int, default: 1 : : The maximum number of estimators fit in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus  +  n_jobs  +  1) are used. For example, with n_jobs  =  -2 all CPUs but one are used. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs iid : boolean, default=True If True, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, sklearn.model_selection.StratifiedKFold is used. In all other cases, sklearn.model_selection.KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. refit : boolean, default=True Refit the best estimator with the entire dataset. If False, it is impossible to make predictions using this RandomizedSearchCV instance after fitting. verbose : integer Controls the verbosity: the higher, the more messages. random_state : int, RandomState instance or None, optional, default=None Pseudo random number generator state used for random uniform sampling from lists of possible values instead of scipy.stats distributions. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . error_score : raise (default) or numeric Value to assign to the score if an error occurs in estimator fitting. If set to raise, the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.", "funcAttrBody": "grid_scores_ : list of named tuples Contains scores for all parameter combinations in param_grid. Each entry corresponds to one parameter setting. Each named tuple has the attributes: parameters , a dict of parameter settings mean_validation_score , the mean score over the cross-validation folds cv_validation_scores , the list of scores for each fold best_estimator_ : estimator Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if refit=False. best_score_ : float Score of best_estimator on the left out data. best_params_ : dict Parameter setting that gave the best results on the hold out data."},
{"libName": "sklearn.grid_search", "methods": [{"methodName": "__init__(estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=2*n_jobs, error_score=raise)", "methodDesc": ""}, {"methodParams": ["X"], "methodName": "decision_function(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call decision_function on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports decision_function ."}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vector, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples] or [n_samples, n_output], optional   Target relative to X for classification or regression; None for unsupervised learning.  ", "methodDesc": "Run fit with all sets of parameters."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodParams": ["Xt"], "methodName": "inverse_transform(Xt)", "methodParamsBody": "Xt : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call inverse_transform on the estimator with the best found parameters. Only available if the underlying estimator implements inverse_transform and refit=True ."}, {"methodParams": ["X"], "methodName": "predict(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call predict on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict ."}, {"methodParams": ["X"], "methodName": "predict_log_proba(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call predict_log_proba on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict_log_proba ."}, {"methodParams": ["X"], "methodName": "predict_proba(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call predict_proba on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict_proba ."}, {"methodName": "score(X, y=None)", "methodReturnsBody": "score : float ", "methodParams": ["X", "y"], "methodReturns": ["score"], "methodDesc": "Returns the score on the given data, if the estimator has been refit. This uses the score defined by scoring where provided, and the best_estimator_.score method otherwise. Notes", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Input data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples] or [n_samples, n_output], optional   Target relative to X for classification or regression; None for unsupervised learning.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodParams": ["X"], "methodName": "transform(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call transform on the estimator with the best found parameters. Only available if the underlying estimator supports transform and refit=True ."}], "allFuncParams": ["estimator", "param_grid", "scoring", "fit_params", "n_jobs: int, default: 1 :", "pre_dispatch", "iid", "cv", "refit", "verbose", "error_score"], "notes": "The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed in which case it is used instead. If n_jobs was set to a value higher than one, the data is copied for each point in the grid (and not n_jobs times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available.  A workaround in this case is to set pre_dispatch . Then, the memory is copied only pre_dispatch many times. A reasonable value for pre_dispatch is 2 * n_jobs .", "funcName": "GridSearchCV", "allFuncAttributes": ["grid_scores_", "best_estimator_", "best_score_", "best_params_", "scorer_"], "funcDesc": "Exhaustive search over specified parameter values for an estimator.", "funcParamBody": "estimator : estimator object. A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a score function, or scoring must be passed. param_grid : dict or list of dictionaries Dictionary with parameters names (string) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings. scoring : string, callable or None, default=None A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator,  X,  y) . If None , the score method of the estimator is used. fit_params : dict, optional Parameters to pass to the fit method. n_jobs: int, default: 1 : : The maximum number of estimators fit in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus  +  n_jobs  +  1) are used. For example, with n_jobs  =  -2 all CPUs but one are used. Changed in version 0.17: Upgraded to joblib 0.9.3. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs iid : boolean, default=True If True, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, sklearn.model_selection.StratifiedKFold is used. In all other cases, sklearn.model_selection.KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. refit : boolean, default=True Refit the best estimator with the entire dataset. If False, it is impossible to make predictions using this GridSearchCV instance after fitting. verbose : integer Controls the verbosity: the higher, the more messages. error_score : raise (default) or numeric Value to assign to the score if an error occurs in estimator fitting. If set to raise, the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.", "funcAttrBody": "grid_scores_ : list of named tuples Contains scores for all parameter combinations in param_grid. Each entry corresponds to one parameter setting. Each named tuple has the attributes: parameters , a dict of parameter settings mean_validation_score , the mean score over the cross-validation folds cv_validation_scores , the list of scores for each fold best_estimator_ : estimator Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if refit=False. best_score_ : float Score of best_estimator on the left out data. best_params_ : dict Parameter setting that gave the best results on the hold out data. scorer_ : function Scorer function used on the held out data to choose the best parameters for the model."},
{"allReturnParams": ["params", "Yields"], "libName": "sklearn.grid_search", "methods": [], "notes": "", "funcName": "ParameterSampler", "allFuncParams": ["param_distributions", "n_iter", "random_state"], "funcDesc": "Generator on parameters sampled from given distributions.", "funcParamBody": "param_distributions : dict Dictionary where the keys are parameters and values are distributions from which a parameter is to be sampled. Distributions either have to provide a rvs function to sample from them, or can be given as a list of values, where a uniform distribution is assumed. n_iter : integer Number of parameter settings that are produced. random_state : int, RandomState instance or None, optional (default=None) Pseudo random number generator state used for random uniform sampling from lists of possible values instead of scipy.stats distributions. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "params : dict of string to any Yields dictionaries mapping each estimator parameter to as sampled value."},
{"libName": "sklearn.grid_search", "methods": [], "notes": "", "funcName": "ParameterGrid", "allFuncParams": ["param_grid"], "funcDesc": "Grid of parameters with a discrete number of values for each.", "funcParamBody": "param_grid : dict of string to sequence, or sequence of such The parameter grid to explore, as a dictionary mapping estimator parameters to sequences of allowed values. An empty dict signifies default parameters. A sequence of dicts signifies a sequence of grids to search, and is useful to avoid exploring parameter combinations that make no sense or have no effect. See the examples below."},
{"funcName": "GaussianProcess", "notes": "The presentation implementation is based on a translation of the DACE Matlab toolbox, see reference [NLNS2002] .", "libName": "sklearn.gaussian_process", "methods": [{"methodName": "__init__(*args, **kwargs)", "methodDesc": "DEPRECATED: GaussianProcess was deprecated in version 0.18 and will be removed in 0.20. Use the GaussianProcessRegressor instead."}, {"methodName": "fit(X, y)", "methodReturnsBody": "gp : self   A fitted Gaussian Process model object awaiting data to perform predictions.  ", "methodParams": ["X", "y"], "methodReturns": ["gp"], "methodDesc": "The Gaussian Process model fitting method.", "methodParamsBody": "X : double array_like   An array with shape (n_samples, n_features) with the input at which observations were made.   y : double array_like   An array with shape (n_samples, ) or shape (n_samples, n_targets) with the observations of the output to be predicted.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X, eval_MSE=False, batch_size=None)", "methodReturnsBody": "y : array_like, shape (n_samples, ) or (n_samples, n_targets)   An array with shape (n_eval, ) if the Gaussian Process was trained on an array of shape (n_samples, ) or an array with shape (n_eval, n_targets) if the Gaussian Process was trained on an array of shape (n_samples, n_targets) with the Best Linear Unbiased Prediction at x.   MSE : array_like, optional (if eval_MSE == True)   An array with shape (n_eval, ) or (n_eval, n_targets) as with y, with the Mean Squared Error at x.  ", "methodParams": ["X", "eval_MSE", "batch_size"], "methodReturns": ["y", "MSE"], "methodDesc": "This function evaluates the Gaussian Process model at x.", "methodParamsBody": "X : array_like   An array with shape (n_eval, n_features) giving the point(s) at which the prediction(s) should be made.   eval_MSE : boolean, optional   A boolean specifying whether the Mean Squared Error should be evaluated or not. Default assumes evalMSE = False and evaluates only the BLUP (mean prediction).   batch_size : integer, optional   An integer giving the maximum number of points that can be evaluated simultaneously (depending on the available memory). Default is None so that all given points are evaluated at the same time.  "}, {"methodName": "reduced_likelihood_function(theta=None)", "methodReturnsBody": "reduced_likelihood_function_value : double   The value of the reduced likelihood function associated to the given autocorrelation parameters theta.   par : dict   A dictionary containing the requested Gaussian Process model parameters:   sigma2 is the Gaussian Process variance.  beta is the generalized least-squares regression weights for Universal Kriging or given beta0 for Ordinary Kriging.  gamma is the Gaussian Process weights.  C is the Cholesky decomposition of the correlation matrix [R].  Ft is the solution of the linear equation system [R] x Ft = F  G is the QR decomposition of the matrix Ft.   ", "methodParams": ["theta"], "methodReturns": ["reduced_likelihood_function_value", "par"], "methodDesc": "This function determines the BLUP parameters and evaluates the reduced likelihood function for the given autocorrelation parameters theta. Maximizing this function wrt the autocorrelation parameters theta is equivalent to maximizing the likelihood of the assumed joint Gaussian distribution of the observations y evaluated onto the design of experiments X.", "methodParamsBody": "theta : array_like, optional   An array containing the autocorrelation parameters at which the Gaussian Process model parameters should be determined. Default uses the built-in autocorrelation parameters (ie theta  =  self.theta_ ).  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "The legacy Gaussian Process model class."},
{"funcName": "RandomizedPCA", "notes": "If whitening is enabled, inverse_transform does not compute the exact inverse operation of transform.", "libName": "sklearn.decomposition", "methods": [{"methodName": "__init__(*args, **kwargs)", "methodDesc": "DEPRECATED: RandomizedPCA was deprecated in 0.18 and will be removed in 0.20. Use PCA(svd_solver=randomized) instead. The new implementation DOES NOT store whiten components_ . Apply transform to get them."}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns the instance itself.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model with X by extracting the first principal components.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples in the number of samples and n_features is the number of features.   y : Ignored. "}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components) ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit the model with X and apply the dimensionality reduction on X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   New data, where n_samples in the number of samples and n_features is the number of features.   y : Ignored. "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_original array-like, shape (n_samples, n_features) : ", "methodParams": ["X"], "methodReturns": ["X_original array-like, shape (n_samples, n_features)"], "methodDesc": "Transform data back to its original space. Returns an array X_original whose transform would be X. Notes If whitening is enabled, inverse_transform does not compute the exact inverse operation of transform.", "methodParamsBody": "X : array-like, shape (n_samples, n_components)   New data, where n_samples in the number of samples and n_components is the number of components.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components) ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Apply dimensionality reduction on X. X is projected on the first principal components previous extracted from a training set.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   New data, where n_samples in the number of samples and n_features is the number of features.  "}], "funcDesc": "Principal component analysis (PCA) using randomized SVD"},
{"libName": "sklearn.cross_validation", "methods": [], "notes": "", "funcName": "PredefinedSplit", "allFuncParams": ["test_fold"], "funcDesc": "Predefined split cross validation iterator", "funcParamBody": "test_fold : array-like, shape (n_samples,) test_fold[i] gives the test set fold of sample i. A value of -1 indicates that the corresponding sample is not part of any test set folds, but will instead always be put into the training fold."},
{"libName": "sklearn.cross_validation", "methods": [], "notes": "", "funcName": "StratifiedShuffleSplit", "allFuncParams": ["y", "n_iter", "test_size", "train_size", "random_state"], "funcDesc": "Stratified ShuffleSplit cross validation iterator", "funcParamBody": "y : array, [n_samples] Labels of samples. n_iter : int (default 10) Number of re-shuffling & splitting iterations. test_size : float (default 0.1), int, or None If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is automatically set to the complement of the train size. train_size : float, int, or None (default is None) If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. random_state : int, RandomState instance or None, optional (default None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random ."},
{"libName": "sklearn.cross_validation", "methods": [], "notes": "All the folds have size trunc(n_samples / n_folds), the last one has the complementary.", "funcName": "StratifiedKFold", "allFuncParams": ["y", "n_folds", "shuffle", "random_state"], "funcDesc": "Stratified K-Folds cross validation iterator", "funcParamBody": "y : array-like, [n_samples] Samples to split in K folds. n_folds : int, default=3 Number of folds. Must be at least 2. shuffle : boolean, optional Whether to shuffle each stratification of the data before splitting into batches. random_state : int, RandomState instance or None, optional, default=None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when shuffle == True."},
{"libName": "sklearn.cross_validation", "methods": [], "notes": "", "funcName": "ShuffleSplit", "allFuncParams": ["n", "n_iter", "test_size", "train_size", "random_state"], "funcDesc": "Random permutation cross-validation iterator.", "funcParamBody": "n : int Total number of elements in the dataset. n_iter : int (default 10) Number of re-shuffling & splitting iterations. test_size : float (default 0.1), int, or None If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is automatically set to the complement of the train size. train_size : float, int, or None (default is None) If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. random_state : int, RandomState instance or None, optional (default None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random ."},
{"libName": "sklearn.cross_validation", "methods": [], "notes": "", "funcName": "LabelShuffleSplit", "allFuncParams": ["labels", "n_iter", "test_size", "train_size", "random_state"], "funcDesc": "Shuffle-Labels-Out cross-validation iterator", "funcParamBody": "labels :  array, [n_samples] Labels of samples n_iter : int (default 5) Number of re-shuffling and splitting iterations. test_size : float (default 0.2), int, or None If float, should be between 0.0 and 1.0 and represent the proportion of the labels to include in the test split. If int, represents the absolute number of test labels. If None, the value is automatically set to the complement of the train size. train_size : float, int, or None (default is None) If float, should be between 0.0 and 1.0 and represent the proportion of the labels to include in the train split. If int, represents the absolute number of train labels. If None, the value is automatically set to the complement of the test size. random_state : int, RandomState instance or None, optional (default None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random ."},
{"libName": "sklearn.cross_validation", "methods": [], "notes": "", "funcName": "LeavePLabelOut", "allFuncParams": ["labels", "p"], "funcDesc": "Leave-P-Label_Out cross-validation iterator", "funcParamBody": "labels : array-like of int with shape (n_samples,) Arbitrary domain-specific stratification of the data to be used to draw the splits. p : int Number of samples to leave out in the test split."},
{"libName": "sklearn.cross_validation", "methods": [], "notes": "", "funcName": "LeavePOut", "allFuncParams": ["n", "p"], "funcDesc": "Leave-P-Out cross validation iterator", "funcParamBody": "n : int Total number of elements in dataset. p : int Size of the test sets."},
{"libName": "sklearn.cross_validation", "methods": [], "notes": "", "funcName": "LeaveOneOut", "allFuncParams": ["n"], "funcDesc": "Leave-One-Out cross validation iterator.", "funcParamBody": "n : int Total number of elements in dataset."},
{"libName": "sklearn.cross_validation", "methods": [], "notes": "", "funcName": "LeaveOneLabelOut", "allFuncParams": ["labels"], "funcDesc": "Leave-One-Label_Out cross-validation iterator", "funcParamBody": "labels : array-like of int with shape (n_samples,) Arbitrary domain-specific stratification of the data to be used to draw the splits."},
{"libName": "sklearn.cross_validation", "methods": [], "notes": "", "funcName": "LabelKFold", "allFuncParams": ["labels", "n_folds"], "funcDesc": "K-fold iterator variant with non-overlapping labels.", "funcParamBody": "labels : array-like with shape (n_samples, ) Contains a label for each sample. The folds are built so that the same label does not appear in two different folds. n_folds : int, default=3 Number of folds. Must be at least 2."},
{"libName": "sklearn.cross_validation", "methods": [], "notes": "The first n % n_folds folds have size n // n_folds + 1, other folds have size n // n_folds.", "funcName": "KFold", "allFuncParams": ["n", "n_folds", "shuffle", "random_state"], "funcDesc": "K-Folds cross validation iterator.", "funcParamBody": "n : int Total number of elements. n_folds : int, default=3 Number of folds. Must be at least 2. shuffle : boolean, optional Whether to shuffle the data before splitting into batches. random_state : int, RandomState instance or None, optional, default=None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when shuffle == True."},
{"funcName": "LSHForest", "notes": "", "libName": "sklearn.neighbors", "methods": [{"methodName": "__init__(n_estimators=10, radius=1.0, n_candidates=50, n_neighbors=5, min_hash_match=4, radius_cutoff_ratio=0.9, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Fit the LSH forest on the data. This creates binary hashes of input data points by getting the dot product of input points and hash_function then transforming the projection into a binary string array based on the sign (positive/negative) of the projection. A sorted array of binary hashes is created.", "methodParamsBody": "X : array_like or sparse (CSR) matrix, shape (n_samples, n_features)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "kneighbors(X, n_neighbors=None, return_distance=True)", "methodReturnsBody": "dist : array, shape (n_samples, n_neighbors)   Array representing the cosine distances to each point, only present if return_distance=True.   ind : array, shape (n_samples, n_neighbors)   Indices of the approximate nearest points in the population matrix.  ", "methodParams": ["X", "n_neighbors", "return_distance"], "methodReturns": ["dist", "ind"], "methodDesc": "Returns n_neighbors of approximate nearest neighbors.", "methodParamsBody": "X : array_like or sparse (CSR) matrix, shape (n_samples, n_features)   List of n_features-dimensional data points.  Each row corresponds to a single query.   n_neighbors : int, optional (default = None)   Number of neighbors required. If not provided, this will return the number specified at the initialization.   return_distance : boolean, optional (default = True)   Returns the distances of neighbors if set to True.  "}, {"methodName": "kneighbors_graph(X=None, n_neighbors=None, mode=connectivity)", "methodReturnsBody": "A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]   n_samples_fit is the number of samples in the fitted data A[i, j] is assigned the weight of edge that connects i to j.  ", "methodParams": ["X", "n_neighbors", "mode"], "methodReturns": ["A"], "methodDesc": "Computes the (weighted) graph of k-Neighbors for points in X Examples", "methodParamsBody": "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == precomputed   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   n_neighbors : int   Number of neighbors for each sample. (default is value passed to the constructor).   mode : {connectivity, distance}, optional   Type of returned matrix: connectivity will return the connectivity matrix with ones and zeros, in distance the edges are Euclidean distance between points.  "}, {"methodParams": ["X"], "methodName": "partial_fit(X, y=None)", "methodParamsBody": "X : array_like or sparse (CSR) matrix, shape (n_samples, n_features)   New data point to be inserted into the LSH Forest.  ", "methodDesc": "Inserts new data into the already fitted LSH Forest. Cost is proportional to new total size, so additions should be batched."}, {"methodName": "radius_neighbors(X, radius=None, return_distance=True)", "methodReturnsBody": "dist : array, shape (n_samples,) of arrays   Each element is an array representing the cosine distances to some points found within radius of the respective query. Only present if return_distance=True .   ind : array, shape (n_samples,) of arrays   Each element is an array of indices for neighbors within radius of the respective query.  ", "methodParams": ["X", "radius", "return_distance"], "methodReturns": ["dist", "ind"], "methodDesc": "Finds the neighbors within a given radius of a point or points. Return the indices and distances of some points from the dataset lying in a ball with size radius around the points of the query array. Points lying on the boundary are included in the results. The result points are not necessarily sorted by distance to their query point. LSH Forest being an approximate method, some true neighbors from the indexed dataset might be missing from the results.", "methodParamsBody": "X : array_like or sparse (CSR) matrix, shape (n_samples, n_features)   List of n_features-dimensional data points. Each row corresponds to a single query.   radius : float   Limiting distance of neighbors to return. (default is the value passed to the constructor).   return_distance : boolean, optional (default = False)   Returns the distances of neighbors if set to True.  "}, {"methodName": "radius_neighbors_graph(X=None, radius=None, mode=connectivity)", "methodReturnsBody": "A : sparse matrix in CSR format, shape = [n_samples, n_samples]   A[i, j] is assigned the weight of edge that connects i to j.  ", "methodParams": ["X", "radius", "mode"], "methodReturns": ["A"], "methodDesc": "Computes the (weighted) graph of Neighbors for points in X Neighborhoods are restricted the points at a distance lower than radius. Examples", "methodParamsBody": "X : array-like, shape = [n_samples, n_features], optional   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   radius : float   Radius of neighborhoods. (default is the value passed to the constructor).   mode : {connectivity, distance}, optional   Type of returned matrix: connectivity will return the connectivity matrix with ones and zeros, in distance the edges are Euclidean distance between points.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "Performs approximate nearest neighbor search using LSH forest."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(*args, **kwargs)", "methodDesc": "DEPRECATED: The class RandomizedLogisticRegression is deprecated in 0.19 and will be removed in 0.21."}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : object   Returns an instance of self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model using X, y as training data.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training data.   y : array-like, shape = [n_samples]   Target values. Will be cast to Xs dtype if necessary  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_support(indices=False)", "methodReturnsBody": "support : array   An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.  ", "methodParams": ["indices"], "methodReturns": ["support"], "methodDesc": "Get a mask, or integer index, of the features selected", "methodParamsBody": "indices : boolean (default False)   If True, the return value will be an array of integers, rather than a boolean mask.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_original_features]   X with columns of zeros inserted where features would have been removed by transform .  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reverse the transformation operation", "methodParamsBody": "X : array of shape [n_samples, n_selected_features]   The input samples.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_selected_features]   The input samples with only the selected features.  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reduce X to the selected features.", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.  "}], "allFuncParams": ["C", "scaling", "sample_fraction", "n_resampling", "selection_threshold", "tol", "fit_intercept", "verbose", "normalize", "random_state", "n_jobs", "pre_dispatch", "memory"], "notes": "", "funcName": "RandomizedLogisticRegression", "allFuncAttributes": ["scores_", "all_scores_"], "funcDesc": "Randomized Logistic Regression", "funcParamBody": "C : float or array-like of shape [n_reg_parameter], optional, default=1 The regularization parameter C in the LogisticRegression. When C is an array, fit will take each regularization parameter in C one by one for LogisticRegression and store results for each one in all_scores_ , where columns and rows represent corresponding reg_parameters and features. scaling : float, optional, default=0.5 The s parameter used to randomly scale the penalty of different features. Should be between 0 and 1. sample_fraction : float, optional, default=0.75 The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used. n_resampling : int, optional, default=200 Number of randomized models. selection_threshold : float, optional, default=0.25 The score above which features should be selected. tol : float, optional, default=1e-3 tolerance for stopping criteria of LogisticRegression fit_intercept : boolean, optional, default=True whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). verbose : boolean or integer, optional Sets the verbosity amount normalize : boolean, optional, default True If True, the regressors X will be normalized before regression. This parameter is ignored when fit_intercept is set to False. When the regressors are normalized, note that this makes the hyperparameters learnt more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use preprocessing.StandardScaler before calling fit on an estimator with normalize=False . random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_jobs : integer, optional Number of CPUs to use during the resampling. If -1, use all the CPUs pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs memory : None, str or object with the joblib.Memory interface, optional             (default=None) Used for internal caching. By default, no caching is done. If a string is given, it is the path to the caching directory.", "funcAttrBody": "scores_ : array, shape = [n_features] Feature scores between 0 and 1. all_scores_ : array, shape = [n_features, n_reg_parameter] Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests scores_ is the max         of all_scores_ ."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(*args, **kwargs)", "methodDesc": "DEPRECATED: The class RandomizedLasso is deprecated in 0.19 and will be removed in 0.21."}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : object   Returns an instance of self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model using X, y as training data.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training data.   y : array-like, shape = [n_samples]   Target values. Will be cast to Xs dtype if necessary  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_support(indices=False)", "methodReturnsBody": "support : array   An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.  ", "methodParams": ["indices"], "methodReturns": ["support"], "methodDesc": "Get a mask, or integer index, of the features selected", "methodParamsBody": "indices : boolean (default False)   If True, the return value will be an array of integers, rather than a boolean mask.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_original_features]   X with columns of zeros inserted where features would have been removed by transform .  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reverse the transformation operation", "methodParamsBody": "X : array of shape [n_samples, n_selected_features]   The input samples.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_selected_features]   The input samples with only the selected features.  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reduce X to the selected features.", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.  "}], "allFuncParams": ["alpha", "scaling", "sample_fraction", "n_resampling", "selection_threshold", "fit_intercept", "verbose", "normalize", "precompute", "max_iter", "eps", "random_state", "n_jobs", "pre_dispatch", "memory"], "notes": "", "funcName": "RandomizedLasso", "allFuncAttributes": ["scores_", "all_scores_"], "funcDesc": "Randomized Lasso.", "funcParamBody": "alpha : float, aic, or bic, optional The regularization parameter alpha parameter in the Lasso. Warning: this is not the alpha parameter in the stability selection article which is scaling. scaling : float, optional The s parameter used to randomly scale the penalty of different features. Should be between 0 and 1. sample_fraction : float, optional The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used. n_resampling : int, optional Number of randomized models. selection_threshold : float, optional The score above which features should be selected. fit_intercept : boolean, optional whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). verbose : boolean or integer, optional Sets the verbosity amount normalize : boolean, optional, default True If True, the regressors X will be normalized before regression. This parameter is ignored when fit_intercept is set to False. When the regressors are normalized, note that this makes the hyperparameters learned more robust and almost independent of the number of samples. The same property is not valid for standardized data. However, if you wish to standardize, please use preprocessing.StandardScaler before calling fit on an estimator with normalize=False . precompute : True | False | auto | array-like Whether to use a precomputed Gram matrix to speed up calculations. If set to auto let us decide. The Gram matrix can also be passed as argument, but it will be used only for the selection of parameter alpha, if alpha is aic or bic. max_iter : integer, optional Maximum number of iterations to perform in the Lars algorithm. eps : float, optional The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the tol parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_jobs : integer, optional Number of CPUs to use during the resampling. If -1, use all the CPUs pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs memory : None, str or object with the joblib.Memory interface, optional             (default=None) Used for internal caching. By default, no caching is done. If a string is given, it is the path to the caching directory.", "funcAttrBody": "scores_ : array, shape = [n_features] Feature scores between 0 and 1. all_scores_ : array, shape = [n_features, n_reg_parameter] Feature scores between 0 and 1 for all values of the regularization         parameter. The reference article suggests scores_ is the max of all_scores_ ."},
{"allReturnParams": ["is_parameter: bool"], "libName": "sklearn.utils.validation", "methods": [], "notes": "", "funcName": "has_fit_parameter", "allFuncParams": ["estimator", "parameter: str"], "funcDesc": "Checks whether the estimators fit method supports the given parameter.", "funcParamBody": "estimator : object An estimator to inspect. parameter: str : The searched parameter.", "funcReturnBody": "is_parameter: bool : Whether the parameter was found to be a named parameter of the estimators fit method."},
{"allReturnParams": ["y"], "libName": "sklearn.utils.validation", "methods": [], "notes": "", "funcName": "column_or_1d", "allFuncParams": ["y", "warn"], "funcDesc": "Ravel column or 1d numpy array, else raises an error", "funcParamBody": "y : array-like warn : boolean, default False To control display of warnings.", "funcReturnBody": "y : array"},
{"allReturnParams": ["array_sym"], "libName": "sklearn.utils.validation", "methods": [], "notes": "", "funcName": "check_symmetric", "allFuncParams": ["array", "tol", "raise_warning", "raise_exception"], "funcDesc": "Make sure that array is 2D, square and symmetric.", "funcParamBody": "array : nd-array or sparse matrix Input object to check / convert. Must be two-dimensional and square, otherwise a ValueError will be raised. tol : float Absolute tolerance for equivalence of arrays. Default = 1E-10. raise_warning : boolean (default=True) If True then raise a warning if conversion is required. raise_exception : boolean (default=False) If True then raise an exception if array is not symmetric.", "funcReturnBody": "array_sym : ndarray or sparse matrix Symmetrized version of the input array, i.e. the average of array and array.transpose(). If sparse, then duplicate entries are first summed and zeros are eliminated."},
{"allReturnParams": ["memory"], "libName": "sklearn.utils.validation", "methods": [], "notes": "", "funcName": "check_memory", "allFuncParams": ["memory"], "funcDesc": "Check that ", "funcParamBody": "memory : None, str or object with the joblib.Memory interface", "funcReturnBody": "memory : object with the joblib.Memory interface"},
{"allReturnParams": ["None"], "libName": "sklearn.utils.validation", "methods": [], "notes": "", "funcName": "check_is_fitted", "allFuncParams": ["estimator", "attributes", "msg", "all_or_any"], "funcDesc": "Perform is_fitted validation for estimator.", "funcParamBody": "estimator : estimator instance. estimator instance for which the check is performed. attributes : attribute name(s) given as string or a list/tuple of strings Eg.: [\"coef_\",  \"estimator_\",  ...],  \"coef_\" msg : string The default error message is, This %(name)s instance is not fitted yet. Call fit with appropriate arguments before using this method. For custom messages if %(name)s is present in the message string, it is substituted for the estimator name. Eg. : Estimator, %(name)s, must be fitted before sparsifying. all_or_any : callable, {all, any}, default all Specify whether all or any of the given attributes must exist.", "funcReturnBody": "None :"},
{"allReturnParams": ["means", "variances"], "libName": "sklearn.utils.sparsefuncs", "methods": [], "notes": "", "funcName": "mean_variance_axis", "allFuncParams": ["X", "axis"], "funcDesc": "Compute mean and variance along an axix on a CSR or CSC matrix", "funcParamBody": "X : CSR or CSC sparse matrix, shape (n_samples, n_features) Input data. axis : int (either 0 or 1) Axis along which the axis should be computed.", "funcReturnBody": "means : float array with shape (n_features,) Feature-wise means variances : float array with shape (n_features,) Feature-wise variances"},
{"libName": "sklearn.utils.sparsefuncs", "methods": [], "notes": "", "funcName": "inplace_swap_column", "allFuncParams": ["X", "m", "n"], "funcDesc": "Swaps two columns of a CSC/CSR matrix in-place.", "funcParamBody": "X : CSR or CSC sparse matrix, shape=(n_samples, n_features) Matrix whose two columns are to be swapped. m : int Index of the column of X to be swapped. n : int Index of the column of X to be swapped."},
{"libName": "sklearn.utils.sparsefuncs", "methods": [], "notes": "", "funcName": "inplace_swap_row", "allFuncParams": ["X", "m", "n"], "funcDesc": "Swaps two rows of a CSC/CSR matrix in-place.", "funcParamBody": "X : CSR or CSC sparse matrix, shape=(n_samples, n_features) Matrix whose two rows are to be swapped. m : int Index of the row of X to be swapped. n : int Index of the row of X to be swapped."},
{"libName": "sklearn.utils.sparsefuncs", "methods": [], "notes": "", "funcName": "inplace_row_scale", "allFuncParams": ["X", "scale"], "funcDesc": "Inplace row scaling of a CSR or CSC matrix.", "funcParamBody": "X : CSR or CSC sparse matrix, shape (n_samples, n_features) Matrix to be scaled. scale : float array with shape (n_features,) Array of precomputed sample-wise values to use for scaling."},
{"libName": "sklearn.utils.sparsefuncs", "methods": [], "notes": "", "funcName": "inplace_column_scale", "allFuncParams": ["X", "scale"], "funcDesc": "Inplace column scaling of a CSC/CSR matrix.", "funcParamBody": "X : CSC or CSR matrix with shape (n_samples, n_features) Matrix to normalize using the variance of the features. scale : float array with shape (n_features,) Array of precomputed feature-wise values to use for scaling."},
{"allReturnParams": ["means", "variances", "n"], "libName": "sklearn.utils.sparsefuncs", "methods": [], "notes": "", "funcName": "incr_mean_variance_axis", "allFuncParams": ["X", "axis", "last_mean", "last_var", "last_n"], "funcDesc": "Compute incremental mean and variance along an axix on a CSR or CSC matrix.", "funcParamBody": "X : CSR or CSC sparse matrix, shape (n_samples, n_features) Input data. axis : int (either 0 or 1) Axis along which the axis should be computed. last_mean : float array with shape (n_features,) Array of feature-wise means to update with the new data X. last_var : float array with shape (n_features,) Array of feature-wise var to update with the new data X. last_n : int Number of samples seen so far, excluded X.", "funcReturnBody": "means : float array with shape (n_features,) Updated feature-wise means. variances : float array with shape (n_features,) Updated feature-wise variances. n : int Updated number of seen samples."},
{"allReturnParams": ["shuffled_arrays"], "libName": "sklearn.utils", "methods": [], "notes": "", "funcName": "shuffle", "allFuncParams": ["*arrays", "random_state", "n_samples"], "funcDesc": "Shuffle arrays or sparse matrices in a consistent way", "funcParamBody": "*arrays : sequence of indexable data-structures Indexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with consistent first dimension. random_state : int, RandomState instance or None, optional (default=None) The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_samples : int, None by default Number of samples to generate. If left to None this is automatically set to the first dimension of the arrays.", "funcReturnBody": "shuffled_arrays : sequence of indexable data-structures Sequence of shuffled views of the collections. The original arrays are not impacted."},
{"allReturnParams": ["subset"], "libName": "sklearn.utils", "methods": [], "notes": "CSR, CSC, and LIL sparse matrices are supported. COO sparse matrices are not supported.", "funcName": "safe_indexing", "allFuncParams": ["X", "indices"], "funcDesc": "Return items or rows from X using indices.", "funcParamBody": "X : array-like, sparse-matrix, list, pandas.DataFrame, pandas.Series. Data from which to sample rows or items. indices : array-like of int Indices according to which X will be subsampled.", "funcReturnBody": "subset : Subset of X on first axis"},
{"allReturnParams": ["resampled_arrays"], "libName": "sklearn.utils", "methods": [], "notes": "", "funcName": "resample", "allFuncParams": ["*arrays", "replace", "n_samples", "random_state"], "funcDesc": "Resample arrays or sparse matrices in a consistent way", "funcParamBody": "*arrays : sequence of indexable data-structures Indexable data-structures can be arrays, lists, dataframes or scipy sparse matrices with consistent first dimension. replace : boolean, True by default Implements resampling with replacement. If False, this will implement (sliced) random permutations. n_samples : int, None by default Number of samples to generate. If left to None this is automatically set to the first dimension of the arrays. If replace is False it should not be larger than the length of arrays. random_state : int, RandomState instance or None, optional (default=None) The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "resampled_arrays : sequence of indexable data-structures Sequence of resampled views of the collections. The original arrays are not impacted."},
{"libName": "sklearn.utils", "methods": [], "notes": "", "funcName": "indexable", "allFuncParams": ["*iterables"], "funcDesc": "Make arrays indexable for cross-validation.", "funcParamBody": "*iterables : lists, dataframes, arrays, sparse matrices List of objects to ensure sliceability."},
{"allReturnParams": ["dot_product"], "libName": "sklearn.utils.extmath", "methods": [], "notes": "", "funcName": "safe_sparse_dot", "allFuncParams": ["a", "b", "dense_output"], "funcDesc": "Dot product that handle the sparse matrix case correctly", "funcParamBody": "a : array or sparse matrix b : array or sparse matrix dense_output : boolean, default False When False, either a or b being sparse will yield sparse output. When True, output will always be an array.", "funcReturnBody": "dot_product : array or sparse matrix sparse if a or b is sparse and dense_output=False ."},
{"libName": "sklearn.utils.estimator_checks", "methods": [], "notes": "", "funcName": "check_estimator", "allFuncParams": ["estimator"], "funcDesc": "Check if estimator adheres to scikit-learn conventions.", "funcParamBody": "estimator : estimator object or class Estimator to check. Estimator is a class object or instance."},
{"allReturnParams": ["sample_weight_vect"], "libName": "sklearn.utils.class_weight", "methods": [], "notes": "", "funcName": "compute_sample_weight", "allFuncParams": ["class_weight", "y", "indices"], "funcDesc": "Estimate sample weights by class for unbalanced datasets.", "funcParamBody": "class_weight : dict, list of dicts, balanced, or None, optional Weights associated with classes in the form {class_label:  weight} . If not given, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data: n_samples  /  (n_classes  *  np.bincount(y)) . For multi-output, the weights of each column of y will be multiplied. y : array-like, shape = [n_samples] or [n_samples, n_outputs] Array of original class labels per sample. indices : array-like, shape (n_subsample,), or None Array of indices to be used in a subsample. Can be of length less than n_samples in the case of a subsample, or equal to n_samples in the case of a bootstrap subsample with repeated indices. If None, the sample weight will be calculated over the full sample. Only balanced is supported for class_weight if this is provided.", "funcReturnBody": "sample_weight_vect : ndarray, shape (n_samples,) Array with sample weights as applied to the original y"},
{"allReturnParams": ["class_weight_vect"], "libName": "sklearn.utils.class_weight", "methods": [], "notes": "", "funcName": "compute_class_weight", "allFuncParams": ["class_weight", "classes", "y"], "funcDesc": "Estimate class weights for unbalanced datasets.", "funcParamBody": "class_weight : dict, balanced or None If balanced, class weights will be given by n_samples  /  (n_classes  *  np.bincount(y)) . If a dictionary is given, keys are classes and values are corresponding class weights. If None is given, the class weights will be uniform. classes : ndarray Array of the classes occurring in the data, as given by np.unique(y_org) with y_org the original class labels. y : array-like, shape (n_samples,) Array of original class labels per sample;", "funcReturnBody": "class_weight_vect : ndarray, shape (n_classes,) Array with class_weight_vect[i] the weight for i-th class"},
{"libName": "sklearn.utils", "methods": [], "notes": "", "funcName": "check_random_state", "allFuncParams": ["seed"], "funcDesc": "Turn seed into a np.random.RandomState instance", "funcParamBody": "seed : None | int | instance of RandomState If seed is None, return the RandomState singleton used by np.random. If seed is an int, return a new RandomState instance seeded with seed. If seed is already a RandomState instance, return it. Otherwise raise ValueError."},
{"libName": "sklearn.utils", "methods": [], "notes": "", "funcName": "check_consistent_length", "allFuncParams": ["*arrays"], "funcDesc": "Check that all arrays have consistent first dimensions.", "funcParamBody": "*arrays : list or tuple of input objects. Objects that will be checked for consistent length."},
{"allReturnParams": ["X_converted"], "libName": "sklearn.utils", "methods": [], "notes": "", "funcName": "check_array", "allFuncParams": ["array", "accept_sparse", "dtype", "order", "copy", "force_all_finite", "ensure_2d", "allow_nd", "ensure_min_samples", "ensure_min_features", "warn_on_dtype", "estimator"], "funcDesc": "Input validation on an array, list, sparse matrix or similar.", "funcParamBody": "array : object Input object to check / convert. accept_sparse : string, boolean or list/tuple of strings (default=False) String[s] representing allowed sparse matrix formats, such as csc, csr, etc. If the input is sparse but not in the allowed format, it will be converted to the first listed format. True allows the input to be any format. False means that a sparse matrix input will raise an error. Deprecated since version 0.19: Passing None to parameter accept_sparse in methods is deprecated in version 0.19 and will be removed in 0.21. Use accept_sparse=False instead. dtype : string, type, list of types or None (default=numeric) Data type of result. If None, the dtype of the input is preserved. If numeric, dtype is preserved unless array.dtype is object. If dtype is a list of types, conversion on the first type is only performed if the dtype of the input is not in the list. order : F, C or None (default=None) Whether an array will be forced to be fortran or c-style. When order is None (default), then if copy=False, nothing is ensured about the memory layout of the output array; otherwise (copy=True) the memory layout of the returned array is kept as close as possible to the original array. copy : boolean (default=False) Whether a forced copy will be triggered. If copy=False, a copy might be triggered by a conversion. force_all_finite : boolean (default=True) Whether to raise an error on np.inf and np.nan in X. ensure_2d : boolean (default=True) Whether to raise a value error if X is not 2d. allow_nd : boolean (default=False) Whether to allow X.ndim > 2. ensure_min_samples : int (default=1) Make sure that the array has a minimum number of samples in its first axis (rows for a 2D array). Setting to 0 disables this check. ensure_min_features : int (default=1) Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when the input data has effectively 2 dimensions or is originally 1D and ensure_2d is True. Setting to 0 disables this check. warn_on_dtype : boolean (default=False) Raise DataConversionWarning if the dtype of the input data structure does not match the requested dtype, causing a memory copy. estimator : str or estimator instance (default=None) If passed, include the name of the estimator in warning messages.", "funcReturnBody": "X_converted : object The converted and validated X."},
{"allReturnParams": ["X_converted", "y_converted"], "libName": "sklearn.utils", "methods": [], "notes": "", "funcName": "check_X_y", "allFuncParams": ["X", "y", "accept_sparse", "dtype", "order", "copy", "force_all_finite", "ensure_2d", "allow_nd", "multi_output", "ensure_min_samples", "ensure_min_features", "y_numeric", "warn_on_dtype", "estimator"], "funcDesc": "Input validation for standard estimators.", "funcParamBody": "X : nd-array, list or sparse matrix Input data. y : nd-array, list or sparse matrix Labels. accept_sparse : string, boolean or list of string (default=False) String[s] representing allowed sparse matrix formats, such as csc, csr, etc. If the input is sparse but not in the allowed format, it will be converted to the first listed format. True allows the input to be any format. False means that a sparse matrix input will raise an error. Deprecated since version 0.19: Passing None to parameter accept_sparse in methods is deprecated in version 0.19 and will be removed in 0.21. Use accept_sparse=False instead. dtype : string, type, list of types or None (default=numeric) Data type of result. If None, the dtype of the input is preserved. If numeric, dtype is preserved unless array.dtype is object. If dtype is a list of types, conversion on the first type is only performed if the dtype of the input is not in the list. order : F, C or None (default=None) Whether an array will be forced to be fortran or c-style. copy : boolean (default=False) Whether a forced copy will be triggered. If copy=False, a copy might be triggered by a conversion. force_all_finite : boolean (default=True) Whether to raise an error on np.inf and np.nan in X. This parameter does not influence whether y can have np.inf or np.nan values. ensure_2d : boolean (default=True) Whether to make X at least 2d. allow_nd : boolean (default=False) Whether to allow X.ndim > 2. multi_output : boolean (default=False) Whether to allow 2-d y (array or sparse matrix). If false, y will be validated as a vector. y cannot have np.nan or np.inf values if multi_output=True. ensure_min_samples : int (default=1) Make sure that X has a minimum number of samples in its first axis (rows for a 2D array). ensure_min_features : int (default=1) Make sure that the 2D array has some minimum number of features (columns). The default value of 1 rejects empty datasets. This check is only enforced when X has effectively 2 dimensions or is originally 1D and ensure_2d is True. Setting to 0 disables this check. y_numeric : boolean (default=False) Whether to ensure that y has a numeric type. If dtype of y is object, it is converted to float64. Should only be used for regression algorithms. warn_on_dtype : boolean (default=False) Raise DataConversionWarning if the dtype of the input data structure does not match the requested dtype, causing a memory copy. estimator : str or estimator instance (default=None) If passed, include the name of the estimator in warning messages.", "funcReturnBody": "X_converted : object The converted and validated X. y_converted : object The converted and validated y."},
{"libName": "sklearn.utils", "methods": [], "notes": "", "funcName": "assert_all_finite", "allFuncParams": [], "funcDesc": "Throw a ValueError if X contains NaN or infinity.", "funcParamBody": "X : array or sparse matrix"},
{"allReturnParams": ["XT"], "libName": "sklearn.utils", "methods": [], "notes": "", "funcName": "as_float_array", "allFuncParams": ["X", "copy", "force_all_finite"], "funcDesc": "Converts an array-like to an array of floats.", "funcParamBody": "X : {array-like, sparse matrix} copy : bool, optional If True, a copy of X will be created. If False, a copy may still be returned if Xs dtype is not a floating point type. force_all_finite : boolean (default=True) Whether to raise an error on np.inf and np.nan in X.", "funcReturnBody": "XT : {array, sparse matrix} An array of type np.float"},
{"allReturnParams": ["dot_data"], "libName": "sklearn.tree", "methods": [], "notes": "", "funcName": "export_graphviz", "allFuncParams": ["decision_tree", "out_file", "max_depth", "feature_names", "class_names", "label", "filled", "leaves_parallel", "impurity", "node_ids", "proportion", "rotate", "rounded", "special_characters", "precision"], "funcDesc": "Export a decision tree in DOT format.", "funcParamBody": "decision_tree : decision tree classifier The decision tree to be exported to GraphViz. out_file : file object or string, optional (default=tree.dot) Handle or name of the output file. If None , the result is returned as a string. This will the default from version 0.20. max_depth : int, optional (default=None) The maximum depth of the representation. If None, the tree is fully generated. feature_names : list of strings, optional (default=None) Names of each of the features. class_names : list of strings, bool or None, optional (default=None) Names of each of the target classes in ascending numerical order. Only relevant for classification and not supported for multi-output. If True , shows a symbolic representation of the class name. label : {all, root, none}, optional (default=all) Whether to show informative labels for impurity, etc. Options include all to show at every node, root to show only at the top root node, or none to not show at any node. filled : bool, optional (default=False) When set to True , paint nodes to indicate majority class for classification, extremity of values for regression, or purity of node for multi-output. leaves_parallel : bool, optional (default=False) When set to True , draw all leaf nodes at the bottom of the tree. impurity : bool, optional (default=True) When set to True , show the impurity at each node. node_ids : bool, optional (default=False) When set to True , show the ID number on each node. proportion : bool, optional (default=False) When set to True , change the display of values and/or samples to be proportions and percentages respectively. rotate : bool, optional (default=False) When set to True , orient tree left to right rather than top-down. rounded : bool, optional (default=False) When set to True , draw node boxes with rounded corners and use Helvetica fonts instead of Times-Roman. special_characters : bool, optional (default=False) When set to False , ignore special characters for PostScript compatibility. precision : int, optional (default=3) Number of digits of precision for floating point in the values of impurity, threshold and value attributes of each node.", "funcReturnBody": "dot_data : string String representation of the input tree in GraphViz dot format. Only returned if out_file is None. New in version 0.18."},
{"funcName": "ExtraTreeRegressor", "notes": "The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.", "libName": "sklearn.tree", "methods": [{"methodName": "__init__(criterion=mse, splitter=random, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=auto, random_state=None, min_impurity_decrease=0.0, min_impurity_split=None, max_leaf_nodes=None)", "methodDesc": ""}, {"methodName": "apply(X, check_input=True)", "methodReturnsBody": "X_leaves : array_like, shape = [n_samples,]   For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within [0;  self.tree_.node_count) , possibly with gaps in the numbering.  ", "methodParams": ["X", "check_input"], "methodReturns": ["X_leaves"], "methodDesc": "Returns the index of the leaf that each sample is predicted as.", "methodParamsBody": "X : array_like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.  "}, {"methodName": "decision_path(X, check_input=True)", "methodReturnsBody": "indicator : sparse csr array, shape = [n_samples, n_nodes]   Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  ", "methodParams": ["X", "check_input"], "methodReturns": ["indicator"], "methodDesc": "Return the decision path in the tree", "methodParamsBody": "X : array_like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.  "}, {"methodName": "fit(X, y, sample_weight=None, check_input=True, X_idx_sorted=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight", "check_input", "X_idx_sorted"], "methodReturns": ["self"], "methodDesc": "Build a decision tree regressor from the training set (X, y).", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The training input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csc_matrix .   y : array-like, shape = [n_samples] or [n_samples, n_outputs]   The target values (real numbers). Use dtype=np.float64 and order='C' for maximum efficiency.   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node.   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.   X_idx_sorted : array-like, shape = [n_samples, n_features], optional   The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Dont use this parameter unless you know what to do.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X, check_input=True)", "methodReturnsBody": "y : array of shape = [n_samples] or [n_samples, n_outputs]   The predicted classes, or the predict values.  ", "methodParams": ["X", "check_input"], "methodReturns": ["y"], "methodDesc": "Predict class or regression value for X. For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "An extremely randomized tree regressor."},
{"funcName": "ExtraTreeClassifier", "notes": "The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.", "libName": "sklearn.tree", "methods": [{"methodName": "__init__(criterion=gini, splitter=random, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=auto, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None)", "methodDesc": ""}, {"methodName": "apply(X, check_input=True)", "methodReturnsBody": "X_leaves : array_like, shape = [n_samples,]   For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within [0;  self.tree_.node_count) , possibly with gaps in the numbering.  ", "methodParams": ["X", "check_input"], "methodReturns": ["X_leaves"], "methodDesc": "Returns the index of the leaf that each sample is predicted as.", "methodParamsBody": "X : array_like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.  "}, {"methodName": "decision_path(X, check_input=True)", "methodReturnsBody": "indicator : sparse csr array, shape = [n_samples, n_nodes]   Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  ", "methodParams": ["X", "check_input"], "methodReturns": ["indicator"], "methodDesc": "Return the decision path in the tree", "methodParamsBody": "X : array_like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.  "}, {"methodName": "fit(X, y, sample_weight=None, check_input=True, X_idx_sorted=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight", "check_input", "X_idx_sorted"], "methodReturns": ["self"], "methodDesc": "Build a decision tree classifier from the training set (X, y).", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The training input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csc_matrix .   y : array-like, shape = [n_samples] or [n_samples, n_outputs]   The target values (class labels) as integers or strings.   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. Splits are also ignored if they would result in any single class carrying a negative weight in either child node.   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.   X_idx_sorted : array-like, shape = [n_samples, n_features], optional   The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Dont use this parameter unless you know what to do.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X, check_input=True)", "methodReturnsBody": "y : array of shape = [n_samples] or [n_samples, n_outputs]   The predicted classes, or the predict values.  ", "methodParams": ["X", "check_input"], "methodReturns": ["y"], "methodDesc": "Predict class or regression value for X. For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.  "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "p : array of shape = [n_samples, n_classes], or a list of n_outputs   such arrays if n_outputs > 1. The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Predict class log-probabilities of the input samples X.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .  "}, {"methodName": "predict_proba(X, check_input=True)", "methodReturnsBody": "p : array of shape = [n_samples, n_classes], or a list of n_outputs   such arrays if n_outputs > 1. The class probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_ .  ", "methodParams": ["X", "check_input"], "methodReturns": ["p"], "methodDesc": "Predict class probabilities of the input samples X. The predicted class probability is the fraction of samples of the same class in a leaf.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .   check_input : bool   Run check_array on X.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "An extremely randomized tree classifier."},
{"funcName": "DecisionTreeRegressor", "notes": "The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed.", "libName": "sklearn.tree", "methods": [{"methodName": "__init__(criterion=mse, splitter=best, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, presort=False)", "methodDesc": ""}, {"methodName": "apply(X, check_input=True)", "methodReturnsBody": "X_leaves : array_like, shape = [n_samples,]   For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within [0;  self.tree_.node_count) , possibly with gaps in the numbering.  ", "methodParams": ["X", "check_input"], "methodReturns": ["X_leaves"], "methodDesc": "Returns the index of the leaf that each sample is predicted as.", "methodParamsBody": "X : array_like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.  "}, {"methodName": "decision_path(X, check_input=True)", "methodReturnsBody": "indicator : sparse csr array, shape = [n_samples, n_nodes]   Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  ", "methodParams": ["X", "check_input"], "methodReturns": ["indicator"], "methodDesc": "Return the decision path in the tree", "methodParamsBody": "X : array_like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.  "}, {"methodName": "fit(X, y, sample_weight=None, check_input=True, X_idx_sorted=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight", "check_input", "X_idx_sorted"], "methodReturns": ["self"], "methodDesc": "Build a decision tree regressor from the training set (X, y).", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The training input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csc_matrix .   y : array-like, shape = [n_samples] or [n_samples, n_outputs]   The target values (real numbers). Use dtype=np.float64 and order='C' for maximum efficiency.   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node.   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.   X_idx_sorted : array-like, shape = [n_samples, n_features], optional   The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Dont use this parameter unless you know what to do.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X, check_input=True)", "methodReturnsBody": "y : array of shape = [n_samples] or [n_samples, n_outputs]   The predicted classes, or the predict values.  ", "methodParams": ["X", "check_input"], "methodReturns": ["y"], "methodDesc": "Predict class or regression value for X. For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "A decision tree regressor."},
{"funcName": "DecisionTreeClassifier", "notes": "The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed.", "libName": "sklearn.tree", "methods": [{"methodName": "__init__(criterion=gini, splitter=best, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)", "methodDesc": ""}, {"methodName": "apply(X, check_input=True)", "methodReturnsBody": "X_leaves : array_like, shape = [n_samples,]   For each datapoint x in X, return the index of the leaf x ends up in. Leaves are numbered within [0;  self.tree_.node_count) , possibly with gaps in the numbering.  ", "methodParams": ["X", "check_input"], "methodReturns": ["X_leaves"], "methodDesc": "Returns the index of the leaf that each sample is predicted as.", "methodParamsBody": "X : array_like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.  "}, {"methodName": "decision_path(X, check_input=True)", "methodReturnsBody": "indicator : sparse csr array, shape = [n_samples, n_nodes]   Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.  ", "methodParams": ["X", "check_input"], "methodReturns": ["indicator"], "methodDesc": "Return the decision path in the tree", "methodParamsBody": "X : array_like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.  "}, {"methodName": "fit(X, y, sample_weight=None, check_input=True, X_idx_sorted=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight", "check_input", "X_idx_sorted"], "methodReturns": ["self"], "methodDesc": "Build a decision tree classifier from the training set (X, y).", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The training input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csc_matrix .   y : array-like, shape = [n_samples] or [n_samples, n_outputs]   The target values (class labels) as integers or strings.   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. Splits are also ignored if they would result in any single class carrying a negative weight in either child node.   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.   X_idx_sorted : array-like, shape = [n_samples, n_features], optional   The indexes of the sorted training input samples. If many tree are grown on the same dataset, this allows the ordering to be cached between trees. If None, the data will be sorted here. Dont use this parameter unless you know what to do.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X, check_input=True)", "methodReturnsBody": "y : array of shape = [n_samples] or [n_samples, n_outputs]   The predicted classes, or the predict values.  ", "methodParams": ["X", "check_input"], "methodReturns": ["y"], "methodDesc": "Predict class or regression value for X. For a classification model, the predicted class for each sample in X is returned. For a regression model, the predicted value based on X is returned.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.  "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "p : array of shape = [n_samples, n_classes], or a list of n_outputs   such arrays if n_outputs > 1. The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Predict class log-probabilities of the input samples X.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .  "}, {"methodName": "predict_proba(X, check_input=True)", "methodReturnsBody": "p : array of shape = [n_samples, n_classes], or a list of n_outputs   such arrays if n_outputs > 1. The class probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_ .  ", "methodParams": ["X", "check_input"], "methodReturns": ["p"], "methodDesc": "Predict class probabilities of the input samples X. The predicted class probability is the fraction of samples of the same class in a leaf.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .   check_input : bool   Run check_array on X.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "A decision tree classifier."},
{"allReturnParams": ["dec_values"], "libName": "sklearn.svm.libsvm", "methods": [], "notes": "", "funcName": "predict_proba", "allFuncParams": ["X", "kernel"], "funcDesc": "Predict probabilities", "funcParamBody": "X : array-like, dtype=float kernel : {linear, rbf, poly, sigmoid, precomputed}", "funcReturnBody": "dec_values : array predicted values."},
{"allReturnParams": ["dec_values"], "libName": "sklearn.svm.libsvm", "methods": [], "notes": "", "funcName": "predict", "allFuncParams": ["X", "svm_type", "kernel", "degree", "gamma", "coef0"], "funcDesc": "Predict target values of X given a model (low-level method)", "funcParamBody": "X : array-like, dtype=float, size=[n_samples, n_features] svm_type : {0, 1, 2, 3, 4} Type of SVM: C SVC, nu SVC, one class, epsilon SVR, nu SVR kernel : {linear, rbf, poly, sigmoid, precomputed} Type of kernel. degree : int Degree of the polynomial kernel. gamma : float Gamma parameter in rbf, poly and sigmoid kernels. Ignored by other kernels. 0.1 by default. coef0 : float Independent parameter in poly/sigmoid kernel.", "funcReturnBody": "dec_values : array predicted values."},
{"allReturnParams": ["support", "support_vectors", "n_class_SV", "sv_coef", "intercept", "probA, probB"], "libName": "sklearn.svm.libsvm", "methods": [], "notes": "", "funcName": "fit", "allFuncParams": ["X", "Y", "svm_type", "kernel", "degree", "gamma", "coef0", "tol", "C", "nu", "epsilon", "class_weight", "sample_weight", "shrinking", "probability", "cache_size", "max_iter", "random_seed"], "funcDesc": "Train the model using libsvm (low-level method)", "funcParamBody": "X : array-like, dtype=float64, size=[n_samples, n_features] Y : array, dtype=float64, size=[n_samples] target vector svm_type : {0, 1, 2, 3, 4}, optional Type of SVM: C_SVC, NuSVC, OneClassSVM, EpsilonSVR or NuSVR respectively. 0 by default. kernel : {linear, rbf, poly, sigmoid, precomputed}, optional Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed. rbf by default. degree : int32, optional Degree of the polynomial kernel (only relevant if kernel is set to polynomial), 3 by default. gamma : float64, optional Gamma parameter in rbf, poly and sigmoid kernels. Ignored by other kernels. 0.1 by default. coef0 : float64, optional Independent parameter in poly/sigmoid kernel. 0 by default. tol : float64, optional Numeric stopping criterion (WRITEME). 1e-3 by default. C : float64, optional C parameter in C-Support Vector Classification. 1 by default. nu : float64, optional 0.5 by default. epsilon : double, optional 0.1 by default. class_weight : array, dtype float64, shape (n_classes,), optional np.empty(0) by default. sample_weight : array, dtype float64, shape (n_samples,), optional np.empty(0) by default. shrinking : int, optional 1 by default. probability : int, optional 0 by default. cache_size : float64, optional Cache size for gram matrix columns (in megabytes). 100 by default. max_iter : int (-1 for no limit), optional. Stop solver after this many iterations regardless of accuracy (XXX Currently there is no API to know whether this kicked in.) -1 by default. random_seed : int, optional Seed for the random number generator used for probability estimates. 0 by default.", "funcReturnBody": "support : array, shape=[n_support] index of support vectors support_vectors : array, shape=[n_support, n_features] support vectors (equivalent to X[support]). Will return an empty array in the case of precomputed kernel. n_class_SV : array number of support vectors in each class. sv_coef : array coefficients of support vectors in decision function. intercept : array intercept in decision function probA, probB : array probability estimates, empty array for probability=False"},
{"funcName": "decision_function", "notes": "", "libName": "sklearn.svm.libsvm", "methods": [], "funcDesc": "Predict margin (libsvm name for this is predict_values)"},
{"allReturnParams": ["target"], "libName": "sklearn.svm.libsvm", "methods": [], "notes": "", "funcName": "cross_validation", "allFuncParams": ["X", "Y", "svm_type", "kernel", "degree", "gamma", "coef0", "tol", "C", "nu", "cache_size", "random_seed"], "funcDesc": "Binding of the cross-validation routine (low-level routine)", "funcParamBody": "X : array-like, dtype=float, size=[n_samples, n_features] Y : array, dtype=float, size=[n_samples] target vector svm_type : {0, 1, 2, 3, 4} Type of SVM: C SVC, nu SVC, one class, epsilon SVR, nu SVR kernel : {linear, rbf, poly, sigmoid, precomputed} Kernel to use in the model: linear, polynomial, RBF, sigmoid or precomputed. degree : int Degree of the polynomial kernel (only relevant if kernel is set to polynomial) gamma : float Gamma parameter in rbf, poly and sigmoid kernels. Ignored by other kernels. 0.1 by default. coef0 : float Independent parameter in poly/sigmoid kernel. tol : float Stopping criteria. C : float C parameter in C-Support Vector Classification nu : float cache_size : float random_seed : int, optional Seed for the random number generator used for probability estimates. 0 by default.", "funcReturnBody": "target : array, float"},
{"allReturnParams": ["l1_min_c"], "libName": "sklearn.svm", "methods": [], "notes": "", "funcName": "l1_min_c", "allFuncParams": ["X", "y", "loss", "fit_intercept", "intercept_scaling"], "funcDesc": "Return the lowest bound for C such that for C in (l1_min_C, infinity) the model is guaranteed not to be empty. This applies to l1 penalized classifiers, such as LinearSVC with penalty=l1 and linear_model.LogisticRegression with penalty=l1.", "funcParamBody": "X : array-like or sparse matrix, shape = [n_samples, n_features] Training vector, where n_samples in the number of samples and n_features is the number of features. y : array, shape = [n_samples] Target vector relative to X loss : {squared_hinge, log}, default squared_hinge Specifies the loss function. With squared_hinge it is the squared hinge loss (a.k.a. L2 loss). With log it is the loss of logistic regression models. l2 is accepted as an alias for squared_hinge, for backward compatibility reasons, but should not be used in new code. fit_intercept : bool, default: True Specifies if the intercept should be fitted by the model. It must match the fit() method parameter. intercept_scaling : float, default: 1 when fit_intercept is True, instance vector x becomes [x, intercept_scaling], i.e. a synthetic feature with constant value equals to intercept_scaling is appended to the instance vector. It must match the fit() method parameter.", "funcReturnBody": "l1_min_c : float minimum value for C"},
{"libName": "sklearn.svm", "methods": [{"methodName": "__init__(kernel=rbf, degree=3, gamma=auto, coef0=0.0, tol=0.001, C=1.0, epsilon=0.1, shrinking=True, cache_size=200, verbose=False, max_iter=-1)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the SVM model according to the given training data. Notes If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied. If X is a dense array, then the other methods will not support sparse matrices as input.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=precomputed, the expected shape of X is (n_samples, n_samples).   y : array-like, shape (n_samples,)   Target values (class labels in classification, real numbers in regression)   sample_weight : array-like, shape (n_samples,)   Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y_pred : array, shape (n_samples,) ", "methodParams": ["X"], "methodReturns": ["y_pred"], "methodDesc": "Perform regression on samples in X. For an one-class model, +1 (inlier) or -1 (outlier) is returned.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   For kernel=precomputed, the expected shape of X is (n_samples_test, n_samples_train).  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["C", "epsilon", "kernel", "degree", "gamma", "coef0", "shrinking", "tol", "cache_size", "verbose", "max_iter"], "notes": "If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied. If X is a dense array, then the other methods will not support sparse matrices as input. If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied. If X is a dense array, then the other methods will not support sparse matrices as input.", "funcName": "SVR", "allFuncAttributes": ["support_", "support_vectors_", "dual_coef_", "coef_", "intercept_", "sample_weight"], "funcDesc": "Epsilon-Support Vector Regression.", "funcParamBody": "C : float, optional (default=1.0) Penalty parameter C of the error term. epsilon : float, optional (default=0.1) Epsilon in the epsilon-SVR model. It specifies the epsilon-tube within which no penalty is associated in the training loss function with points predicted within a distance epsilon from the actual value. kernel : string, optional (default=rbf) Specifies the kernel type to be used in the algorithm. It must be one of linear, poly, rbf, sigmoid, precomputed or a callable. If none is given, rbf will be used. If a callable is given it is used to precompute the kernel matrix. degree : int, optional (default=3) Degree of the polynomial kernel function (poly). Ignored by all other kernels. gamma : float, optional (default=auto) Kernel coefficient for rbf, poly and sigmoid. If gamma is auto then 1/n_features will be used instead. coef0 : float, optional (default=0.0) Independent term in kernel function. It is only significant in poly and sigmoid. shrinking : boolean, optional (default=True) Whether to use the shrinking heuristic. tol : float, optional (default=1e-3) Tolerance for stopping criterion. cache_size : float, optional Specify the size of the kernel cache (in MB). verbose : bool, default: False Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. max_iter : int, optional (default=-1) Hard limit on iterations within solver, or -1 for no limit.", "funcAttrBody": "support_ : array-like, shape = [n_SV] Indices of support vectors. support_vectors_ : array-like, shape = [nSV, n_features] Support vectors. dual_coef_ : array, shape = [1, n_SV] Coefficients of the support vector in the decision function. coef_ : array, shape = [1, n_features] Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. coef_ is readonly property derived from dual_coef_ and support_vectors_ . intercept_ : array, shape = [1] Constants in decision function. sample_weight : array-like, shape = [n_samples] Individual weights for each sample"},
{"libName": "sklearn.svm", "methods": [{"methodName": "__init__(C=1.0, kernel=rbf, degree=3, gamma=auto, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=ovr, random_state=None)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "X : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)   Returns the decision function of the sample for each class in the model. If decision_function_shape=ovr, the shape is (n_samples, n_classes)  ", "methodParams": ["X"], "methodReturns": ["X"], "methodDesc": "Distance of the samples X to the separating hyperplane.", "methodParamsBody": "X : array-like, shape (n_samples, n_features) "}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the SVM model according to the given training data. Notes If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied. If X is a dense array, then the other methods will not support sparse matrices as input.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=precomputed, the expected shape of X is (n_samples, n_samples).   y : array-like, shape (n_samples,)   Target values (class labels in classification, real numbers in regression)   sample_weight : array-like, shape (n_samples,)   Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y_pred : array, shape (n_samples,)   Class labels for samples in X.  ", "methodParams": ["X"], "methodReturns": ["y_pred"], "methodDesc": "Perform classification on samples in X. For an one-class model, +1 or -1 is returned.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   For kernel=precomputed, the expected shape of X is [n_samples_test, n_samples_train]  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["C", "kernel", "degree", "gamma", "coef0", "probability", "shrinking", "tol", "cache_size", "class_weight", "verbose", "max_iter", "decision_function_shape", "random_state"], "notes": "If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied. If X is a dense array, then the other methods will not support sparse matrices as input. The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets. The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets. If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied. If X is a dense array, then the other methods will not support sparse matrices as input. The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets. The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets.", "funcName": "SVC", "allFuncAttributes": ["support_", "support_vectors_", "n_support_", "dual_coef_", "coef_", "intercept_"], "funcDesc": "C-Support Vector Classification.", "funcParamBody": "C : float, optional (default=1.0) Penalty parameter C of the error term. kernel : string, optional (default=rbf) Specifies the kernel type to be used in the algorithm. It must be one of linear, poly, rbf, sigmoid, precomputed or a callable. If none is given, rbf will be used. If a callable is given it is used to pre-compute the kernel matrix from data matrices; that matrix should be an array of shape (n_samples,  n_samples) . degree : int, optional (default=3) Degree of the polynomial kernel function (poly). Ignored by all other kernels. gamma : float, optional (default=auto) Kernel coefficient for rbf, poly and sigmoid. If gamma is auto then 1/n_features will be used instead. coef0 : float, optional (default=0.0) Independent term in kernel function. It is only significant in poly and sigmoid. probability : boolean, optional (default=False) Whether to enable probability estimates. This must be enabled prior to calling fit , and will slow down that method. shrinking : boolean, optional (default=True) Whether to use the shrinking heuristic. tol : float, optional (default=1e-3) Tolerance for stopping criterion. cache_size : float, optional Specify the size of the kernel cache (in MB). class_weight : {dict, balanced}, optional Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples  /  (n_classes  *  np.bincount(y)) verbose : bool, default: False Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. max_iter : int, optional (default=-1) Hard limit on iterations within solver, or -1 for no limit. decision_function_shape : ovo, ovr, default=ovr Whether to return a one-vs-rest (ovr) decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (ovo) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). Changed in version 0.19: decision_function_shape is ovr by default. New in version 0.17: decision_function_shape=ovr is recommended. Changed in version 0.17: Deprecated decision_function_shape=ovo and None . random_state : int, RandomState instance or None, optional (default=None) The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcAttrBody": "support_ : array-like, shape = [n_SV] Indices of support vectors. support_vectors_ : array-like, shape = [n_SV, n_features] Support vectors. n_support_ : array-like, dtype=int32, shape = [n_class] Number of support vectors for each class. dual_coef_ : array, shape = [n_class-1, n_SV] Coefficients of the support vector in the decision function. For multiclass, coefficient for all 1-vs-1 classifiers. The layout of the coefficients in the multiclass case is somewhat non-trivial. See the section about multi-class classification in the SVM section of the User Guide for details. coef_ : array, shape = [n_class-1, n_features] Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. coef_ is a readonly property derived from dual_coef_ and support_vectors_ . intercept_ : array, shape = [n_class * (n_class-1) / 2] Constants in decision function."},
{"libName": "sklearn.svm", "methods": [{"methodName": "__init__(kernel=rbf, degree=3, gamma=auto, coef0=0.0, tol=0.001, nu=0.5, shrinking=True, cache_size=200, verbose=False, max_iter=-1, random_state=None)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "X : array-like, shape (n_samples,)   Returns the decision function of the samples.  ", "methodParams": ["X"], "methodReturns": ["X"], "methodDesc": "Signed distance to the separating hyperplane. Signed distance is positive for an inlier and negative for an outlier.", "methodParamsBody": "X : array-like, shape (n_samples, n_features) "}, {"methodName": "fit(X, y=None, sample_weight=None, **params)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Detects the soft boundary of the set of samples X. Notes If X is not a C-ordered contiguous array it is copied.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Set of samples, where n_samples is the number of samples and n_features is the number of features.   sample_weight : array-like, shape (n_samples,)   Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y_pred : array, shape (n_samples,)   Class labels for samples in X.  ", "methodParams": ["X"], "methodReturns": ["y_pred"], "methodDesc": "Perform classification on samples in X. For an one-class model, +1 or -1 is returned.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   For kernel=precomputed, the expected shape of X is [n_samples_test, n_samples_train]  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["kernel", "nu", "degree", "gamma", "coef0", "tol", "shrinking", "cache_size", "verbose", "max_iter", "random_state"], "notes": "If X is not a C-ordered contiguous array it is copied. If X is not a C-ordered contiguous array it is copied.", "funcName": "OneClassSVM", "allFuncAttributes": ["support_", "support_vectors_", "dual_coef_", "coef_", "intercept_"], "funcDesc": "Unsupervised Outlier Detection.", "funcParamBody": "kernel : string, optional (default=rbf) Specifies the kernel type to be used in the algorithm. It must be one of linear, poly, rbf, sigmoid, precomputed or a callable. If none is given, rbf will be used. If a callable is given it is used to precompute the kernel matrix. nu : float, optional An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1]. By default 0.5 will be taken. degree : int, optional (default=3) Degree of the polynomial kernel function (poly). Ignored by all other kernels. gamma : float, optional (default=auto) Kernel coefficient for rbf, poly and sigmoid. If gamma is auto then 1/n_features will be used instead. coef0 : float, optional (default=0.0) Independent term in kernel function. It is only significant in poly and sigmoid. tol : float, optional Tolerance for stopping criterion. shrinking : boolean, optional Whether to use the shrinking heuristic. cache_size : float, optional Specify the size of the kernel cache (in MB). verbose : bool, default: False Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. max_iter : int, optional (default=-1) Hard limit on iterations within solver, or -1 for no limit. random_state : int, RandomState instance or None, optional (default=None) The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcAttrBody": "support_ : array-like, shape = [n_SV] Indices of support vectors. support_vectors_ : array-like, shape = [nSV, n_features] Support vectors. dual_coef_ : array, shape = [1, n_SV] Coefficients of the support vectors in the decision function. coef_ : array, shape = [1, n_features] Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. coef_ is readonly property derived from dual_coef_ and support_vectors_ intercept_ : array, shape = [1,] Constant in the decision function."},
{"libName": "sklearn.svm", "methods": [{"methodName": "__init__(nu=0.5, C=1.0, kernel=rbf, degree=3, gamma=auto, coef0=0.0, shrinking=True, tol=0.001, cache_size=200, verbose=False, max_iter=-1)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the SVM model according to the given training data. Notes If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied. If X is a dense array, then the other methods will not support sparse matrices as input.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=precomputed, the expected shape of X is (n_samples, n_samples).   y : array-like, shape (n_samples,)   Target values (class labels in classification, real numbers in regression)   sample_weight : array-like, shape (n_samples,)   Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y_pred : array, shape (n_samples,) ", "methodParams": ["X"], "methodReturns": ["y_pred"], "methodDesc": "Perform regression on samples in X. For an one-class model, +1 (inlier) or -1 (outlier) is returned.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   For kernel=precomputed, the expected shape of X is (n_samples_test, n_samples_train).  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["C", "nu", "kernel", "degree", "gamma", "coef0", "shrinking", "tol", "cache_size", "verbose", "max_iter"], "notes": "If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied. If X is a dense array, then the other methods will not support sparse matrices as input. If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied. If X is a dense array, then the other methods will not support sparse matrices as input.", "funcName": "NuSVR", "allFuncAttributes": ["support_", "support_vectors_", "dual_coef_", "coef_", "intercept_"], "funcDesc": "Nu Support Vector Regression.", "funcParamBody": "C : float, optional (default=1.0) Penalty parameter C of the error term. nu : float, optional An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1].  By default 0.5 will be taken. kernel : string, optional (default=rbf) Specifies the kernel type to be used in the algorithm. It must be one of linear, poly, rbf, sigmoid, precomputed or a callable. If none is given, rbf will be used. If a callable is given it is used to precompute the kernel matrix. degree : int, optional (default=3) Degree of the polynomial kernel function (poly). Ignored by all other kernels. gamma : float, optional (default=auto) Kernel coefficient for rbf, poly and sigmoid. If gamma is auto then 1/n_features will be used instead. coef0 : float, optional (default=0.0) Independent term in kernel function. It is only significant in poly and sigmoid. shrinking : boolean, optional (default=True) Whether to use the shrinking heuristic. tol : float, optional (default=1e-3) Tolerance for stopping criterion. cache_size : float, optional Specify the size of the kernel cache (in MB). verbose : bool, default: False Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. max_iter : int, optional (default=-1) Hard limit on iterations within solver, or -1 for no limit.", "funcAttrBody": "support_ : array-like, shape = [n_SV] Indices of support vectors. support_vectors_ : array-like, shape = [nSV, n_features] Support vectors. dual_coef_ : array, shape = [1, n_SV] Coefficients of the support vector in the decision function. coef_ : array, shape = [1, n_features] Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. coef_ is readonly property derived from dual_coef_ and support_vectors_ . intercept_ : array, shape = [1] Constants in decision function."},
{"libName": "sklearn.svm", "methods": [{"methodName": "__init__(nu=0.5, kernel=rbf, degree=3, gamma=auto, coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape=ovr, random_state=None)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "X : array-like, shape (n_samples, n_classes * (n_classes-1) / 2)   Returns the decision function of the sample for each class in the model. If decision_function_shape=ovr, the shape is (n_samples, n_classes)  ", "methodParams": ["X"], "methodReturns": ["X"], "methodDesc": "Distance of the samples X to the separating hyperplane.", "methodParamsBody": "X : array-like, shape (n_samples, n_features) "}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the SVM model according to the given training data. Notes If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied. If X is a dense array, then the other methods will not support sparse matrices as input.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Training vectors, where n_samples is the number of samples and n_features is the number of features. For kernel=precomputed, the expected shape of X is (n_samples, n_samples).   y : array-like, shape (n_samples,)   Target values (class labels in classification, real numbers in regression)   sample_weight : array-like, shape (n_samples,)   Per-sample weights. Rescale C per sample. Higher weights force the classifier to put more emphasis on these points.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y_pred : array, shape (n_samples,)   Class labels for samples in X.  ", "methodParams": ["X"], "methodReturns": ["y_pred"], "methodDesc": "Perform classification on samples in X. For an one-class model, +1 or -1 is returned.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   For kernel=precomputed, the expected shape of X is [n_samples_test, n_samples_train]  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["nu", "kernel", "degree", "gamma", "coef0", "probability", "shrinking", "tol", "cache_size", "class_weight", "verbose", "max_iter", "decision_function_shape", "random_state"], "notes": "If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied. If X is a dense array, then the other methods will not support sparse matrices as input. The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets. The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets. If X and y are not C-ordered and contiguous arrays of np.float64 and X is not a scipy.sparse.csr_matrix, X and/or y may be copied. If X is a dense array, then the other methods will not support sparse matrices as input. The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets. The probability model is created using cross validation, so the results can be slightly different than those obtained by predict. Also, it will produce meaningless results on very small datasets.", "funcName": "NuSVC", "allFuncAttributes": ["support_", "support_vectors_", "n_support_", "dual_coef_", "coef_", "intercept_"], "funcDesc": "Nu-Support Vector Classification.", "funcParamBody": "nu : float, optional (default=0.5) An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors. Should be in the interval (0, 1]. kernel : string, optional (default=rbf) Specifies the kernel type to be used in the algorithm. It must be one of linear, poly, rbf, sigmoid, precomputed or a callable. If none is given, rbf will be used. If a callable is given it is used to precompute the kernel matrix. degree : int, optional (default=3) Degree of the polynomial kernel function (poly). Ignored by all other kernels. gamma : float, optional (default=auto) Kernel coefficient for rbf, poly and sigmoid. If gamma is auto then 1/n_features will be used instead. coef0 : float, optional (default=0.0) Independent term in kernel function. It is only significant in poly and sigmoid. probability : boolean, optional (default=False) Whether to enable probability estimates. This must be enabled prior to calling fit , and will slow down that method. shrinking : boolean, optional (default=True) Whether to use the shrinking heuristic. tol : float, optional (default=1e-3) Tolerance for stopping criterion. cache_size : float, optional Specify the size of the kernel cache (in MB). class_weight : {dict, balanced}, optional Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies as n_samples  /  (n_classes  *  np.bincount(y)) verbose : bool, default: False Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in libsvm that, if enabled, may not work properly in a multithreaded context. max_iter : int, optional (default=-1) Hard limit on iterations within solver, or -1 for no limit. decision_function_shape : ovo, ovr, default=ovr Whether to return a one-vs-rest (ovr) decision function of shape (n_samples, n_classes) as all other classifiers, or the original one-vs-one (ovo) decision function of libsvm which has shape (n_samples, n_classes * (n_classes - 1) / 2). Changed in version 0.19: decision_function_shape is ovr by default. New in version 0.17: decision_function_shape=ovr is recommended. Changed in version 0.17: Deprecated decision_function_shape=ovo and None . random_state : int, RandomState instance or None, optional (default=None) The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcAttrBody": "support_ : array-like, shape = [n_SV] Indices of support vectors. support_vectors_ : array-like, shape = [n_SV, n_features] Support vectors. n_support_ : array-like, dtype=int32, shape = [n_class] Number of support vectors for each class. dual_coef_ : array, shape = [n_class-1, n_SV] Coefficients of the support vector in the decision function. For multiclass, coefficient for all 1-vs-1 classifiers. The layout of the coefficients in the multiclass case is somewhat non-trivial. See the section about multi-class classification in the SVM section of the User Guide for details. coef_ : array, shape = [n_class-1, n_features] Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. coef_ is readonly property derived from dual_coef_ and support_vectors_ . intercept_ : array, shape = [n_class * (n_class-1) / 2] Constants in decision function."},
{"libName": "sklearn.svm", "methods": [{"methodName": "__init__(epsilon=0.0, tol=0.0001, C=1.0, loss=epsilon_insensitive, fit_intercept=True, intercept_scaling=1.0, dual=True, verbose=0, random_state=None, max_iter=1000)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the model according to the given training data.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training vector, where n_samples in the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples]   Target vector relative to X   sample_weight : array-like, shape = [n_samples], optional   Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["C", "loss", "epsilon", "dual", "tol", "fit_intercept", "intercept_scaling", "verbose", "random_state", "max_iter"], "notes": "", "funcName": "LinearSVR", "allFuncAttributes": ["coef_", "intercept_"], "funcDesc": "Linear Support Vector Regression.", "funcParamBody": "C : float, optional (default=1.0) Penalty parameter C of the error term. The penalty is a squared l2 penalty. The bigger this parameter, the less regularization is used. loss : string, epsilon_insensitive or squared_epsilon_insensitive (default=epsilon_insensitive) Specifies the loss function. l1 is the epsilon-insensitive loss (standard SVR) while l2 is the squared epsilon-insensitive loss. epsilon : float, optional (default=0.1) Epsilon parameter in the epsilon-insensitive loss function. Note that the value of this parameter depends on the scale of the target variable y. If unsure, set epsilon=0 . dual : bool, (default=True) Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features. tol : float, optional (default=1e-4) Tolerance for stopping criteria. fit_intercept : boolean, optional (default=True) Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be already centered). intercept_scaling : float, optional (default=1) When self.fit_intercept is True, instance vector x becomes [x, self.intercept_scaling], i.e. a synthetic feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. verbose : int, (default=0) Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in liblinear that, if enabled, may not work properly in a multithreaded context. random_state : int, RandomState instance or None, optional (default=None) The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . max_iter : int, (default=1000) The maximum number of iterations to be run.", "funcAttrBody": "coef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features] Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. coef_ is a readonly property derived from raw_coef_ that follows the internal memory layout of liblinear. intercept_ : array, shape = [1] if n_classes == 2 else [n_classes] Constants in decision function."},
{"libName": "sklearn.svm", "methods": [{"methodName": "__init__(penalty=l2, loss=squared_hinge, dual=True, tol=0.0001, C=1.0, multi_class=ovr, fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :   Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted.  ", "methodParams": ["X"], "methodReturns": ["array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"], "methodDesc": "Predict confidence scores for samples. The confidence score for a sample is the signed distance of that sample to the hyperplane.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "densify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to dense array format. Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.", "methodReturnsBody": "self : estimator"}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the model according to the given training data.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training vector, where n_samples in the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples]   Target vector relative to X   sample_weight : array-like, shape = [n_samples], optional   Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples]   Predicted class label per sample.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict class labels for samples in X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "sparsify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to sparse format. Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation. The intercept_ member is not converted. Notes For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "methodReturnsBody": "self : estimator"}], "allFuncParams": ["penalty", "loss", "dual", "tol", "C", "multi_class", "fit_intercept", "intercept_scaling", "class_weight", "verbose", "random_state", "max_iter"], "notes": "The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. The underlying implementation, liblinear, uses a sparse internal representation for the data that will incur a memory copy. Predict output may not match that of standalone liblinear in certain cases. See differences from liblinear in the narrative documentation. LIBLINEAR: A Library for Large Linear Classification For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify. The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. The underlying implementation, liblinear, uses a sparse internal representation for the data that will incur a memory copy. Predict output may not match that of standalone liblinear in certain cases. See differences from liblinear in the narrative documentation. LIBLINEAR: A Library for Large Linear Classification For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "funcName": "LinearSVC", "allFuncAttributes": ["coef_", "intercept_"], "funcDesc": "Linear Support Vector Classification.", "funcParamBody": "penalty : string, l1 or l2 (default=l2) Specifies the norm used in the penalization. The l2 penalty is the standard used in SVC. The l1 leads to coef_ loss : string, hinge or squared_hinge (default=squared_hinge) Specifies the loss function. hinge is the standard SVM loss (used e.g. by the SVC class) while squared_hinge is the square of the hinge loss. dual : bool, (default=True) Select the algorithm to either solve the dual or primal optimization problem. Prefer dual=False when n_samples > n_features. tol : float, optional (default=1e-4) Tolerance for stopping criteria. C : float, optional (default=1.0) Penalty parameter C of the error term. multi_class : string, ovr or crammer_singer (default=ovr) Determines the multi-class strategy if y contains more than two classes. \"ovr\" trains n_classes one-vs-rest classifiers, while \"crammer_singer\" optimizes a joint objective over all classes. While crammer_singer is interesting from a theoretical perspective as it is consistent, it is seldom used in practice as it rarely leads to better accuracy and is more expensive to compute. If \"crammer_singer\" is chosen, the options loss, penalty and dual will be ignored. fit_intercept : boolean, optional (default=True) Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (i.e. data is expected to be already centered). intercept_scaling : float, optional (default=1) When self.fit_intercept is True, instance vector x becomes [x,  self.intercept_scaling] , i.e. a synthetic feature with constant value equals to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling * synthetic feature weight Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. class_weight : {dict, balanced}, optional Set the parameter C of class i to class_weight[i]*C for SVC. If not given, all classes are supposed to have weight one. The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples  /  (n_classes  *  np.bincount(y)) verbose : int, (default=0) Enable verbose output. Note that this setting takes advantage of a per-process runtime setting in liblinear that, if enabled, may not work properly in a multithreaded context. random_state : int, RandomState instance or None, optional (default=None) The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . max_iter : int, (default=1000) The maximum number of iterations to be run.", "funcAttrBody": "coef_ : array, shape = [n_features] if n_classes == 2 else [n_classes, n_features] Weights assigned to the features (coefficients in the primal problem). This is only available in the case of a linear kernel. coef_ is a readonly property derived from raw_coef_ that follows the internal memory layout of liblinear. intercept_ : array, shape = [1] if n_classes == 2 else [n_classes] Constants in decision function."},
{"libName": "sklearn.semi_supervised", "methods": [{"methodName": "__init__(kernel=rbf, gamma=20, n_neighbors=7, alpha=0.2, max_iter=30, tol=0.001, n_jobs=1)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit a semi-supervised label propagation model based All the input data is provided matrix X (labeled and unlabeled) and corresponding label matrix y with a dedicated marker value for unlabeled samples.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   A {n_samples by n_samples} size matrix will be created from this   y : array_like, shape = [n_samples]   n_labeled_samples (unlabeled points are marked as -1) All unlabeled samples will be transductively assigned labels  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array_like, shape = [n_samples]   Predictions for input data  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Performs inductive inference across the model.", "methodParamsBody": "X : array_like, shape = [n_samples, n_features] "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "probabilities : array, shape = [n_samples, n_classes]   Normalized probability distributions across class labels  ", "methodParams": ["X"], "methodReturns": ["probabilities"], "methodDesc": "Predict probability for each possible outcome. Compute the probability estimates for each single sample in X and each possible outcome seen during training (categorical distribution).", "methodParamsBody": "X : array_like, shape = [n_samples, n_features] "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["kernel", "gamma", "n_neighbors", "alpha", "max_iter", "tol", "n_jobs"], "notes": "", "funcName": "LabelSpreading", "allFuncAttributes": ["X_", "classes_", "label_distributions_", "transduction_", "n_iter_"], "funcDesc": "LabelSpreading model for semi-supervised learning", "funcParamBody": "kernel : {knn, rbf, callable} String identifier for kernel function to use or the kernel function itself. Only rbf and knn strings are valid inputs. The function passed should take two inputs, each of shape [n_samples, n_features], and return a [n_samples, n_samples] shaped weight matrix gamma : float parameter for rbf kernel n_neighbors : integer > 0 parameter for knn kernel alpha : float Clamping factor. A value in [0, 1] that specifies the relative amount that an instance should adopt the information from its neighbors as opposed to its initial label. alpha=0 means keeping the initial label information; alpha=1 means replacing all initial information. max_iter : integer maximum number of iterations allowed tol : float Convergence tolerance: threshold to consider the system at steady state n_jobs : int, optional (default = 1) The number of parallel jobs to run. If -1 , then the number of jobs is set to the number of CPU cores.", "funcAttrBody": "X_ : array, shape = [n_samples, n_features] Input array. classes_ : array, shape = [n_classes] The distinct labels used in classifying instances. label_distributions_ : array, shape = [n_samples, n_classes] Categorical distribution for each item. transduction_ : array, shape = [n_samples] Label assigned to each item via the transduction. n_iter_ : int Number of iterations run."},
{"libName": "sklearn.semi_supervised", "methods": [{"methodName": "__init__(kernel=rbf, gamma=20, n_neighbors=7, alpha=None, max_iter=1000, tol=0.001, n_jobs=1)", "methodDesc": ""}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array_like, shape = [n_samples]   Predictions for input data  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Performs inductive inference across the model.", "methodParamsBody": "X : array_like, shape = [n_samples, n_features] "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "probabilities : array, shape = [n_samples, n_classes]   Normalized probability distributions across class labels  ", "methodParams": ["X"], "methodReturns": ["probabilities"], "methodDesc": "Predict probability for each possible outcome. Compute the probability estimates for each single sample in X and each possible outcome seen during training (categorical distribution).", "methodParamsBody": "X : array_like, shape = [n_samples, n_features] "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["kernel", "gamma", "n_neighbors", "alpha", "max_iter", "tol", "n_jobs"], "notes": "", "funcName": "LabelPropagation", "allFuncAttributes": ["X_", "classes_", "label_distributions_", "transduction_", "n_iter_"], "funcDesc": "Label Propagation classifier", "funcParamBody": "kernel : {knn, rbf, callable} String identifier for kernel function to use or the kernel function itself. Only rbf and knn strings are valid inputs. The function passed should take two inputs, each of shape [n_samples, n_features], and return a [n_samples, n_samples] shaped weight matrix. gamma : float Parameter for rbf kernel n_neighbors : integer > 0 Parameter for knn kernel alpha : float Clamping factor. Deprecated since version 0.19: This parameter will be removed in 0.21. alpha is fixed to zero in LabelPropagation. max_iter : integer Change maximum number of iterations allowed tol : float Convergence tolerance: threshold to consider the system at steady state n_jobs : int, optional (default = 1) The number of parallel jobs to run. If -1 , then the number of jobs is set to the number of CPU cores.", "funcAttrBody": "X_ : array, shape = [n_samples, n_features] Input array. classes_ : array, shape = [n_classes] The distinct labels used in classifying instances. label_distributions_ : array, shape = [n_samples, n_classes] Categorical distribution for each item. transduction_ : array, shape = [n_samples] Label assigned to each item via the transduction. n_iter_ : int Number of iterations run."},
{"allReturnParams": ["n_components"], "libName": "sklearn.random_projection", "methods": [], "notes": "", "funcName": "johnson_lindenstrauss_min_dim", "allFuncParams": ["n_samples", "eps"], "funcDesc": "Find a safe number of components to randomly project to", "funcParamBody": "n_samples : int or numpy array of int greater than 0, Number of samples. If an array is given, it will compute a safe number of components array-wise. eps : float or numpy array of float in ]0,1[, optional (default=0.1) Maximum distortion rate as defined by the Johnson-Lindenstrauss lemma. If an array is given, it will compute a safe number of components array-wise.", "funcReturnBody": "n_components : int or numpy array of int, The minimal number of components to guarantee with good probability an eps-embedding with n_samples."},
{"funcName": "SparseRandomProjection", "notes": "", "libName": "sklearn.random_projection", "methods": [{"methodName": "__init__(n_components=auto, density=auto, eps=0.1, dense_output=False, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Generate a sparse random projection matrix", "methodParamsBody": "X : numpy array or scipy.sparse of shape [n_samples, n_features]   Training set: only the shape is used to find optimal random matrix dimensions based on the theory referenced in the afore mentioned papers.   y : is not used: placeholder to allow for usage in a Pipeline. "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : numpy array or scipy sparse of shape [n_samples, n_components]   Projected array.  ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Project the data by using matrix product with the random matrix", "methodParamsBody": "X : numpy array or scipy.sparse of shape [n_samples, n_features]   The input data to project into a smaller dimensional space.  "}], "funcDesc": "Reduce dimensionality through sparse random projection"},
{"libName": "sklearn.random_projection", "methods": [{"methodName": "__init__(n_components=auto, eps=0.1, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Generate a sparse random projection matrix", "methodParamsBody": "X : numpy array or scipy.sparse of shape [n_samples, n_features]   Training set: only the shape is used to find optimal random matrix dimensions based on the theory referenced in the afore mentioned papers.   y : is not used: placeholder to allow for usage in a Pipeline. "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : numpy array or scipy sparse of shape [n_samples, n_components]   Projected array.  ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Project the data by using matrix product with the random matrix", "methodParamsBody": "X : numpy array or scipy.sparse of shape [n_samples, n_features]   The input data to project into a smaller dimensional space.  "}], "allFuncParams": ["n_components", "eps", "random_state"], "notes": "", "funcName": "GaussianRandomProjection", "allFuncAttributes": ["n_component_", "components_"], "funcDesc": "Reduce dimensionality through Gaussian random projection", "funcParamBody": "n_components : int or auto, optional (default = auto) Dimensionality of the target projection space. n_components can be automatically adjusted according to the number of samples in the dataset and the bound given by the Johnson-Lindenstrauss lemma. In that case the quality of the embedding is controlled by the eps parameter. It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset. eps : strictly positive float, optional (default=0.1) Parameter to control the quality of the embedding according to the Johnson-Lindenstrauss lemma when n_components is set to auto. Smaller values lead to better embedding and higher number of dimensions (n_components) in the target projection space. random_state : int, RandomState instance or None, optional (default=None) Control the pseudo random number generator used to generate the matrix at fit time.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcAttrBody": "n_component_ : int Concrete number of components computed when n_components=auto. components_ : numpy array of shape [n_components, n_features] Random matrix used for the projection."},
{"libName": "sklearn.preprocessing", "methods": [], "notes": "This implementation will refuse to center scipy.sparse matrices since it would make them non-sparse and would potentially crash the program with memory exhaustion problems. Instead the caller is expected to either set explicitly with_mean=False (in that case, only variance scaling will be performed on the features of the CSC matrix) or to call X.toarray() if he/she expects the materialized dense array to fit in memory. To avoid memory copy the caller should pass a CSC matrix. For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py . This implementation will refuse to center scipy.sparse matrices since it would make them non-sparse and would potentially crash the program with memory exhaustion problems. Instead the caller is expected to either set explicitly with_mean=False (in that case, only variance scaling will be performed on the features of the CSC matrix) or to call X.toarray() if he/she expects the materialized dense array to fit in memory. To avoid memory copy the caller should pass a CSC matrix. For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py .", "funcName": "scale", "allFuncParams": ["X", "axis", "with_mean", "with_std", "copy"], "funcDesc": "Standardize a dataset along any axis", "funcParamBody": "X : {array-like, sparse matrix} The data to center and scale. axis : int (0 by default) axis used to compute the means and standard deviations along. If 0, independently standardize each feature, otherwise (if 1) standardize each sample. with_mean : boolean, True by default If True, center the data before scaling. with_std : boolean, True by default If True, scale the data to unit variance (or equivalently, unit standard deviation). copy : boolean, optional, default True set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSC matrix and if axis is 1)."},
{"libName": "sklearn.preprocessing", "methods": [], "notes": "This implementation will refuse to center scipy.sparse matrices since it would make them non-sparse and would potentially crash the program with memory exhaustion problems. Instead the caller is expected to either set explicitly with_centering=False (in that case, only variance scaling will be performed on the features of the CSR matrix) or to call X.toarray() if he/she expects the materialized dense array to fit in memory. To avoid memory copy the caller should pass a CSR matrix. For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py .", "funcName": "robust_scale", "allFuncParams": ["X", "axis", "with_centering", "with_scaling", "quantile_range", "copy"], "funcDesc": "Standardize a dataset along any axis", "funcParamBody": "X : array-like The data to center and scale. axis : int (0 by default) axis used to compute the medians and IQR along. If 0, independently scale each feature, otherwise (if 1) scale each sample. with_centering : boolean, True by default If True, center the data before scaling. with_scaling : boolean, True by default If True, scale the data to unit variance (or equivalently, unit standard deviation). quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0 Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR Quantile range used to calculate scale_ . New in version 0.18. copy : boolean, optional, default is True set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix and if axis is 1)."},
{"libName": "sklearn.preprocessing", "methods": [], "allFuncParams": ["X", "axis", "n_quantiles", "output_distribution", "ignore_implicit_zeros", "subsample", "random_state", "copy"], "notes": "For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py .", "funcName": "quantile_transform", "allFuncAttributes": ["quantiles_", "references_"], "funcDesc": "Transform features using quantiles information.", "funcParamBody": "X : array-like, sparse matrix The data to transform. axis : int, (default=0) Axis used to compute the means and standard deviations along. If 0, transform each feature, otherwise (if 1) transform each sample. n_quantiles : int, optional (default=1000) Number of quantiles to be computed. It corresponds to the number of landmarks used to discretize the cumulative density function. output_distribution : str, optional (default=uniform) Marginal distribution for the transformed data. The choices are uniform (default) or normal. ignore_implicit_zeros : bool, optional (default=False) Only applies to sparse matrices. If True, the sparse entries of the matrix are discarded to compute the quantile statistics. If False, these entries are treated as zeros. subsample : int, optional (default=1e5) Maximum number of samples used to estimate the quantiles for computational efficiency. Note that the subsampling procedure may differ for value-identical sparse and dense matrices. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Note that this is used by subsampling and smoothing noise. copy : boolean, optional, (default=True) Set to False to perform inplace transformation and avoid a copy (if the input is already a numpy array).", "funcAttrBody": "quantiles_ : ndarray, shape (n_quantiles, n_features) The values corresponding the quantiles of reference. references_ : ndarray, shape(n_quantiles, ) Quantiles of references."},
{"allReturnParams": ["X", "norms"], "libName": "sklearn.preprocessing", "methods": [], "notes": "For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py .", "funcName": "normalize", "allFuncParams": ["X", "norm", "axis", "copy", "return_norm"], "funcDesc": "Scale input vectors individually to unit norm (vector length).", "funcParamBody": "X : {array-like, sparse matrix}, shape [n_samples, n_features] The data to normalize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy. norm : l1, l2, or max, optional (l2 by default) The norm to use to normalize each non zero sample (or each non-zero feature if axis is 0). axis : 0 or 1, optional (1 by default) axis used to normalize the data along. If 1, independently normalize each sample, otherwise (if 0) normalize each feature. copy : boolean, optional, default True set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix and if axis is 1). return_norm : boolean, default False whether to return the computed norms", "funcReturnBody": "X : {array-like, sparse matrix}, shape [n_samples, n_features] Normalized input X. norms : array, shape [n_samples] if axis=1 else [n_features] An array of norms along given axis for X. When X is sparse, a NotImplementedError will be raised for norm l1 or l2."},
{"libName": "sklearn.preprocessing", "methods": [], "notes": "For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py . For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py .", "funcName": "minmax_scale", "allFuncParams": ["X", "feature_range", "axis", "copy"], "funcDesc": "Transforms features by scaling each feature to a given range.", "funcParamBody": "X : array-like, shape (n_samples, n_features) The data. feature_range : tuple (min, max), default=(0, 1) Desired range of transformed data. axis : int (0 by default) axis used to scale along. If 0, independently scale each feature, otherwise (if 1) scale each sample. copy : boolean, optional, default is True Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy array)."},
{"libName": "sklearn.preprocessing", "methods": [], "notes": "For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py .", "funcName": "maxabs_scale", "allFuncParams": ["X", "axis", "copy"], "funcDesc": "Scale each feature to the [-1, 1] range without breaking the sparsity.", "funcParamBody": "X : array-like, shape (n_samples, n_features) The data. axis : int (0 by default) axis used to scale along. If 0, independently scale each feature, otherwise (if 1) scale each sample. copy : boolean, optional, default is True Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy array)."},
{"allReturnParams": ["Y"], "libName": "sklearn.preprocessing", "methods": [], "notes": "", "funcName": "label_binarize", "allFuncParams": ["y", "classes", "neg_label", "pos_label", "sparse_output"], "funcDesc": "Binarize labels in a one-vs-all fashion", "funcParamBody": "y : array-like Sequence of integer labels or multilabel data to encode. classes : array-like of shape [n_classes] Uniquely holds the label for each class. neg_label : int (default: 0) Value with which negative labels must be encoded. pos_label : int (default: 1) Value with which positive labels must be encoded. sparse_output : boolean (default: False), Set to true if output binary array is desired in CSR sparse format", "funcReturnBody": "Y : numpy array or CSR matrix of shape [n_samples, n_classes] Shape will be [n_samples, 1] for binary problems."},
{"libName": "sklearn.preprocessing", "methods": [], "notes": "", "funcName": "binarize", "allFuncParams": ["X", "threshold", "copy"], "funcDesc": "Boolean thresholding of array-like or scipy.sparse matrix", "funcParamBody": "X : {array-like, sparse matrix}, shape [n_samples, n_features] The data to binarize, element by element. scipy.sparse matrices should be in CSR or CSC format to avoid an un-necessary copy. threshold : float, optional (0.0 by default) Feature values below or equal to this are replaced by 0, above it by 1. Threshold may not be less than 0 for operations on sparse matrices. copy : boolean, optional, default True set to False to perform inplace binarization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR / CSC matrix and if axis is 1)."},
{"allReturnParams": ["X"], "libName": "sklearn.preprocessing", "methods": [], "notes": "", "funcName": "add_dummy_feature", "allFuncParams": ["X", "value"], "funcDesc": "Augment dataset with an additional dummy feature.", "funcParamBody": "X : {array-like, sparse matrix}, shape [n_samples, n_features] Data. value : float Value to use for the dummy feature.", "funcReturnBody": "X : {array, sparse matrix}, shape [n_samples, n_features + 1] Same data with dummy feature added as first column."},
{"libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__(copy=True, with_mean=True, with_std=True)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : {array-like, sparse matrix}, shape [n_samples, n_features]   The data used to compute the mean and standard deviation used for later scaling along the features axis.   y : Passthrough for Pipeline compatibility. ", "methodDesc": "Compute the mean and std to be used for later scaling."}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "inverse_transform(X, copy=None)", "methodReturnsBody": "X_tr : array-like, shape [n_samples, n_features]   Transformed array.  ", "methodParams": ["X", "copy"], "methodReturns": ["X_tr"], "methodDesc": "Scale back the data to the original representation", "methodParamsBody": "X : array-like, shape [n_samples, n_features]   The data used to scale along the features axis.   copy : bool, optional (default: None)   Copy the input X or not.  "}, {"methodParams": ["X", "y"], "methodName": "partial_fit(X, y=None)", "methodParamsBody": "X : {array-like, sparse matrix}, shape [n_samples, n_features]   The data used to compute the mean and standard deviation used for later scaling along the features axis.   y : Passthrough for Pipeline compatibility. ", "methodDesc": "Online computation of mean and std on X for later scaling. All of X is processed as a single batch. This is intended for cases when fit is not feasible due to very large number of n_samples or because X is read from a continuous stream. The algorithm for incremental mean and std is given in Equation 1.5a,b in Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. Algorithms for computing the sample variance: Analysis and recommendations. The American Statistician 37.3 (1983): 242-247:"}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodParams": ["X", "y", "copy"], "methodName": "transform(X, y=deprecated, copy=None)", "methodParamsBody": "X : array-like, shape [n_samples, n_features]   The data used to scale along the features axis.   y : (ignored)    Deprecated since version 0.19: This parameter will be removed in 0.21.    copy : bool, optional (default: None)   Copy the input X or not.  ", "methodDesc": "Perform standardization by centering and scaling"}], "allFuncParams": ["copy", "with_mean", "with_std"], "notes": "For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py . For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py .", "funcName": "StandardScaler", "allFuncAttributes": ["scale_", "mean_", "var_", "n_samples_seen_"], "funcDesc": "Standardize features by removing the mean and scaling to unit variance", "funcParamBody": "copy : boolean, optional, default True If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned. with_mean : boolean, True by default If True, center the data before scaling. This does not work (and will raise an exception) when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. with_std : boolean, True by default If True, scale the data to unit variance (or equivalently, unit standard deviation).", "funcAttrBody": "scale_ : ndarray, shape (n_features,) Per feature relative scaling of the data. New in version 0.17: scale_ mean_ : array of floats with shape [n_features] The mean value for each feature in the training set. var_ : array of floats with shape [n_features] The variance for each feature in the training set. Used to compute scale_ n_samples_seen_ : int The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across partial_fit calls."},
{"libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0), copy=True)", "methodDesc": ""}, {"methodParams": ["X"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like, shape [n_samples, n_features]   The data used to compute the median and quantiles used for later scaling along the features axis.  ", "methodDesc": "Compute the median and quantiles to be used for scaling."}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodParams": ["X"], "methodName": "inverse_transform(X)", "methodParamsBody": "X : array-like   The data used to scale along the specified axis.  ", "methodDesc": "Scale back the data to the original representation"}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodParams": ["X"], "methodName": "transform(X)", "methodParamsBody": "X : {array-like, sparse matrix}   The data used to scale along the specified axis.  ", "methodDesc": "Center and scale the data. Can be called on sparse input, provided that RobustScaler has been fitted to dense input and with_centering=False ."}], "allFuncParams": ["with_centering", "with_scaling", "quantile_range", "copy"], "notes": "For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py . https://en.wikipedia.org/wiki/Median_(statistics ) https://en.wikipedia.org/wiki/Interquartile_range For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py . https://en.wikipedia.org/wiki/Median_(statistics ) https://en.wikipedia.org/wiki/Interquartile_range", "funcName": "RobustScaler", "allFuncAttributes": ["center_", "scale_"], "funcDesc": "Scale features using statistics that are robust to outliers.", "funcParamBody": "with_centering : boolean, True by default If True, center the data before scaling. This will cause transform to raise an exception when attempted on sparse matrices, because centering them entails building a dense matrix which in common use cases is likely to be too large to fit in memory. with_scaling : boolean, True by default If True, scale the data to interquartile range. quantile_range : tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0 Default: (25.0, 75.0) = (1st quantile, 3rd quantile) = IQR Quantile range used to calculate scale_ . New in version 0.18. copy : boolean, optional, default is True If False, try to avoid a copy and do inplace scaling instead. This is not guaranteed to always work inplace; e.g. if the data is not a NumPy array or scipy.sparse CSR matrix, a copy may still be returned.", "funcAttrBody": "center_ : array of floats The median value for each feature in the training set. scale_ : array of floats The (scaled) interquartile range for each feature in the training set. New in version 0.17: scale_ attribute."},
{"libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__(n_quantiles=1000, output_distribution=uniform, ignore_implicit_zeros=False, subsample=100000, random_state=None, copy=True)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns self  ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Compute the quantiles used for transforming.", "methodParamsBody": "X : ndarray or sparse matrix, shape (n_samples, n_features)   The data used to scale along the features axis. If a sparse matrix is provided, it will be converted into a sparse csc_matrix . Additionally, the sparse matrix needs to be nonnegative if ignore_implicit_zeros is False.  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "Xt : ndarray or sparse matrix, shape (n_samples, n_features)   The projected data.  ", "methodParams": ["X"], "methodReturns": ["Xt"], "methodDesc": "Back-projection to the original space.", "methodParamsBody": "X : ndarray or sparse matrix, shape (n_samples, n_features)   The data used to scale along the features axis. If a sparse matrix is provided, it will be converted into a sparse csc_matrix . Additionally, the sparse matrix needs to be nonnegative if ignore_implicit_zeros is False.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "Xt : ndarray or sparse matrix, shape (n_samples, n_features)   The projected data.  ", "methodParams": ["X"], "methodReturns": ["Xt"], "methodDesc": "Feature-wise transformation of the data.", "methodParamsBody": "X : ndarray or sparse matrix, shape (n_samples, n_features)   The data used to scale along the features axis. If a sparse matrix is provided, it will be converted into a sparse csc_matrix . Additionally, the sparse matrix needs to be nonnegative if ignore_implicit_zeros is False.  "}], "allFuncParams": ["n_quantiles", "output_distribution", "ignore_implicit_zeros", "subsample", "random_state", "copy"], "notes": "For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py . For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py .", "funcName": "QuantileTransformer", "allFuncAttributes": ["quantiles_", "references_"], "funcDesc": "Transform features using quantiles information.", "funcParamBody": "n_quantiles : int, optional (default=1000) Number of quantiles to be computed. It corresponds to the number of landmarks used to discretize the cumulative density function. output_distribution : str, optional (default=uniform) Marginal distribution for the transformed data. The choices are uniform (default) or normal. ignore_implicit_zeros : bool, optional (default=False) Only applies to sparse matrices. If True, the sparse entries of the matrix are discarded to compute the quantile statistics. If False, these entries are treated as zeros. subsample : int, optional (default=1e5) Maximum number of samples used to estimate the quantiles for computational efficiency. Note that the subsampling procedure may differ for value-identical sparse and dense matrices. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random. Note that this is used by subsampling and smoothing noise. copy : boolean, optional, (default=True) Set to False to perform inplace transformation and avoid a copy (if the input is already a numpy array).", "funcAttrBody": "quantiles_ : ndarray, shape (n_quantiles, n_features) The values corresponding the quantiles of reference. references_ : ndarray, shape(n_quantiles, ) Quantiles of references."},
{"libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__(degree=2, interaction_only=False, include_bias=True)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : instance ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Compute number of output features.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   The data.  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_feature_names(input_features=None)", "methodReturnsBody": "output_feature_names : list of string, length n_output_features ", "methodParams": ["input_features"], "methodReturns": ["output_feature_names"], "methodDesc": "Return feature names for output features", "methodParamsBody": "input_features : list of string, length n_features, optional   String names for input features if available. By default, x0, x1, ... xn_features is used.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "XP : np.ndarray shape [n_samples, NP]   The matrix of features, where NP is the number of polynomial features generated from the combination of inputs.  ", "methodParams": ["X"], "methodReturns": ["XP"], "methodDesc": "Transform data to polynomial features", "methodParamsBody": "X : array-like, shape [n_samples, n_features]   The data to transform, row by row.  "}], "allFuncParams": ["degree", "interaction_only", "include_bias"], "notes": "Be aware that the number of features in the output array scales polynomially in the number of features of the input array, and exponentially in the degree. High degrees can cause overfitting. See examples/linear_model/plot_polynomial_interpolation.py Be aware that the number of features in the output array scales polynomially in the number of features of the input array, and exponentially in the degree. High degrees can cause overfitting. See examples/linear_model/plot_polynomial_interpolation.py", "funcName": "PolynomialFeatures", "allFuncAttributes": ["powers_", "n_input_features_", "n_output_features_"], "funcDesc": "Generate polynomial and interaction features.", "funcParamBody": "degree : integer The degree of the polynomial features. Default = 2. interaction_only : boolean, default = False If true, only interaction features are produced: features that are products of at most degree  distinct input features (so not x[1]  **  2 , x[0]  *  x[2]  **  3 , etc.). include_bias : boolean If True (default), then include a bias column, the feature in which all polynomial powers are zero (i.e. a column of ones - acts as an intercept term in a linear model).", "funcAttrBody": "powers_ : array, shape (n_output_features, n_input_features) powers_[i, j] is the exponent of the jth input in the ith output. n_input_features_ : int The total number of input features. n_output_features_ : int The total number of polynomial output features. The number of output features is computed by iterating over all suitably sized combinations of input features."},
{"libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__(n_values=auto, categorical_features=all, dtype=<class numpy.float64>, sparse=True, handle_unknown=error)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Fit OneHotEncoder to X.", "methodParamsBody": "X : array-like, shape [n_samples, n_feature]   Input array of type int.  "}, {"methodParams": ["X"], "methodName": "fit_transform(X, y=None)", "methodParamsBody": "X : array-like, shape [n_samples, n_feature]   Input array of type int.  ", "methodDesc": "Fit OneHotEncoder to X, then transform X. Equivalent to self.fit(X).transform(X), but more convenient and more efficient. See fit for the parameters, transform for the return value."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_out : sparse matrix if sparse=True else a 2-d array, dtype=int   Transformed input.  ", "methodParams": ["X"], "methodReturns": ["X_out"], "methodDesc": "Transform X using one-hot encoding.", "methodParamsBody": "X : array-like, shape [n_samples, n_features]   Input array of type int.  "}], "allFuncParams": ["n_values", "categorical_features", "dtype", "sparse", "handle_unknown"], "notes": "", "funcName": "OneHotEncoder", "allFuncAttributes": ["active_features_", "feature_indices_", "n_values_"], "funcDesc": "Encode categorical integer features using a one-hot aka one-of-K scheme.", "funcParamBody": "n_values : auto, int or array of ints Number of values per feature. auto : determine value range from training data. int :  number of categorical values per feature. Each feature value should be in range(n_values) array :  n_values[i] is the number of categorical values in X[:,  i] . Each feature value should be in range(n_values[i]) categorical_features : all or array of indices or mask Specify what features are treated as categorical. all (default): All features are treated as categorical. array of indices: Array of categorical feature indices. mask: Array of length n_features and with dtype=bool. Non-categorical features are always stacked to the right of the matrix. dtype : number type, default=np.float Desired dtype of output. sparse : boolean, default=True Will return sparse matrix if set True else will return an array. handle_unknown : str, error or ignore Whether to raise an error or ignore if a unknown categorical feature is present during transform.", "funcAttrBody": "active_features_ : array Indices for active features, meaning values that actually occur in the training set. Only available when n_values is 'auto' . feature_indices_ : array of shape (n_features,) Indices to feature ranges. Feature i in the original data is mapped to features from feature_indices_[i] to feature_indices_[i+1] active_features_ afterwards) n_values_ : array of shape (n_features,) Maximum number of values per feature."},
{"libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__(norm=l2, copy=True)", "methodDesc": ""}, {"methodParams": [], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like", "methodDesc": "Do nothing and return the estimator unchanged This method is just there to implement the usual API and hence work in pipelines."}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodParams": ["X", "y", "copy"], "methodName": "transform(X, y=deprecated, copy=None)", "methodParamsBody": "X : {array-like, sparse matrix}, shape [n_samples, n_features]   The data to normalize, row by row. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.   y : (ignored)    Deprecated since version 0.19: This parameter will be removed in 0.21.    copy : bool, optional (default: None)   Copy the input X or not.  ", "methodDesc": "Scale each non zero row of X to unit norm"}], "notes": "This estimator is stateless (besides constructor parameters), the fit method does nothing but is useful when used in a pipeline. For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py . This estimator is stateless (besides constructor parameters), the fit method does nothing but is useful when used in a pipeline. For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py .", "funcName": "Normalizer", "allFuncParams": ["norm", "copy"], "funcDesc": "Normalize samples individually to unit norm.", "funcParamBody": "norm : l1, l2, or max, optional (l2 by default) The norm to use to normalize each non zero sample. copy : boolean, optional, default True set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix)."},
{"libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__(feature_range=(0, 1), copy=True)", "methodDesc": ""}, {"methodParams": ["X"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like, shape [n_samples, n_features]   The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.  ", "methodDesc": "Compute the minimum and maximum to be used for later scaling."}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodParams": ["X"], "methodName": "inverse_transform(X)", "methodParamsBody": "X : array-like, shape [n_samples, n_features]   Input data that will be transformed. It cannot be sparse.  ", "methodDesc": "Undo the scaling of X according to feature_range."}, {"methodParams": ["X", "y"], "methodName": "partial_fit(X, y=None)", "methodParamsBody": "X : array-like, shape [n_samples, n_features]   The data used to compute the mean and standard deviation used for later scaling along the features axis.   y : Passthrough for Pipeline compatibility. ", "methodDesc": "Online computation of min and max on X for later scaling. All of X is processed as a single batch. This is intended for cases when fit is not feasible due to very large number of n_samples or because X is read from a continuous stream."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodParams": ["X"], "methodName": "transform(X)", "methodParamsBody": "X : array-like, shape [n_samples, n_features]   Input data that will be transformed.  ", "methodDesc": "Scaling features of X according to feature_range."}], "allFuncParams": ["feature_range", "copy"], "notes": "For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py . For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py .", "funcName": "MinMaxScaler", "allFuncAttributes": ["min_", "scale_", "data_min_", "data_max_", "data_range_"], "funcDesc": "Transforms features by scaling each feature to a given range.", "funcParamBody": "feature_range : tuple (min, max), default=(0, 1) Desired range of transformed data. copy : boolean, optional, default True Set to False to perform inplace row normalization and avoid a copy (if the input is already a numpy array).", "funcAttrBody": "min_ : ndarray, shape (n_features,) Per feature adjustment for minimum. scale_ : ndarray, shape (n_features,) Per feature relative scaling of the data. New in version 0.17: scale_ attribute. data_min_ : ndarray, shape (n_features,) Per feature minimum seen in the data New in version 0.17: data_min_ data_max_ : ndarray, shape (n_features,) Per feature maximum seen in the data New in version 0.17: data_max_ data_range_ : ndarray, shape (n_features,) Per feature range (data_max_  -  data_min_) seen in the data New in version 0.17: data_range_"},
{"libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__(copy=True)", "methodDesc": ""}, {"methodParams": ["X"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : {array-like, sparse matrix}, shape [n_samples, n_features]   The data used to compute the per-feature minimum and maximum used for later scaling along the features axis.  ", "methodDesc": "Compute the maximum absolute value to be used for later scaling."}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodParams": ["X"], "methodName": "inverse_transform(X)", "methodParamsBody": "X : {array-like, sparse matrix}   The data that should be transformed back.  ", "methodDesc": "Scale back the data to the original representation"}, {"methodParams": ["X", "y"], "methodName": "partial_fit(X, y=None)", "methodParamsBody": "X : {array-like, sparse matrix}, shape [n_samples, n_features]   The data used to compute the mean and standard deviation used for later scaling along the features axis.   y : Passthrough for Pipeline compatibility. ", "methodDesc": "Online computation of max absolute value of X for later scaling. All of X is processed as a single batch. This is intended for cases when fit is not feasible due to very large number of n_samples or because X is read from a continuous stream."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodParams": ["X"], "methodName": "transform(X)", "methodParamsBody": "X : {array-like, sparse matrix}   The data that should be scaled.  ", "methodDesc": "Scale the data"}], "allFuncParams": ["copy"], "notes": "For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py . For a comparison of the different scalers, transformers, and normalizers, see examples/preprocessing/plot_all_scaling.py .", "funcName": "MaxAbsScaler", "allFuncAttributes": ["scale_", "max_abs_", "n_samples_seen_"], "funcDesc": "Scale each feature by its maximum absolute value.", "funcParamBody": "copy : boolean, optional, default is True Set to False to perform inplace scaling and avoid a copy (if the input is already a numpy array).", "funcAttrBody": "scale_ : ndarray, shape (n_features,) Per feature relative scaling of the data. New in version 0.17: scale_ attribute. max_abs_ : ndarray, shape (n_features,) Per feature maximum absolute value. n_samples_seen_ : int The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across partial_fit calls."},
{"libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__(classes=None, sparse_output=False)", "methodDesc": ""}, {"methodName": "fit(y)", "methodReturnsBody": "self : returns this MultiLabelBinarizer instance ", "methodParams": ["y"], "methodReturns": ["self"], "methodDesc": "Fit the label sets binarizer, storing classes_", "methodParamsBody": "y : iterable of iterables   A set of labels (any orderable and hashable object) for each sample. If the classes parameter is set, y will not be iterated.  "}, {"methodName": "fit_transform(y)", "methodReturnsBody": "y_indicator : array or CSR matrix, shape (n_samples, n_classes)   A matrix such that y_indicator[i, j] = 1 iff classes_[j] is in y[i] , and 0 otherwise.  ", "methodParams": ["y"], "methodReturns": ["y_indicator"], "methodDesc": "Fit the label sets binarizer and transform the given label sets", "methodParamsBody": "y : iterable of iterables   A set of labels (any orderable and hashable object) for each sample. If the classes parameter is set, y will not be iterated.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "inverse_transform(yt)", "methodReturnsBody": "y : list of tuples   The set of labels for each sample such that y[i] consists of classes_[j] for each yt[i, j] == 1 .  ", "methodParams": ["yt"], "methodReturns": ["y"], "methodDesc": "Transform the given indicator matrix into label sets", "methodParamsBody": "yt : array or sparse matrix of shape (n_samples, n_classes)   A matrix containing only 1s ands 0s.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(y)", "methodReturnsBody": "y_indicator : array or CSR matrix, shape (n_samples, n_classes)   A matrix such that y_indicator[i, j] = 1 iff classes_[j] is in y[i] , and 0 otherwise.  ", "methodParams": ["y"], "methodReturns": ["y_indicator"], "methodDesc": "Transform the given label sets", "methodParamsBody": "y : iterable of iterables   A set of labels (any orderable and hashable object) for each sample. If the classes parameter is set, y will not be iterated.  "}], "allFuncParams": ["classes", "sparse_output"], "notes": "", "funcName": "MultiLabelBinarizer", "allFuncAttributes": ["classes_"], "funcDesc": "Transform between iterable of iterables and a multilabel format", "funcParamBody": "classes : array-like of shape [n_classes] (optional) Indicates an ordering for the class labels sparse_output : boolean (default: False), Set to true if output binary array is desired in CSR sparse format", "funcAttrBody": "classes_ : array of labels A copy of the classes parameter where provided, or otherwise, the sorted set of classes found when fitting."},
{"libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__()", "methodDesc": "Initialize self.  See help(type(self)) for accurate signature."}, {"methodName": "fit(y)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["y"], "methodReturns": ["self"], "methodDesc": "Fit label encoder", "methodParamsBody": "y : array-like of shape (n_samples,)   Target values.  "}, {"methodName": "fit_transform(y)", "methodReturnsBody": "y : array-like of shape [n_samples] ", "methodParams": ["y"], "methodReturns": ["y"], "methodDesc": "Fit label encoder and return encoded labels", "methodParamsBody": "y : array-like of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "inverse_transform(y)", "methodReturnsBody": "y : numpy array of shape [n_samples] ", "methodParams": ["y"], "methodReturns": ["y"], "methodDesc": "Transform labels back to original encoding.", "methodParamsBody": "y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(y)", "methodReturnsBody": "y : array-like of shape [n_samples] ", "methodParams": ["y"], "methodReturns": ["y"], "methodDesc": "Transform labels to normalized encoding.", "methodParamsBody": "y : array-like of shape [n_samples]   Target values.  "}], "notes": "", "funcName": "LabelEncoder", "allFuncAttributes": ["classes_"], "funcDesc": "Encode labels with value between 0 and n_classes-1.", "funcAttrBody": "classes_ : array of shape (n_class,) Holds the label for each class."},
{"libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__(neg_label=0, pos_label=1, sparse_output=False)", "methodDesc": ""}, {"methodName": "fit(y)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["y"], "methodReturns": ["self"], "methodDesc": "Fit label binarizer", "methodParamsBody": "y : array of shape [n_samples,] or [n_samples, n_classes]   Target values. The 2-d matrix should only contain 0 and 1, represents multilabel classification.  "}, {"methodName": "fit_transform(y)", "methodReturnsBody": "Y : array or CSR matrix of shape [n_samples, n_classes]   Shape will be [n_samples, 1] for binary problems.  ", "methodParams": ["y"], "methodReturns": ["Y"], "methodDesc": "Fit label binarizer and transform multi-class labels to binary labels. The output of transform is sometimes referred to    as the 1-of-K coding scheme.", "methodParamsBody": "y : array or sparse matrix of shape [n_samples,] or             [n_samples, n_classes]   Target values. The 2-d matrix should only contain 0 and 1, represents multilabel classification. Sparse matrix can be CSR, CSC, COO, DOK, or LIL.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "inverse_transform(Y, threshold=None)", "methodReturnsBody": "y : numpy array or CSR matrix of shape [n_samples] Target values. ", "methodParams": ["Y", "threshold"], "methodReturns": ["y"], "methodDesc": "Transform binary labels back to multi-class labels Notes In the case when the binary labels are fractional (probabilistic), inverse_transform chooses the class with the greatest value. Typically, this allows to use the output of a linear models decision_function method directly as the input of inverse_transform.", "methodParamsBody": "Y : numpy array or sparse matrix with shape [n_samples, n_classes]   Target values. All sparse matrices are converted to CSR before inverse transformation.   threshold : float or None   Threshold used in the binary and multi-label cases.  Use 0 when Y contains the output of decision_function (classifier). Use 0.5 when Y contains the output of predict_proba.  If None, the threshold is assumed to be half way between neg_label and pos_label.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(y)", "methodReturnsBody": "Y : numpy array or CSR matrix of shape [n_samples, n_classes]   Shape will be [n_samples, 1] for binary problems.  ", "methodParams": ["y"], "methodReturns": ["Y"], "methodDesc": "Transform multi-class labels to binary labels The output of transform is sometimes referred to by some authors as the 1-of-K coding scheme.", "methodParamsBody": "y : array or sparse matrix of shape [n_samples,] or             [n_samples, n_classes]   Target values. The 2-d matrix should only contain 0 and 1, represents multilabel classification. Sparse matrix can be CSR, CSC, COO, DOK, or LIL.  "}], "allFuncParams": ["neg_label", "pos_label", "sparse_output"], "notes": "In the case when the binary labels are fractional (probabilistic), inverse_transform chooses the class with the greatest value. Typically, this allows to use the output of a linear models decision_function method directly as the input of inverse_transform.", "funcName": "LabelBinarizer", "allFuncAttributes": ["classes_", "y_type_", "sparse_input_"], "funcDesc": "Binarize labels in a one-vs-all fashion", "funcParamBody": "neg_label : int (default: 0) Value with which negative labels must be encoded. pos_label : int (default: 1) Value with which positive labels must be encoded. sparse_output : boolean (default: False) True if the returned array from transform is desired to be in sparse CSR format.", "funcAttrBody": "classes_ : array of shape [n_class] Holds the label for each class. y_type_ : str, Represents the type of the target data as evaluated by utils.multiclass.type_of_target. Possible type are continuous, continuous-multioutput, binary, multiclass, multiclass-multioutput, multilabel-indicator, and unknown. sparse_input_ : boolean, True if the input data to transform is given as a sparse matrix, False otherwise."},
{"funcName": "KernelCenterer", "notes": "", "libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__()", "methodDesc": "Initialize self.  See help(type(self)) for accurate signature."}, {"methodName": "fit(K, y=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["K"], "methodReturns": ["self"], "methodDesc": "Fit KernelCenterer", "methodParamsBody": "K : numpy array of shape [n_samples, n_samples]   Kernel matrix.  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(K, y=deprecated, copy=True)", "methodReturnsBody": "K_new : numpy array of shape [n_samples1, n_samples2] ", "methodParams": ["K", "y", "copy"], "methodReturns": ["K_new"], "methodDesc": "Center kernel matrix.", "methodParamsBody": "K : numpy array of shape [n_samples1, n_samples2]   Kernel matrix.   y : (ignored)    Deprecated since version 0.19: This parameter will be removed in 0.21.    copy : boolean, optional, default True   Set to False to perform inplace computation.  "}], "funcDesc": "Center a kernel matrix"},
{"libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__(missing_values=NaN, strategy=mean, axis=0, verbose=0, copy=True)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : Imputer   Returns self.  ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Fit the imputer on X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Input data, where n_samples is the number of samples and n_features is the number of features.  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodParams": ["X"], "methodName": "transform(X)", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   The input data to complete.  ", "methodDesc": "Impute all missing values in X."}], "allFuncParams": ["missing_values", "strategy", "axis", "verbose", "copy"], "notes": "", "funcName": "Imputer", "allFuncAttributes": ["statistics_"], "funcDesc": "Imputation transformer for completing missing values.", "funcParamBody": "missing_values : integer or NaN, optional (default=NaN) The placeholder for the missing values. All occurrences of missing_values will be imputed. For missing values encoded as np.nan, use the string value NaN. strategy : string, optional (default=mean) The imputation strategy. If mean, then replace missing values using the mean along the axis. If median, then replace missing values using the median along the axis. If most_frequent, then replace missing using the most frequent value along the axis. axis : integer, optional (default=0) The axis along which to impute. If axis=0 , then impute along columns. If axis=1 , then impute along rows. verbose : integer, optional (default=0) Controls the verbosity of the imputer. copy : boolean, optional (default=True) If True, a copy of X will be created. If False, imputation will be done in-place whenever possible. Note that, in the following cases, a new copy will always be made, even if copy=False : If X is not an array of floating values; If X is sparse and missing_values=0 ; If axis=0 and X is encoded as a CSR matrix; If axis=1 and X is encoded as a CSC matrix.", "funcAttrBody": "statistics_ : array of shape (n_features,) The imputation fill value for each feature if axis == 0."},
{"libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__(func=None, inverse_func=None, validate=True, accept_sparse=False, pass_y=deprecated, kw_args=None, inv_kw_args=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Fit transformer by checking X. If validate is True , X will be checked.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Input array.  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "inverse_transform(X, y=deprecated)", "methodReturnsBody": "X_out : array-like, shape (n_samples, n_features)   Transformed input.  ", "methodParams": ["X", "y"], "methodReturns": ["X_out"], "methodDesc": "Transform X using the inverse function.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Input array.   y : (ignored)   "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X, y=deprecated)", "methodReturnsBody": "X_out : array-like, shape (n_samples, n_features)   Transformed input.  ", "methodParams": ["X", "y"], "methodReturns": ["X_out"], "methodDesc": "Transform X using the forward function.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Input array.   y : (ignored)   "}], "notes": "", "funcName": "FunctionTransformer", "allFuncParams": ["func", "inverse_func", "validate", "accept_sparse", "pass_y", "kw_args", "inv_kw_args"], "funcDesc": "Constructs a transformer from an arbitrary callable.", "funcParamBody": "func : callable, optional default=None The callable to use for the transformation. This will be passed the same arguments as transform, with args and kwargs forwarded. If func is None, then func will be the identity function. inverse_func : callable, optional default=None The callable to use for the inverse transformation. This will be passed the same arguments as inverse transform, with args and kwargs forwarded. If inverse_func is None, then inverse_func will be the identity function. validate : bool, optional default=True Indicate that the input X array should be checked before calling func. If validate is false, there will be no input validation. If it is true, then X will be converted to a 2-dimensional NumPy array or sparse matrix. If this conversion is not possible or X contains NaN or infinity, an exception is raised. accept_sparse : boolean, optional Indicate that func accepts a sparse matrix as input. If validate is False, this has no effect. Otherwise, if accept_sparse is false, sparse matrix inputs will cause an exception to be raised. pass_y : bool, optional default=False Indicate that transform should forward the y argument to the inner callable. kw_args : dict, optional Dictionary of additional keyword arguments to pass to func. inv_kw_args : dict, optional Dictionary of additional keyword arguments to pass to inverse_func."},
{"libName": "sklearn.preprocessing", "methods": [{"methodName": "__init__(threshold=0.0, copy=True)", "methodDesc": ""}, {"methodParams": [], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like", "methodDesc": "Do nothing and return the estimator unchanged This method is just there to implement the usual API and hence work in pipelines."}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodParams": ["X", "y", "copy"], "methodName": "transform(X, y=deprecated, copy=None)", "methodParamsBody": "X : {array-like, sparse matrix}, shape [n_samples, n_features]   The data to binarize, element by element. scipy.sparse matrices should be in CSR format to avoid an un-necessary copy.   y : (ignored)    Deprecated since version 0.19: This parameter will be removed in 0.21.    copy : bool   Copy the input X or not.  ", "methodDesc": "Binarize each element of X"}], "notes": "If the input is a sparse matrix, only the non-zero values are subject to update by the Binarizer class. This estimator is stateless (besides constructor parameters), the fit method does nothing but is useful when used in a pipeline.", "funcName": "Binarizer", "allFuncParams": ["threshold", "copy"], "funcDesc": "Binarize data (set feature values to 0 or 1) according to a threshold", "funcParamBody": "threshold : float, optional (0.0 by default) Feature values below or equal to this are replaced by 0, above it by 1. Threshold may not be less than 0 for operations on sparse matrices. copy : boolean, optional, default True set to False to perform inplace binarization and avoid a copy (if the input is already a numpy array or a scipy.sparse CSR matrix)."},
{"allReturnParams": ["f"], "libName": "sklearn.pipeline", "methods": [], "notes": "", "funcName": "make_union", "allFuncParams": ["*transformers", "n_jobs"], "funcDesc": "Construct a FeatureUnion from the given transformers.", "funcParamBody": "*transformers : list of estimators n_jobs : int, optional Number of jobs to run in parallel (default 1).", "funcReturnBody": "f : FeatureUnion"},
{"allReturnParams": ["p"], "libName": "sklearn.pipeline", "methods": [], "notes": "", "funcName": "make_pipeline", "allFuncParams": ["*steps", "memory"], "funcDesc": "Construct a Pipeline from the given estimators.", "funcParamBody": "*steps : list of estimators, memory : None, str or object with the joblib.Memory interface, optional Used to cache the fitted transformers of the pipeline. By default, no caching is performed. If a string is given, it is the path to the caching directory. Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. Use the attribute named_steps or steps to inspect estimators within the pipeline. Caching the transformers is advantageous when fitting is time consuming.", "funcReturnBody": "p : Pipeline"},
{"libName": "sklearn.pipeline", "methods": [{"methodName": "__init__(steps, memory=None)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "y_score : array-like, shape = [n_samples, n_classes] ", "methodParams": ["X"], "methodReturns": ["y_score"], "methodDesc": "Apply transforms, and decision_function of the final estimator", "methodParamsBody": "X : iterable   Data to predict on. Must fulfill input requirements of first step of the pipeline.  "}, {"methodName": "fit(X, y=None, **fit_params)", "methodReturnsBody": "self : Pipeline   This estimator  ", "methodParams": ["X", "y", "**fit_params"], "methodReturns": ["self"], "methodDesc": "Fit the model Fit all the transforms one after the other and transform the data, then fit the transformed data using the final estimator.", "methodParamsBody": "X : iterable   Training data. Must fulfill input requirements of first step of the pipeline.   y : iterable, default=None   Training targets. Must fulfill label requirements for all steps of the pipeline.   **fit_params : dict of string -> object   Parameters passed to the fit method of each step, where each parameter name is prefixed such that parameter p for step s has key s__p .  "}, {"methodName": "fit_predict(X, y=None, **fit_params)", "methodReturnsBody": "y_pred : array-like ", "methodParams": ["X", "y", "**fit_params"], "methodReturns": ["y_pred"], "methodDesc": "Applies fit_predict of last step in pipeline after transforms. Applies fit_transforms of a pipeline to the data, followed by the fit_predict method of the final estimator in the pipeline. Valid only if the final estimator implements fit_predict.", "methodParamsBody": "X : iterable   Training data. Must fulfill input requirements of first step of the pipeline.   y : iterable, default=None   Training targets. Must fulfill label requirements for all steps of the pipeline.   **fit_params : dict of string -> object   Parameters passed to the fit method of each step, where each parameter name is prefixed such that parameter p for step s has key s__p .  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "Xt : array-like, shape = [n_samples, n_transformed_features]   Transformed samples  ", "methodParams": ["X", "y", "**fit_params"], "methodReturns": ["Xt"], "methodDesc": "Fit the model and transform with the final estimator Fits all the transforms one after the other and transforms the data, then uses fit_transform on transformed data with the final estimator.", "methodParamsBody": "X : iterable   Training data. Must fulfill input requirements of first step of the pipeline.   y : iterable, default=None   Training targets. Must fulfill label requirements for all steps of the pipeline.   **fit_params : dict of string -> object   Parameters passed to the fit method of each step, where each parameter name is prefixed such that parameter p for step s has key s__p .  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y_pred : array-like ", "methodParams": ["X"], "methodReturns": ["y_pred"], "methodDesc": "Apply transforms to the data, and predict with the final estimator", "methodParamsBody": "X : iterable   Data to predict on. Must fulfill input requirements of first step of the pipeline.  "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "y_score : array-like, shape = [n_samples, n_classes] ", "methodParams": ["X"], "methodReturns": ["y_score"], "methodDesc": "Apply transforms, and predict_log_proba of the final estimator", "methodParamsBody": "X : iterable   Data to predict on. Must fulfill input requirements of first step of the pipeline.  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "y_proba : array-like, shape = [n_samples, n_classes] ", "methodParams": ["X"], "methodReturns": ["y_proba"], "methodDesc": "Apply transforms, and predict_proba of the final estimator", "methodParamsBody": "X : iterable   Data to predict on. Must fulfill input requirements of first step of the pipeline.  "}, {"methodName": "score(X, y=None, sample_weight=None)", "methodReturnsBody": "score : float ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Apply transforms, and score with the final estimator", "methodParamsBody": "X : iterable   Data to predict on. Must fulfill input requirements of first step of the pipeline.   y : iterable, default=None   Targets used for scoring. Must fulfill label requirements for all steps of the pipeline.   sample_weight : array-like, default=None   If not None, this argument is passed as sample_weight keyword argument to the score method of the final estimator.  "}, {"methodName": "set_params(**kwargs)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. Valid parameter keys can be listed with get_params() .", "methodReturnsBody": "self :"}], "allFuncParams": ["steps", "memory"], "notes": "", "funcName": "Pipeline", "allFuncAttributes": ["named_steps"], "funcDesc": "Pipeline of transforms with a final estimator.", "funcParamBody": "steps : list List of (name, transform) tuples (implementing fit/transform) that are chained, in the order in which they are chained, with the last object an estimator. memory : None, str or object with the joblib.Memory interface, optional Used to cache the fitted transformers of the pipeline. By default, no caching is performed. If a string is given, it is the path to the caching directory. Enabling caching triggers a clone of the transformers before fitting. Therefore, the transformer instance given to the pipeline cannot be inspected directly. Use the attribute named_steps or steps to inspect estimators within the pipeline. Caching the transformers is advantageous when fitting is time consuming.", "funcAttrBody": "named_steps : bunch object, a dictionary with attribute access Read-only attribute to access any step parameter by user given name. Keys are step names and values are steps parameters."},
{"libName": "sklearn.pipeline", "methods": [{"methodName": "__init__(transformer_list, n_jobs=1, transformer_weights=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : FeatureUnion   This estimator  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit all transformers using X.", "methodParamsBody": "X : iterable or array-like, depending on transformers   Input data, used to fit transformers.   y : array-like, shape (n_samples, ...), optional   Targets for supervised learning.  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)   hstack of results of transformers. sum_n_components is the sum of n_components (output dimension) over transformers.  ", "methodParams": ["X", "y"], "methodReturns": ["X_t"], "methodDesc": "Fit all transformers, transform the data and concatenate results.", "methodParamsBody": "X : iterable or array-like, depending on transformers   Input data to be transformed.   y : array-like, shape (n_samples, ...), optional   Targets for supervised learning.  "}, {"methodName": "get_feature_names()", "methodReturns": ["feature_names"], "methodDesc": "Get feature names from all transformers.", "methodReturnsBody": "feature_names : list of strings   Names of the features produced by transform.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**kwargs)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. Valid parameter keys can be listed with get_params() .", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_t : array-like or sparse matrix, shape (n_samples, sum_n_components)   hstack of results of transformers. sum_n_components is the sum of n_components (output dimension) over transformers.  ", "methodParams": ["X"], "methodReturns": ["X_t"], "methodDesc": "Transform X separately by each transformer, concatenate results.", "methodParamsBody": "X : iterable or array-like, depending on transformers   Input data to be transformed.  "}], "notes": "", "funcName": "FeatureUnion", "allFuncParams": ["transformer_list", "n_jobs", "transformer_weights"], "funcDesc": "Concatenates results of multiple transformer objects.", "funcParamBody": "transformer_list : list of (string, transformer) tuples List of transformer objects to be applied to the data. The first half of each tuple is the name of the transformer. n_jobs : int, optional Number of jobs to run in parallel (default 1). transformer_weights : dict, optional Multiplicative weights for features per transformer. Keys are transformer names, values the weights."},
{"libName": "sklearn.neural_network", "methods": [{"methodName": "__init__(hidden_layer_sizes=(100, ), activation=relu, solver=adam, alpha=0.0001, batch_size=auto, learning_rate=constant, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : returns a trained MLP model. ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model to data matrix X and target(s) y.", "methodParamsBody": "X : array-like or sparse matrix, shape (n_samples, n_features)   The input data.   y : array-like, shape (n_samples,) or (n_samples, n_outputs)   The target values (class labels in classification, real numbers in regression).  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array-like, shape (n_samples, n_outputs)   The predicted values.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict using the multi-layer perceptron model.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   The input data.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["hidden_layer_sizes", "activation", "solver", "alpha", "batch_size", "learning_rate", "learning_rate_init", "power_t", "max_iter", "shuffle", "random_state", "tol", "verbose", "warm_start", "momentum", "nesterovs_momentum", "early_stopping", "validation_fraction", "beta_1", "beta_2", "epsilon"], "notes": "MLPRegressor trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters. It can also have a regularization term added to the loss function that shrinks model parameters to prevent overfitting. This implementation works with data represented as dense and sparse numpy arrays of floating point values.", "funcName": "MLPRegressor", "allFuncAttributes": ["loss_", "coefs_", "intercepts_", "n_iter_", "n_layers_", "n_outputs_", "out_activation_"], "funcDesc": "Multi-layer Perceptron regressor.", "funcParamBody": "hidden_layer_sizes : tuple, length = n_layers - 2, default (100,) The ith element represents the number of neurons in the ith hidden layer. activation : {identity, logistic, tanh, relu}, default relu Activation function for the hidden layer. identity, no-op activation, useful to implement linear bottleneck, returns f(x) = x logistic, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)). tanh, the hyperbolic tan function, returns f(x) = tanh(x). relu, the rectified linear unit function, returns f(x) = max(0, x) solver : {lbfgs, sgd, adam}, default adam The solver for weight optimization. lbfgs is an optimizer in the family of quasi-Newton methods. sgd refers to stochastic gradient descent. adam refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba Note: The default solver adam works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, lbfgs can converge faster and perform better. alpha : float, optional, default 0.0001 L2 penalty (regularization term) parameter. batch_size : int, optional, default auto Size of minibatches for stochastic optimizers. If the solver is lbfgs, the classifier will not use minibatch. When set to auto, batch_size=min(200, n_samples) learning_rate : {constant, invscaling, adaptive}, default constant Learning rate schedule for weight updates. constant is a constant learning rate given by learning_rate_init. invscaling gradually decreases the learning rate learning_rate_ adaptive keeps the learning rate constant to learning_rate_init as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if early_stopping is on, the current learning rate is divided by 5. Only used when solver=sgd. learning_rate_init : double, optional, default 0.001 The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=sgd or adam. power_t : double, optional, default 0.5 The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to invscaling. Only used when solver=sgd. max_iter : int, optional, default 200 Maximum number of iterations. The solver iterates until convergence (determined by tol) or this number of iterations. For stochastic solvers (sgd, adam), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps. shuffle : bool, optional, default True Whether to shuffle samples in each iteration. Only used when solver=sgd or adam. random_state : int, RandomState instance or None, optional, default None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . tol : float, optional, default 1e-4 Tolerance for the optimization. When the loss or score is not improving by at least tol for two consecutive iterations, unless learning_rate verbose : bool, optional, default False Whether to print progress messages to stdout. warm_start : bool, optional, default False When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. momentum : float, default 0.9 Momentum for gradient descent update.  Should be between 0 and 1. Only used when solver=sgd. nesterovs_momentum : boolean, default True Whether to use Nesterovs momentum. Only used when solver=sgd and momentum > 0. early_stopping : bool, default False Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for two consecutive epochs. Only effective when solver=sgd or adam validation_fraction : float, optional, default 0.1 The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True beta_1 : float, optional, default 0.9 Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver=adam beta_2 : float, optional, default 0.999 Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver=adam epsilon : float, optional, default 1e-8 Value for numerical stability in adam. Only used when solver=adam", "funcAttrBody": "loss_ : float The current loss computed with the loss function. coefs_ : list, length n_layers - 1 The ith element in the list represents the weight matrix corresponding to layer i. intercepts_ : list, length n_layers - 1 The ith element in the list represents the bias vector corresponding to layer i + 1. n_iter_ : int, The number of iterations the solver has ran. n_layers_ : int Number of layers. n_outputs_ : int Number of outputs. out_activation_ : string Name of the output activation function."},
{"libName": "sklearn.neural_network", "methods": [{"methodName": "__init__(hidden_layer_sizes=(100, ), activation=relu, solver=adam, alpha=0.0001, batch_size=auto, learning_rate=constant, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : returns a trained MLP model. ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model to data matrix X and target(s) y.", "methodParamsBody": "X : array-like or sparse matrix, shape (n_samples, n_features)   The input data.   y : array-like, shape (n_samples,) or (n_samples, n_outputs)   The target values (class labels in classification, real numbers in regression).  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array-like, shape (n_samples,) or (n_samples, n_classes)   The predicted classes.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict using the multi-layer perceptron classifier", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   The input data.  "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "log_y_prob : array-like, shape (n_samples, n_classes)   The predicted log-probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ . Equivalent to log(predict_proba(X))  ", "methodParams": ["X"], "methodReturns": ["log_y_prob"], "methodDesc": "Return the log of probability estimates.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   The input data.  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "y_prob : array-like, shape (n_samples, n_classes)   The predicted probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ .  ", "methodParams": ["X"], "methodReturns": ["y_prob"], "methodDesc": "Probability estimates.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   The input data.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["hidden_layer_sizes", "activation", "solver", "alpha", "batch_size", "learning_rate", "learning_rate_init", "power_t", "max_iter", "shuffle", "random_state", "tol", "verbose", "warm_start", "momentum", "nesterovs_momentum", "early_stopping", "validation_fraction", "beta_1", "beta_2", "epsilon"], "notes": "MLPClassifier trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters. It can also have a regularization term added to the loss function that shrinks model parameters to prevent overfitting. This implementation works with data represented as dense numpy arrays or sparse scipy arrays of floating point values. MLPClassifier trains iteratively since at each time step the partial derivatives of the loss function with respect to the model parameters are computed to update the parameters. It can also have a regularization term added to the loss function that shrinks model parameters to prevent overfitting. This implementation works with data represented as dense numpy arrays or sparse scipy arrays of floating point values.", "funcName": "MLPClassifier", "allFuncAttributes": ["classes_", "loss_", "coefs_", "intercepts_", "n_iter_", "n_layers_", "n_outputs_", "out_activation_"], "funcDesc": "Multi-layer Perceptron classifier.", "funcParamBody": "hidden_layer_sizes : tuple, length = n_layers - 2, default (100,) The ith element represents the number of neurons in the ith hidden layer. activation : {identity, logistic, tanh, relu}, default relu Activation function for the hidden layer. identity, no-op activation, useful to implement linear bottleneck, returns f(x) = x logistic, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)). tanh, the hyperbolic tan function, returns f(x) = tanh(x). relu, the rectified linear unit function, returns f(x) = max(0, x) solver : {lbfgs, sgd, adam}, default adam The solver for weight optimization. lbfgs is an optimizer in the family of quasi-Newton methods. sgd refers to stochastic gradient descent. adam refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba Note: The default solver adam works pretty well on relatively large datasets (with thousands of training samples or more) in terms of both training time and validation score. For small datasets, however, lbfgs can converge faster and perform better. alpha : float, optional, default 0.0001 L2 penalty (regularization term) parameter. batch_size : int, optional, default auto Size of minibatches for stochastic optimizers. If the solver is lbfgs, the classifier will not use minibatch. When set to auto, batch_size=min(200, n_samples) learning_rate : {constant, invscaling, adaptive}, default constant Learning rate schedule for weight updates. constant is a constant learning rate given by learning_rate_init. invscaling gradually decreases the learning rate learning_rate_ adaptive keeps the learning rate constant to learning_rate_init as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol if early_stopping is on, the current learning rate is divided by 5. Only used when solver='sgd' . learning_rate_init : double, optional, default 0.001 The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=sgd or adam. power_t : double, optional, default 0.5 The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to invscaling. Only used when solver=sgd. max_iter : int, optional, default 200 Maximum number of iterations. The solver iterates until convergence (determined by tol) or this number of iterations. For stochastic solvers (sgd, adam), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps. shuffle : bool, optional, default True Whether to shuffle samples in each iteration. Only used when solver=sgd or adam. random_state : int, RandomState instance or None, optional, default None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . tol : float, optional, default 1e-4 Tolerance for the optimization. When the loss or score is not improving by at least tol for two consecutive iterations, unless learning_rate verbose : bool, optional, default False Whether to print progress messages to stdout. warm_start : bool, optional, default False When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. momentum : float, default 0.9 Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=sgd. nesterovs_momentum : boolean, default True Whether to use Nesterovs momentum. Only used when solver=sgd and momentum > 0. early_stopping : bool, default False Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10% of training data as validation and terminate training when validation score is not improving by at least tol for two consecutive epochs. Only effective when solver=sgd or adam validation_fraction : float, optional, default 0.1 The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if early_stopping is True beta_1 : float, optional, default 0.9 Exponential decay rate for estimates of first moment vector in adam, should be in [0, 1). Only used when solver=adam beta_2 : float, optional, default 0.999 Exponential decay rate for estimates of second moment vector in adam, should be in [0, 1). Only used when solver=adam epsilon : float, optional, default 1e-8 Value for numerical stability in adam. Only used when solver=adam", "funcAttrBody": "classes_ : array or list of array of shape (n_classes,) Class labels for each output. loss_ : float The current loss computed with the loss function. coefs_ : list, length n_layers - 1 The ith element in the list represents the weight matrix corresponding to layer i. intercepts_ : list, length n_layers - 1 The ith element in the list represents the bias vector corresponding to layer i + 1. n_iter_ : int, The number of iterations the solver has ran. n_layers_ : int Number of layers. n_outputs_ : int Number of outputs. out_activation_ : string Name of the output activation function."},
{"libName": "sklearn.neural_network", "methods": [{"methodName": "__init__(n_components=256, learning_rate=0.1, batch_size=10, n_iter=10, verbose=0, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : BernoulliRBM   The fitted model.  ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Fit the model to the data X.", "methodParamsBody": "X : {array-like, sparse matrix} shape (n_samples, n_features)   Training data.  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "gibbs(v)", "methodReturnsBody": "v_new : array-like, shape (n_samples, n_features)   Values of the visible layer after one Gibbs step.  ", "methodParams": ["v"], "methodReturns": ["v_new"], "methodDesc": "Perform one Gibbs sampling step.", "methodParamsBody": "v : array-like, shape (n_samples, n_features)   Values of the visible layer to start from.  "}, {"methodName": "partial_fit(X, y=None)", "methodReturnsBody": "self : BernoulliRBM   The fitted model.  ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Fit the model to the data X which should contain a partial segment of the data.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data.  "}, {"methodName": "score_samples(X)", "methodReturnsBody": "pseudo_likelihood : array-like, shape (n_samples,)   Value of the pseudo-likelihood (proxy for likelihood).  ", "methodParams": ["X"], "methodReturns": ["pseudo_likelihood"], "methodDesc": "Compute the pseudo-likelihood of X. Notes This method is not deterministic: it computes a quantity called the free energy on X, then on a randomly corrupted version of X, and returns the log of the logistic function of the difference.", "methodParamsBody": "X : {array-like, sparse matrix} shape (n_samples, n_features)   Values of the visible layer. Must be all-boolean (not checked).  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "h : array, shape (n_samples, n_components)   Latent representations of the data.  ", "methodParams": ["X"], "methodReturns": ["h"], "methodDesc": "Compute the hidden layer activation probabilities, P(h=1|v=X).", "methodParamsBody": "X : {array-like, sparse matrix} shape (n_samples, n_features)   The data to be transformed.  "}], "allFuncParams": ["n_components", "learning_rate", "batch_size", "n_iter", "verbose", "random_state"], "notes": "This method is not deterministic: it computes a quantity called the free energy on X, then on a randomly corrupted version of X, and returns the log of the logistic function of the difference. This method is not deterministic: it computes a quantity called the free energy on X, then on a randomly corrupted version of X, and returns the log of the logistic function of the difference.", "funcName": "BernoulliRBM", "allFuncAttributes": ["intercept_hidden_", "intercept_visible_", "components_"], "funcDesc": "Bernoulli Restricted Boltzmann Machine (RBM).", "funcParamBody": "n_components : int, optional Number of binary hidden units. learning_rate : float, optional The learning rate for weight updates. It is highly recommended to tune this hyper-parameter. Reasonable values are in the 10**[0., -3.] range. batch_size : int, optional Number of examples per minibatch. n_iter : int, optional Number of iterations/sweeps over the training dataset to perform during training. verbose : int, optional The verbosity level. The default, zero, means silent mode. random_state : integer or numpy.RandomState, optional A random number generator instance to define the state of the random permutations generator. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator.", "funcAttrBody": "intercept_hidden_ : array-like, shape (n_components,) Biases of the hidden units. intercept_visible_ : array-like, shape (n_features,) Biases of the visible units. components_ : array-like, shape (n_components, n_features) Weight matrix, where n_features in the number of visible units and n_components is the number of hidden units."},
{"allReturnParams": ["A"], "libName": "sklearn.neighbors", "methods": [], "notes": "", "funcName": "radius_neighbors_graph", "allFuncParams": ["X", "radius", "mode", "metric", "p", "metric_params", "include_self", "n_jobs"], "funcDesc": "Computes the (weighted) graph of Neighbors for points in X", "funcParamBody": "X : array-like or BallTree, shape = [n_samples, n_features] Sample data, in the form of a numpy array or a precomputed BallTree . radius : float Radius of neighborhoods. mode : {connectivity, distance}, optional Type of returned matrix: connectivity will return the connectivity matrix with ones and zeros, and distance will return the distances between neighbors according to the given metric. metric : string, default minkowski The distance metric used to calculate the neighbors within a given radius for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is euclidean (minkowski metric with the param equal to 2.) p : int, default 2 Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. metric_params : dict, optional additional keyword arguments for the metric function. include_self : bool, default=False Whether or not to mark each sample as the first nearest neighbor to itself. If None , then True is used for mode=connectivity and False for mode=distance as this will preserve backwards compatibilty. n_jobs : int, optional (default = 1) The number of parallel jobs to run for neighbors search. If -1 , then the number of jobs is set to the number of CPU cores.", "funcReturnBody": "A : sparse matrix in CSR format, shape = [n_samples, n_samples] A[i, j] is assigned the weight of edge that connects i to j."},
{"allReturnParams": ["A"], "libName": "sklearn.neighbors", "methods": [], "notes": "", "funcName": "kneighbors_graph", "allFuncParams": ["X", "n_neighbors", "mode", "metric", "p", "metric_params", "include_self", "n_jobs"], "funcDesc": "Computes the (weighted) graph of k-Neighbors for points in X", "funcParamBody": "X : array-like or BallTree, shape = [n_samples, n_features] Sample data, in the form of a numpy array or a precomputed BallTree . n_neighbors : int Number of neighbors for each sample. mode : {connectivity, distance}, optional Type of returned matrix: connectivity will return the connectivity matrix with ones and zeros, and distance will return the distances between neighbors according to the given metric. metric : string, default minkowski The distance metric used to calculate the k-Neighbors for each sample point. The DistanceMetric class gives a list of available metrics. The default distance is euclidean (minkowski metric with the p param equal to 2.) p : int, default 2 Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. metric_params : dict, optional additional keyword arguments for the metric function. include_self : bool, default=False. Whether or not to mark each sample as the first nearest neighbor to itself. If None , then True is used for mode=connectivity and False for mode=distance as this will preserve backwards compatibilty. n_jobs : int, optional (default = 1) The number of parallel jobs to run for neighbors search. If -1 , then the number of jobs is set to the number of CPU cores.", "funcReturnBody": "A : sparse matrix in CSR format, shape = [n_samples, n_samples] A[i, j] is assigned the weight of edge that connects i to j."},
{"libName": "sklearn.neighbors", "methods": [{"methodName": "__init__(n_neighbors=5, radius=1.0, algorithm=auto, leaf_size=30, metric=minkowski, p=2, metric_params=None, n_jobs=1, **kwargs)", "methodDesc": ""}, {"methodParams": ["X"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : {array-like, sparse matrix, BallTree, KDTree}   Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric=precomputed.  ", "methodDesc": "Fit the model using X as training data"}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "kneighbors(X=None, n_neighbors=None, return_distance=True)", "methodReturnsBody": "dist : array   Array representing the lengths to points, only present if return_distance=True   ind : array   Indices of the nearest points in the population matrix.  ", "methodParams": ["X", "n_neighbors", "return_distance"], "methodReturns": ["dist", "ind"], "methodDesc": "Finds the K-neighbors of a point. Returns indices of and distances to the neighbors of each point. Examples In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask whos the closest point to [1,1,1] As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third element of samples (indexes start at 0). You can also query for multiple points:", "methodParamsBody": "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == precomputed   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   n_neighbors : int   Number of neighbors to get (default is the value passed to the constructor).   return_distance : boolean, optional. Defaults to True.   If False, distances will not be returned  "}, {"methodName": "kneighbors_graph(X=None, n_neighbors=None, mode=connectivity)", "methodReturnsBody": "A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]   n_samples_fit is the number of samples in the fitted data A[i, j] is assigned the weight of edge that connects i to j.  ", "methodParams": ["X", "n_neighbors", "mode"], "methodReturns": ["A"], "methodDesc": "Computes the (weighted) graph of k-Neighbors for points in X Examples", "methodParamsBody": "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == precomputed   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   n_neighbors : int   Number of neighbors for each sample. (default is value passed to the constructor).   mode : {connectivity, distance}, optional   Type of returned matrix: connectivity will return the connectivity matrix with ones and zeros, in distance the edges are Euclidean distance between points.  "}, {"methodName": "radius_neighbors(X=None, radius=None, return_distance=True)", "methodReturnsBody": "dist : array, shape (n_samples,) of arrays   Array representing the distances to each point, only present if return_distance=True. The distance values are computed according to the metric constructor parameter.   ind : array, shape (n_samples,) of arrays   An array of arrays of indices of the approximate nearest points from the population matrix that lie within a ball of size radius around the query points.  ", "methodParams": ["X", "radius", "return_distance"], "methodReturns": ["dist", "ind"], "methodDesc": "Finds the neighbors within a given radius of a point or points. Return the indices and distances of each point from the dataset lying in a ball with size radius around the points of the query array. Points lying on the boundary are included in the results. The result points are not necessarily sorted by distance to their query point. Notes Because the number of neighbors of each point is not necessarily equal, the results for multiple query points cannot be fit in a standard data array. For efficiency, radius_neighbors returns arrays of objects, where each object is a 1D array of indices or distances. Examples In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask whos the closest point to [1, 1, 1]: The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices.  In general, multiple points can be queried at the same time.", "methodParamsBody": "X : array-like, (n_samples, n_features), optional   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   radius : float   Limiting distance of neighbors to return. (default is the value passed to the constructor).   return_distance : boolean, optional. Defaults to True.   If False, distances will not be returned  "}, {"methodName": "radius_neighbors_graph(X=None, radius=None, mode=connectivity)", "methodReturnsBody": "A : sparse matrix in CSR format, shape = [n_samples, n_samples]   A[i, j] is assigned the weight of edge that connects i to j.  ", "methodParams": ["X", "radius", "mode"], "methodReturns": ["A"], "methodDesc": "Computes the (weighted) graph of Neighbors for points in X Neighborhoods are restricted the points at a distance lower than radius. Examples", "methodParamsBody": "X : array-like, shape = [n_samples, n_features], optional   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   radius : float   Radius of neighborhoods. (default is the value passed to the constructor).   mode : {connectivity, distance}, optional   Type of returned matrix: connectivity will return the connectivity matrix with ones and zeros, in distance the edges are Euclidean distance between points.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "See Nearest Neighbors in the online documentation for a discussion of the choice of algorithm and leaf_size . https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm Because the number of neighbors of each point is not necessarily equal, the results for multiple query points cannot be fit in a standard data array. For efficiency, radius_neighbors returns arrays of objects, where each object is a 1D array of indices or distances. In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask whos the closest point to [1, 1, 1]: The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices.  In general, multiple points can be queried at the same time.", "funcName": "NearestNeighbors", "allFuncParams": ["n_neighbors", "radius", "algorithm", "leaf_size", "metric", "p", "metric_params", "n_jobs"], "funcDesc": "Unsupervised learner for implementing neighbor searches.", "funcParamBody": "n_neighbors : int, optional (default = 5) Number of neighbors to use by default for kneighbors queries. radius : float, optional (default = 1.0) Range of parameter space to use by default for radius_neighbors algorithm : {auto, ball_tree, kd_tree, brute}, optional Algorithm used to compute the nearest neighbors: ball_tree will use BallTree kd_tree will use KDTree brute will use a brute-force search. auto will attempt to decide the most appropriate algorithm based on the values passed to fit method. Note: fitting on sparse input will override the setting of this parameter, using brute force. leaf_size : int, optional (default = 30) Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem. metric : string or callable, default minkowski metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used. If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipys metrics, but is less efficient than passing the metric name as a string. Distance matrices are not supported. Valid values for metric are: from scikit-learn: [cityblock, cosine, euclidean, l1, l2, manhattan] from scipy.spatial.distance: [braycurtis, canberra, chebyshev, correlation, dice, hamming, jaccard, kulsinski, mahalanobis, matching, minkowski, rogerstanimoto, russellrao, seuclidean, sokalmichener, sokalsneath, sqeuclidean, yule] See the documentation for scipy.spatial.distance for details on these metrics. p : integer, optional (default = 2) Parameter for the Minkowski metric from sklearn.metrics.pairwise.pairwise_distances. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. metric_params : dict, optional (default = None) Additional keyword arguments for the metric function. n_jobs : int, optional (default = 1) The number of parallel jobs to run for neighbors search. If -1 , then the number of jobs is set to the number of CPU cores. Affects only kneighbors and kneighbors_graph methods."},
{"libName": "sklearn.neighbors", "methods": [{"methodName": "__init__(metric=euclidean, shrink_threshold=None)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training vector, where n_samples in the number of samples and n_features is the number of features. Note that centroid shrinking cannot be used with sparse matrices.   y : array, shape = [n_samples]   Target values (integers)  ", "methodDesc": "Fit the NearestCentroid model according to the given training data."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples]", "methodParams": [], "methodReturns": [], "methodDesc": "Perform classification on an array of test vectors X. The predicted class C for each sample in X is returned. Notes If the metric constructor parameter is precomputed, X is assumed to be the distance matrix between the data to be predicted and self.centroids_ .", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]"}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["metric", "shrink_threshold"], "notes": "When used for text classification with tf-idf vectors, this classifier is also known as the Rocchio classifier. Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of multiple cancer types by shrunken centroids of gene expression. Proceedings of the National Academy of Sciences of the United States of America, 99(10), 6567-6572. The National Academy of Sciences. If the metric constructor parameter is precomputed, X is assumed to be the distance matrix between the data to be predicted and self.centroids_ . When used for text classification with tf-idf vectors, this classifier is also known as the Rocchio classifier. Tibshirani, R., Hastie, T., Narasimhan, B., & Chu, G. (2002). Diagnosis of multiple cancer types by shrunken centroids of gene expression. Proceedings of the National Academy of Sciences of the United States of America, 99(10), 6567-6572. The National Academy of Sciences. If the metric constructor parameter is precomputed, X is assumed to be the distance matrix between the data to be predicted and self.centroids_ .", "funcName": "NearestCentroid", "allFuncAttributes": ["centroids_"], "funcDesc": "Nearest centroid classifier.", "funcParamBody": "metric : string, or callable The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.pairwise_distances for its metric parameter. The centroids for the samples corresponding to each class is the point from which the sum of the distances (according to the metric) of all samples that belong to that particular class are minimized. If the manhattan metric is provided, this centroid is the median and for all other metrics, the centroid is now set to be the mean. shrink_threshold : float, optional (default = None) Threshold for shrinking centroids to remove features.", "funcAttrBody": "centroids_ : array-like, shape = [n_classes, n_features] Centroid of each class"},
{"libName": "sklearn.neighbors", "methods": [{"methodName": "__init__(radius=1.0, weights=uniform, algorithm=auto, leaf_size=30, p=2, metric=minkowski, metric_params=None, **kwargs)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : {array-like, sparse matrix, BallTree, KDTree}   Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric=precomputed.   y : {array-like, sparse matrix}    Target values, array of float values, shape = [n_samples]  or [n_samples, n_outputs]    ", "methodDesc": "Fit the model using X as training data and y as target values"}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of int, shape = [n_samples] or [n_samples, n_outputs]   Target values  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict the target for the provided data", "methodParamsBody": "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == precomputed   Test samples.  "}, {"methodName": "radius_neighbors(X=None, radius=None, return_distance=True)", "methodReturnsBody": "dist : array, shape (n_samples,) of arrays   Array representing the distances to each point, only present if return_distance=True. The distance values are computed according to the metric constructor parameter.   ind : array, shape (n_samples,) of arrays   An array of arrays of indices of the approximate nearest points from the population matrix that lie within a ball of size radius around the query points.  ", "methodParams": ["X", "radius", "return_distance"], "methodReturns": ["dist", "ind"], "methodDesc": "Finds the neighbors within a given radius of a point or points. Return the indices and distances of each point from the dataset lying in a ball with size radius around the points of the query array. Points lying on the boundary are included in the results. The result points are not necessarily sorted by distance to their query point. Notes Because the number of neighbors of each point is not necessarily equal, the results for multiple query points cannot be fit in a standard data array. For efficiency, radius_neighbors returns arrays of objects, where each object is a 1D array of indices or distances. Examples In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask whos the closest point to [1, 1, 1]: The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices.  In general, multiple points can be queried at the same time.", "methodParamsBody": "X : array-like, (n_samples, n_features), optional   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   radius : float   Limiting distance of neighbors to return. (default is the value passed to the constructor).   return_distance : boolean, optional. Defaults to True.   If False, distances will not be returned  "}, {"methodName": "radius_neighbors_graph(X=None, radius=None, mode=connectivity)", "methodReturnsBody": "A : sparse matrix in CSR format, shape = [n_samples, n_samples]   A[i, j] is assigned the weight of edge that connects i to j.  ", "methodParams": ["X", "radius", "mode"], "methodReturns": ["A"], "methodDesc": "Computes the (weighted) graph of Neighbors for points in X Neighborhoods are restricted the points at a distance lower than radius. Examples", "methodParamsBody": "X : array-like, shape = [n_samples, n_features], optional   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   radius : float   Radius of neighborhoods. (default is the value passed to the constructor).   mode : {connectivity, distance}, optional   Type of returned matrix: connectivity will return the connectivity matrix with ones and zeros, in distance the edges are Euclidean distance between points.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "See Nearest Neighbors in the online documentation for a discussion of the choice of algorithm and leaf_size . https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm Because the number of neighbors of each point is not necessarily equal, the results for multiple query points cannot be fit in a standard data array. For efficiency, radius_neighbors returns arrays of objects, where each object is a 1D array of indices or distances. In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask whos the closest point to [1, 1, 1]: The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices.  In general, multiple points can be queried at the same time.", "funcName": "RadiusNeighborsRegressor", "allFuncParams": ["radius", "weights", "algorithm", "leaf_size", "p", "metric", "metric_params"], "funcDesc": "Regression based on neighbors within a fixed radius.", "funcParamBody": "radius : float, optional (default = 1.0) Range of parameter space to use by default for radius_neighbors weights : str or callable weight function used in prediction.  Possible values: uniform : uniform weights.  All points in each neighborhood are weighted equally. distance : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights. Uniform weights are used by default. algorithm : {auto, ball_tree, kd_tree, brute}, optional Algorithm used to compute the nearest neighbors: ball_tree will use BallTree kd_tree will use KDTree brute will use a brute-force search. auto will attempt to decide the most appropriate algorithm based on the values passed to fit method. Note: fitting on sparse input will override the setting of this parameter, using brute force. leaf_size : int, optional (default = 30) Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem. p : integer, optional (default = 2) Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. metric : string or callable, default minkowski the distance metric to use for the tree.  The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. See the documentation of the DistanceMetric class for a list of available metrics. metric_params : dict, optional (default = None) Additional keyword arguments for the metric function."},
{"libName": "sklearn.neighbors", "methods": [{"methodName": "__init__(radius=1.0, weights=uniform, algorithm=auto, leaf_size=30, p=2, metric=minkowski, outlier_label=None, metric_params=None, **kwargs)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : {array-like, sparse matrix, BallTree, KDTree}   Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric=precomputed.   y : {array-like, sparse matrix}   Target values of shape = [n_samples] or [n_samples, n_outputs]  ", "methodDesc": "Fit the model using X as training data and y as target values"}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of shape [n_samples] or [n_samples, n_outputs]   Class labels for each data sample.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict the class labels for the provided data", "methodParamsBody": "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == precomputed   Test samples.  "}, {"methodName": "radius_neighbors(X=None, radius=None, return_distance=True)", "methodReturnsBody": "dist : array, shape (n_samples,) of arrays   Array representing the distances to each point, only present if return_distance=True. The distance values are computed according to the metric constructor parameter.   ind : array, shape (n_samples,) of arrays   An array of arrays of indices of the approximate nearest points from the population matrix that lie within a ball of size radius around the query points.  ", "methodParams": ["X", "radius", "return_distance"], "methodReturns": ["dist", "ind"], "methodDesc": "Finds the neighbors within a given radius of a point or points. Return the indices and distances of each point from the dataset lying in a ball with size radius around the points of the query array. Points lying on the boundary are included in the results. The result points are not necessarily sorted by distance to their query point. Notes Because the number of neighbors of each point is not necessarily equal, the results for multiple query points cannot be fit in a standard data array. For efficiency, radius_neighbors returns arrays of objects, where each object is a 1D array of indices or distances. Examples In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask whos the closest point to [1, 1, 1]: The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices.  In general, multiple points can be queried at the same time.", "methodParamsBody": "X : array-like, (n_samples, n_features), optional   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   radius : float   Limiting distance of neighbors to return. (default is the value passed to the constructor).   return_distance : boolean, optional. Defaults to True.   If False, distances will not be returned  "}, {"methodName": "radius_neighbors_graph(X=None, radius=None, mode=connectivity)", "methodReturnsBody": "A : sparse matrix in CSR format, shape = [n_samples, n_samples]   A[i, j] is assigned the weight of edge that connects i to j.  ", "methodParams": ["X", "radius", "mode"], "methodReturns": ["A"], "methodDesc": "Computes the (weighted) graph of Neighbors for points in X Neighborhoods are restricted the points at a distance lower than radius. Examples", "methodParamsBody": "X : array-like, shape = [n_samples, n_features], optional   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   radius : float   Radius of neighborhoods. (default is the value passed to the constructor).   mode : {connectivity, distance}, optional   Type of returned matrix: connectivity will return the connectivity matrix with ones and zeros, in distance the edges are Euclidean distance between points.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "See Nearest Neighbors in the online documentation for a discussion of the choice of algorithm and leaf_size . https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm Because the number of neighbors of each point is not necessarily equal, the results for multiple query points cannot be fit in a standard data array. For efficiency, radius_neighbors returns arrays of objects, where each object is a 1D array of indices or distances. In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask whos the closest point to [1, 1, 1]: The first array returned contains the distances to all points which are closer than 1.6, while the second array returned contains their indices.  In general, multiple points can be queried at the same time.", "funcName": "RadiusNeighborsClassifier", "allFuncParams": ["radius", "weights", "algorithm", "leaf_size", "p", "metric", "outlier_label", "metric_params"], "funcDesc": "Classifier implementing a vote among neighbors within a given radius", "funcParamBody": "radius : float, optional (default = 1.0) Range of parameter space to use by default for radius_neighbors weights : str or callable weight function used in prediction.  Possible values: uniform : uniform weights.  All points in each neighborhood are weighted equally. distance : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights. Uniform weights are used by default. algorithm : {auto, ball_tree, kd_tree, brute}, optional Algorithm used to compute the nearest neighbors: ball_tree will use BallTree kd_tree will use KDTree brute will use a brute-force search. auto will attempt to decide the most appropriate algorithm based on the values passed to fit method. Note: fitting on sparse input will override the setting of this parameter, using brute force. leaf_size : int, optional (default = 30) Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem. p : integer, optional (default = 2) Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. metric : string or callable, default minkowski the distance metric to use for the tree.  The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. See the documentation of the DistanceMetric class for a list of available metrics. outlier_label : int, optional (default = None) Label, which is given for outlier samples (samples with no neighbors on given radius). If set to None, ValueError is raised, when outlier is detected. metric_params : dict, optional (default = None) Additional keyword arguments for the metric function."},
{"funcName": "LocalOutlierFactor", "notes": "", "libName": "sklearn.neighbors", "methods": [{"methodName": "__init__(n_neighbors=20, algorithm=auto, leaf_size=30, metric=minkowski, p=2, metric_params=None, contamination=0.1, n_jobs=1)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Fit the model using X as training data.", "methodParamsBody": "X : {array-like, sparse matrix, BallTree, KDTree}   Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric=precomputed.  "}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "is_inlier : array, shape (n_samples,)   Returns -1 for anomalies/outliers and 1 for inliers.  ", "methodParams": ["X"], "methodReturns": ["is_inlier"], "methodDesc": "Fits the model to the training set X and returns the labels (1 inlier, -1 outlier) on the training set according to the LOF score and the contamination parameter.", "methodParamsBody": "X : array-like, shape (n_samples, n_features), default=None   The query sample or samples to compute the Local Outlier Factor w.r.t. to the training samples.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "kneighbors(X=None, n_neighbors=None, return_distance=True)", "methodReturnsBody": "dist : array   Array representing the lengths to points, only present if return_distance=True   ind : array   Indices of the nearest points in the population matrix.  ", "methodParams": ["X", "n_neighbors", "return_distance"], "methodReturns": ["dist", "ind"], "methodDesc": "Finds the K-neighbors of a point. Returns indices of and distances to the neighbors of each point. Examples In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask whos the closest point to [1,1,1] As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third element of samples (indexes start at 0). You can also query for multiple points:", "methodParamsBody": "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == precomputed   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   n_neighbors : int   Number of neighbors to get (default is the value passed to the constructor).   return_distance : boolean, optional. Defaults to True.   If False, distances will not be returned  "}, {"methodName": "kneighbors_graph(X=None, n_neighbors=None, mode=connectivity)", "methodReturnsBody": "A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]   n_samples_fit is the number of samples in the fitted data A[i, j] is assigned the weight of edge that connects i to j.  ", "methodParams": ["X", "n_neighbors", "mode"], "methodReturns": ["A"], "methodDesc": "Computes the (weighted) graph of k-Neighbors for points in X Examples", "methodParamsBody": "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == precomputed   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   n_neighbors : int   Number of neighbors for each sample. (default is value passed to the constructor).   mode : {connectivity, distance}, optional   Type of returned matrix: connectivity will return the connectivity matrix with ones and zeros, in distance the edges are Euclidean distance between points.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "Unsupervised Outlier Detection using Local Outlier Factor (LOF)"},
{"libName": "sklearn.neighbors", "methods": [{"methodName": "__init__(n_neighbors=5, weights=uniform, algorithm=auto, leaf_size=30, p=2, metric=minkowski, metric_params=None, n_jobs=1, **kwargs)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : {array-like, sparse matrix, BallTree, KDTree}   Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric=precomputed.   y : {array-like, sparse matrix}    Target values, array of float values, shape = [n_samples]  or [n_samples, n_outputs]    ", "methodDesc": "Fit the model using X as training data and y as target values"}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "kneighbors(X=None, n_neighbors=None, return_distance=True)", "methodReturnsBody": "dist : array   Array representing the lengths to points, only present if return_distance=True   ind : array   Indices of the nearest points in the population matrix.  ", "methodParams": ["X", "n_neighbors", "return_distance"], "methodReturns": ["dist", "ind"], "methodDesc": "Finds the K-neighbors of a point. Returns indices of and distances to the neighbors of each point. Examples In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask whos the closest point to [1,1,1] As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third element of samples (indexes start at 0). You can also query for multiple points:", "methodParamsBody": "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == precomputed   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   n_neighbors : int   Number of neighbors to get (default is the value passed to the constructor).   return_distance : boolean, optional. Defaults to True.   If False, distances will not be returned  "}, {"methodName": "kneighbors_graph(X=None, n_neighbors=None, mode=connectivity)", "methodReturnsBody": "A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]   n_samples_fit is the number of samples in the fitted data A[i, j] is assigned the weight of edge that connects i to j.  ", "methodParams": ["X", "n_neighbors", "mode"], "methodReturns": ["A"], "methodDesc": "Computes the (weighted) graph of k-Neighbors for points in X Examples", "methodParamsBody": "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == precomputed   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   n_neighbors : int   Number of neighbors for each sample. (default is value passed to the constructor).   mode : {connectivity, distance}, optional   Type of returned matrix: connectivity will return the connectivity matrix with ones and zeros, in distance the edges are Euclidean distance between points.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of int, shape = [n_samples] or [n_samples, n_outputs]   Target values  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict the target for the provided data", "methodParamsBody": "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == precomputed   Test samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "See Nearest Neighbors in the online documentation for a discussion of the choice of algorithm and leaf_size . https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm See Nearest Neighbors in the online documentation for a discussion of the choice of algorithm and leaf_size . https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm", "funcName": "KNeighborsRegressor", "allFuncParams": ["n_neighbors", "weights", "algorithm", "leaf_size", "p", "metric", "metric_params", "n_jobs"], "funcDesc": "Regression based on k-nearest neighbors.", "funcParamBody": "n_neighbors : int, optional (default = 5) Number of neighbors to use by default for kneighbors queries. weights : str or callable weight function used in prediction.  Possible values: uniform : uniform weights.  All points in each neighborhood are weighted equally. distance : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights. Uniform weights are used by default. algorithm : {auto, ball_tree, kd_tree, brute}, optional Algorithm used to compute the nearest neighbors: ball_tree will use BallTree kd_tree will use KDTree brute will use a brute-force search. auto will attempt to decide the most appropriate algorithm based on the values passed to fit method. Note: fitting on sparse input will override the setting of this parameter, using brute force. leaf_size : int, optional (default = 30) Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem. p : integer, optional (default = 2) Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. metric : string or callable, default minkowski the distance metric to use for the tree.  The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. See the documentation of the DistanceMetric class for a list of available metrics. metric_params : dict, optional (default = None) Additional keyword arguments for the metric function. n_jobs : int, optional (default = 1) The number of parallel jobs to run for neighbors search. If -1 , then the number of jobs is set to the number of CPU cores. Doesnt affect fit method."},
{"libName": "sklearn.neighbors", "methods": [{"methodName": "__init__(n_neighbors=5, weights=uniform, algorithm=auto, leaf_size=30, p=2, metric=minkowski, metric_params=None, n_jobs=1, **kwargs)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : {array-like, sparse matrix, BallTree, KDTree}   Training data. If array or matrix, shape [n_samples, n_features], or [n_samples, n_samples] if metric=precomputed.   y : {array-like, sparse matrix}   Target values of shape = [n_samples] or [n_samples, n_outputs]  ", "methodDesc": "Fit the model using X as training data and y as target values"}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "kneighbors(X=None, n_neighbors=None, return_distance=True)", "methodReturnsBody": "dist : array   Array representing the lengths to points, only present if return_distance=True   ind : array   Indices of the nearest points in the population matrix.  ", "methodParams": ["X", "n_neighbors", "return_distance"], "methodReturns": ["dist", "ind"], "methodDesc": "Finds the K-neighbors of a point. Returns indices of and distances to the neighbors of each point. Examples In the following example, we construct a NeighborsClassifier class from an array representing our data set and ask whos the closest point to [1,1,1] As you can see, it returns [[0.5]], and [[2]], which means that the element is at distance 0.5 and is the third element of samples (indexes start at 0). You can also query for multiple points:", "methodParamsBody": "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == precomputed   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   n_neighbors : int   Number of neighbors to get (default is the value passed to the constructor).   return_distance : boolean, optional. Defaults to True.   If False, distances will not be returned  "}, {"methodName": "kneighbors_graph(X=None, n_neighbors=None, mode=connectivity)", "methodReturnsBody": "A : sparse matrix in CSR format, shape = [n_samples, n_samples_fit]   n_samples_fit is the number of samples in the fitted data A[i, j] is assigned the weight of edge that connects i to j.  ", "methodParams": ["X", "n_neighbors", "mode"], "methodReturns": ["A"], "methodDesc": "Computes the (weighted) graph of k-Neighbors for points in X Examples", "methodParamsBody": "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == precomputed   The query point or points. If not provided, neighbors of each indexed point are returned. In this case, the query point is not considered its own neighbor.   n_neighbors : int   Number of neighbors for each sample. (default is value passed to the constructor).   mode : {connectivity, distance}, optional   Type of returned matrix: connectivity will return the connectivity matrix with ones and zeros, in distance the edges are Euclidean distance between points.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of shape [n_samples] or [n_samples, n_outputs]   Class labels for each data sample.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict the class labels for the provided data", "methodParamsBody": "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == precomputed   Test samples.  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "p : array of shape = [n_samples, n_classes], or a list of n_outputs   of such arrays if n_outputs > 1. The class probabilities of the input samples. Classes are ordered by lexicographic order.  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Return probability estimates for the test data X.", "methodParamsBody": "X : array-like, shape (n_query, n_features),                 or (n_query, n_indexed) if metric == precomputed   Test samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "See Nearest Neighbors in the online documentation for a discussion of the choice of algorithm and leaf_size . https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm See Nearest Neighbors in the online documentation for a discussion of the choice of algorithm and leaf_size . https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm", "funcName": "KNeighborsClassifier", "allFuncParams": ["n_neighbors", "weights", "algorithm", "leaf_size", "p", "metric", "metric_params", "n_jobs"], "funcDesc": "Classifier implementing the k-nearest neighbors vote.", "funcParamBody": "n_neighbors : int, optional (default = 5) Number of neighbors to use by default for kneighbors queries. weights : str or callable, optional (default = uniform) weight function used in prediction.  Possible values: uniform : uniform weights.  All points in each neighborhood are weighted equally. distance : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away. [callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights. algorithm : {auto, ball_tree, kd_tree, brute}, optional Algorithm used to compute the nearest neighbors: ball_tree will use BallTree kd_tree will use KDTree brute will use a brute-force search. auto will attempt to decide the most appropriate algorithm based on the values passed to fit method. Note: fitting on sparse input will override the setting of this parameter, using brute force. leaf_size : int, optional (default = 30) Leaf size passed to BallTree or KDTree.  This can affect the speed of the construction and query, as well as the memory required to store the tree.  The optimal value depends on the nature of the problem. p : integer, optional (default = 2) Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used. metric : string or callable, default minkowski the distance metric to use for the tree.  The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. See the documentation of the DistanceMetric class for a list of available metrics. metric_params : dict, optional (default = None) Additional keyword arguments for the metric function. n_jobs : int, optional (default = 1) The number of parallel jobs to run for neighbors search. If -1 , then the number of jobs is set to the number of CPU cores. Doesnt affect fit method."},
{"libName": "sklearn.neighbors", "methods": [{"methodName": "__init__(bandwidth=1.0, algorithm=auto, kernel=gaussian, metric=euclidean, atol=0, rtol=0, breadth_first=True, leaf_size=40, metric_params=None)", "methodDesc": ""}, {"methodParams": ["X"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array_like, shape (n_samples, n_features)   List of n_features-dimensional data points.  Each row corresponds to a single data point.  ", "methodDesc": "Fit the Kernel Density model on the data."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "sample(n_samples=1, random_state=None)", "methodReturnsBody": "X : array_like, shape (n_samples, n_features)   List of samples.  ", "methodParams": ["n_samples", "random_state"], "methodReturns": ["X"], "methodDesc": "Generate random samples from the model. Currently, this is implemented only for gaussian and tophat kernels.", "methodParamsBody": "n_samples : int, optional   Number of samples to generate. Defaults to 1.   random_state : int, RandomState instance or None. default to None   If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .  "}, {"methodName": "score(X, y=None)", "methodReturnsBody": "logprob : float   Total log-likelihood of the data in X.  ", "methodParams": ["X"], "methodReturns": ["logprob"], "methodDesc": "Compute the total log probability under the model.", "methodParamsBody": "X : array_like, shape (n_samples, n_features)   List of n_features-dimensional data points.  Each row corresponds to a single data point.  "}, {"methodName": "score_samples(X)", "methodReturnsBody": "density : ndarray, shape (n_samples,)   The array of log(density) evaluations.  ", "methodParams": ["X"], "methodReturns": ["density"], "methodDesc": "Evaluate the density model on the data.", "methodParamsBody": "X : array_like, shape (n_samples, n_features)   An array of points to query.  Last dimension should match dimension of training data (n_features).  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "", "funcName": "KernelDensity", "allFuncParams": ["bandwidth", "algorithm", "kernel", "metric", "atol", "rtol", "breadth_first", "leaf_size", "metric_params"], "funcDesc": "Kernel Density Estimation", "funcParamBody": "bandwidth : float The bandwidth of the kernel. algorithm : string The tree algorithm to use.  Valid options are [kd_tree|ball_tree|auto].  Default is auto. kernel : string The kernel to use.  Valid kernels are [gaussian|tophat|epanechnikov|exponential|linear|cosine] Default is gaussian. metric : string The distance metric to use.  Note that not all metrics are valid with all algorithms.  Refer to the documentation of BallTree and KDTree for a description of available algorithms.  Note that the normalization of the density output is correct only for the Euclidean distance metric. Default is euclidean. atol : float The desired absolute tolerance of the result.  A larger tolerance will generally lead to faster execution. Default is 0. rtol : float The desired relative tolerance of the result.  A larger tolerance will generally lead to faster execution.  Default is 1E-8. breadth_first : boolean If true (default), use a breadth-first approach to the problem. Otherwise use a depth-first approach. leaf_size : int Specify the leaf size of the underlying tree.  See BallTree KDTree for details.  Default is 40. metric_params : dict Additional parameters to be passed to the tree for use with the metric.  For more information, see the documentation of BallTree or KDTree ."},
{"libName": "sklearn.neighbors", "methods": [{"methodName": "__init__()", "methodDesc": "Initialize self.  See help(type(self)) for accurate signature."}, {"methodName": "kernel_density(self, X, h, kernel=gaussian, atol=0, rtol=1E-8, breadth_first=True, return_log=False)", "methodReturnsBody": "density : ndarray   The array of (log)-density evaluations, shape = X.shape[:-1]  ", "methodParams": ["X", "h", "kernel", "atol, rtol", "breadth_first", "return_log"], "methodReturns": ["density"], "methodDesc": "Compute the kernel density estimate at points X with the given kernel, using the distance metric specified at tree creation. Examples Compute a gaussian kernel density estimate:", "methodParamsBody": "X : array_like   An array of points to query.  Last dimension should match dimension of training data.   h : float   the bandwidth of the kernel   kernel : string   specify the kernel to use.  Options are - gaussian - tophat - epanechnikov - exponential - linear - cosine Default is kernel = gaussian   atol, rtol : float (default = 0)   Specify the desired relative and absolute tolerance of the result. If the true result is K_true, then the returned result K_ret satisfies abs(K_true  -  K_ret)  <  atol  +  rtol  *  K_ret The default is zero (i.e. machine precision) for both.   breadth_first : boolean (default = False)   if True, use a breadth-first search.  If False (default) use a depth-first search.  Breadth-first is generally faster for compact kernels and/or high tolerances.   return_log : boolean (default = False)   return the logarithm of the result.  This can be more accurate than returning the result itself for narrow kernels.  "}, {"methodName": "query(X, k=1, return_distance=True, dualtree=False, breadth_first=False)", "methodReturnsBody": "i : if return_distance == False  (d,i) : if return_distance == True  d : array of doubles - shape: x.shape[:-1] + (k,)   each entry gives the list of distances to the neighbors of the corresponding point   i : array of integers - shape: x.shape[:-1] + (k,)   each entry gives the list of indices of neighbors of the corresponding point  ", "methodParams": ["X", "k", "return_distance", "dualtree", "breadth_first", "sort_results"], "methodReturns": ["i", "(d,i)", "d", "i"], "methodDesc": "query the tree for the k nearest neighbors Examples Query for k-nearest neighbors", "methodParamsBody": "X : array-like, last dimension self.dim   An array of points to query   k : integer  (default = 1)   The number of nearest neighbors to return   return_distance : boolean (default = True)   if True, return a tuple (d, i) of distances and indices if False, return array i   dualtree : boolean (default = False)   if True, use the dual tree formalism for the query: a tree is built for the query points, and the pair of trees is used to efficiently search this space.  This can lead to better performance as the number of points grows large.   breadth_first : boolean (default = False)   if True, then query the nodes in a breadth-first manner. Otherwise, query the nodes in a depth-first manner.   sort_results : boolean (default = True)   if True, then distances and indices of each point are sorted on return, so that the first column contains the closest points. Otherwise, neighbors are returned in an arbitrary order.  "}, {"methodName": "query_radius()", "methodReturnsBody": "count : if count_only == True  ind : if count_only == False and return_distance == False  (ind, dist) : if count_only == False and return_distance == True  count : array of integers, shape = X.shape[:-1]   each entry gives the number of neighbors within a distance r of the corresponding point.   ind : array of objects, shape = X.shape[:-1]   each element is a numpy integer array listing the indices of neighbors of the corresponding point.  Note that unlike the results of a k-neighbors query, the returned neighbors are not sorted by distance by default.   dist : array of objects, shape = X.shape[:-1]   each element is a numpy double array listing the distances corresponding to indices in i.  ", "methodParams": ["X", "r", "return_distance", "count_only", "sort_results"], "methodReturns": ["count", "ind", "(ind, dist)", "count", "ind", "dist"], "methodDesc": "query_radius(self, X, r, count_only = False): query the tree for neighbors within a radius r Examples Query for neighbors in a given radius", "methodParamsBody": "X : array-like, last dimension self.dim   An array of points to query   r : distance within which neighbors are returned   r can be a single value, or an array of values of shape x.shape[:-1] if different radii are desired for each point.   return_distance : boolean (default = False)   if True,  return distances to neighbors of each point if False, return only neighbors Note that unlike the query() method, setting return_distance=True here adds to the computation time.  Not all distances need to be calculated explicitly for return_distance=False.  Results are not sorted by default: see sort_results keyword.   count_only : boolean (default = False)   if True,  return only the count of points within distance r if False, return the indices of all points within distance r If return_distance==True, setting count_only=True will result in an error.   sort_results : boolean (default = False)   if True, the distances and indices will be sorted before being returned.  If False, the results will not be sorted.  If return_distance == False, setting sort_results = True will result in an error.  "}, {"methodName": "two_point_correlation()", "methodReturnsBody": "counts : ndarray   counts[i] contains the number of pairs of points with distance less than or equal to r[i]  ", "methodParams": ["X", "r", "dualtree"], "methodReturns": ["counts"], "methodDesc": "Compute the two-point correlation function Examples Compute the two-point autocorrelation function of X:", "methodParamsBody": "X : array_like   An array of points to query.  Last dimension should match dimension of training data.   r : array_like   A one-dimensional array of distances   dualtree : boolean (default = False)   If true, use a dualtree algorithm.  Otherwise, use a single-tree algorithm.  Dual tree algorithms can have better scaling for large N.  "}], "allFuncParams": ["X", "leaf_size", "metric", "Additional keywords are passed to the distance metric class."], "notes": "", "funcName": "KDTree", "allFuncAttributes": ["data"], "funcDesc": "KDTree for fast generalized N-point problems", "funcParamBody": "X : array-like, shape = [n_samples, n_features] n_samples is the number of points in the data set, and n_features is the dimension of the parameter space. Note: if X is a C-contiguous array of doubles then data will not be copied. Otherwise, an internal copy will be made. leaf_size : positive integer (default = 40) Number of points at which to switch to brute-force. Changing leaf_size will not affect the results of a query, but can significantly impact the speed of a query and the memory required to store the constructed tree.  The amount of memory needed to store the tree scales as approximately n_samples / leaf_size. For a specified leaf_size , a leaf node is guaranteed to satisfy leaf_size  <=  n_points  <=  2  *  leaf_size , except in the case that n_samples  <  leaf_size . metric : string or DistanceMetric object the distance metric to use for the tree.  Default=minkowski with p=2 (that is, a euclidean metric). See the documentation of the DistanceMetric class for a list of available metrics. kd_tree.valid_metrics gives a list of the metrics which are valid for KDTree. Additional keywords are passed to the distance metric class. :", "funcAttrBody": "data : np.ndarray The training data"},
{"funcName": "DistanceMetric", "notes": "", "libName": "sklearn.neighbors", "methods": [{"methodName": "__init__()", "methodDesc": "Initialize self.  See help(type(self)) for accurate signature."}, {"methodName": "dist_to_rdist()", "methodDesc": "Convert the true distance to the reduced distance. The reduced distance, defined for some metrics, is a computationally more efficient measure which preserves the rank of the true distance. For example, in the Euclidean distance metric, the reduced distance is the squared-euclidean distance."}, {"methodParams": ["metric", "**kwargs"], "methodName": "get_metric()", "methodParamsBody": "metric : string or class name   The distance metric to use   **kwargs :   additional arguments will be passed to the requested metric  ", "methodDesc": "Get the given distance metric from the string identifier. See the docstring of DistanceMetric for a list of available metrics."}, {"methodParams": ["X", "Y", "Returns", "\u2014\u2014-", "dist"], "methodName": "pairwise()", "methodParamsBody": "X : array_like   Array of shape (Nx, D), representing Nx points in D dimensions.   Y : array_like (optional)   Array of shape (Ny, D), representing Ny points in D dimensions. If not specified, then Y=X.   Returns :  - :  dist : ndarray   The shape (Nx, Ny) array of pairwise distances between points in X and Y.  ", "methodDesc": "Compute the pairwise distances between X and Y This is a convenience routine for the sake of testing.  For many metrics, the utilities in scipy.spatial.distance.cdist and scipy.spatial.distance.pdist will be faster."}, {"methodName": "rdist_to_dist()", "methodDesc": "Convert the Reduced distance to the true distance. The reduced distance, defined for some metrics, is a computationally more efficient measure which preserves the rank of the true distance. For example, in the Euclidean distance metric, the reduced distance is the squared-euclidean distance."}], "funcDesc": "DistanceMetric class"},
{"libName": "sklearn.neighbors", "methods": [{"methodName": "__init__()", "methodDesc": "Initialize self.  See help(type(self)) for accurate signature."}, {"methodName": "kernel_density(self, X, h, kernel=gaussian, atol=0, rtol=1E-8, breadth_first=True, return_log=False)", "methodReturnsBody": "density : ndarray   The array of (log)-density evaluations, shape = X.shape[:-1]  ", "methodParams": ["X", "h", "kernel", "atol, rtol", "breadth_first", "return_log"], "methodReturns": ["density"], "methodDesc": "Compute the kernel density estimate at points X with the given kernel, using the distance metric specified at tree creation. Examples Compute a gaussian kernel density estimate:", "methodParamsBody": "X : array_like   An array of points to query.  Last dimension should match dimension of training data.   h : float   the bandwidth of the kernel   kernel : string   specify the kernel to use.  Options are - gaussian - tophat - epanechnikov - exponential - linear - cosine Default is kernel = gaussian   atol, rtol : float (default = 0)   Specify the desired relative and absolute tolerance of the result. If the true result is K_true, then the returned result K_ret satisfies abs(K_true  -  K_ret)  <  atol  +  rtol  *  K_ret The default is zero (i.e. machine precision) for both.   breadth_first : boolean (default = False)   if True, use a breadth-first search.  If False (default) use a depth-first search.  Breadth-first is generally faster for compact kernels and/or high tolerances.   return_log : boolean (default = False)   return the logarithm of the result.  This can be more accurate than returning the result itself for narrow kernels.  "}, {"methodName": "query(X, k=1, return_distance=True, dualtree=False, breadth_first=False)", "methodReturnsBody": "i : if return_distance == False  (d,i) : if return_distance == True  d : array of doubles - shape: x.shape[:-1] + (k,)   each entry gives the list of distances to the neighbors of the corresponding point   i : array of integers - shape: x.shape[:-1] + (k,)   each entry gives the list of indices of neighbors of the corresponding point  ", "methodParams": ["X", "k", "return_distance", "dualtree", "breadth_first", "sort_results"], "methodReturns": ["i", "(d,i)", "d", "i"], "methodDesc": "query the tree for the k nearest neighbors Examples Query for k-nearest neighbors", "methodParamsBody": "X : array-like, last dimension self.dim   An array of points to query   k : integer  (default = 1)   The number of nearest neighbors to return   return_distance : boolean (default = True)   if True, return a tuple (d, i) of distances and indices if False, return array i   dualtree : boolean (default = False)   if True, use the dual tree formalism for the query: a tree is built for the query points, and the pair of trees is used to efficiently search this space.  This can lead to better performance as the number of points grows large.   breadth_first : boolean (default = False)   if True, then query the nodes in a breadth-first manner. Otherwise, query the nodes in a depth-first manner.   sort_results : boolean (default = True)   if True, then distances and indices of each point are sorted on return, so that the first column contains the closest points. Otherwise, neighbors are returned in an arbitrary order.  "}, {"methodName": "query_radius()", "methodReturnsBody": "count : if count_only == True  ind : if count_only == False and return_distance == False  (ind, dist) : if count_only == False and return_distance == True  count : array of integers, shape = X.shape[:-1]   each entry gives the number of neighbors within a distance r of the corresponding point.   ind : array of objects, shape = X.shape[:-1]   each element is a numpy integer array listing the indices of neighbors of the corresponding point.  Note that unlike the results of a k-neighbors query, the returned neighbors are not sorted by distance by default.   dist : array of objects, shape = X.shape[:-1]   each element is a numpy double array listing the distances corresponding to indices in i.  ", "methodParams": ["X", "r", "return_distance", "count_only", "sort_results"], "methodReturns": ["count", "ind", "(ind, dist)", "count", "ind", "dist"], "methodDesc": "query_radius(self, X, r, count_only = False): query the tree for neighbors within a radius r Examples Query for neighbors in a given radius", "methodParamsBody": "X : array-like, last dimension self.dim   An array of points to query   r : distance within which neighbors are returned   r can be a single value, or an array of values of shape x.shape[:-1] if different radii are desired for each point.   return_distance : boolean (default = False)   if True,  return distances to neighbors of each point if False, return only neighbors Note that unlike the query() method, setting return_distance=True here adds to the computation time.  Not all distances need to be calculated explicitly for return_distance=False.  Results are not sorted by default: see sort_results keyword.   count_only : boolean (default = False)   if True,  return only the count of points within distance r if False, return the indices of all points within distance r If return_distance==True, setting count_only=True will result in an error.   sort_results : boolean (default = False)   if True, the distances and indices will be sorted before being returned.  If False, the results will not be sorted.  If return_distance == False, setting sort_results = True will result in an error.  "}, {"methodName": "two_point_correlation()", "methodReturnsBody": "counts : ndarray   counts[i] contains the number of pairs of points with distance less than or equal to r[i]  ", "methodParams": ["X", "r", "dualtree"], "methodReturns": ["counts"], "methodDesc": "Compute the two-point correlation function Examples Compute the two-point autocorrelation function of X:", "methodParamsBody": "X : array_like   An array of points to query.  Last dimension should match dimension of training data.   r : array_like   A one-dimensional array of distances   dualtree : boolean (default = False)   If true, use a dualtree algorithm.  Otherwise, use a single-tree algorithm.  Dual tree algorithms can have better scaling for large N.  "}], "allFuncParams": ["X", "leaf_size", "metric", "Additional keywords are passed to the distance metric class."], "notes": "", "funcName": "BallTree", "allFuncAttributes": ["data"], "funcDesc": "BallTree for fast generalized N-point problems", "funcParamBody": "X : array-like, shape = [n_samples, n_features] n_samples is the number of points in the data set, and n_features is the dimension of the parameter space. Note: if X is a C-contiguous array of doubles then data will not be copied. Otherwise, an internal copy will be made. leaf_size : positive integer (default = 40) Number of points at which to switch to brute-force. Changing leaf_size will not affect the results of a query, but can significantly impact the speed of a query and the memory required to store the constructed tree.  The amount of memory needed to store the tree scales as approximately n_samples / leaf_size. For a specified leaf_size , a leaf node is guaranteed to satisfy leaf_size  <=  n_points  <=  2  *  leaf_size , except in the case that n_samples  <  leaf_size . metric : string or DistanceMetric object the distance metric to use for the tree.  Default=minkowski with p=2 (that is, a euclidean metric). See the documentation of the DistanceMetric class for a list of available metrics. ball_tree.valid_metrics gives a list of the metrics which are valid for BallTree. Additional keywords are passed to the distance metric class. :", "funcAttrBody": "data : np.ndarray The training data"},
{"libName": "sklearn.naive_bayes", "methods": [{"methodName": "__init__(alpha=1.0, fit_prior=True, class_prior=None)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit Naive Bayes classifier according to X, y", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples]   Target values.   sample_weight : array-like, shape = [n_samples], (default=None)   Weights applied to individual samples (1. for unweighted).  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "partial_fit(X, y, classes=None, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "classes", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Incremental fit on a batch of samples. This method is expected to be called several times consecutively on different chunks of a dataset so as to implement out-of-core or online learning. This is especially useful when the whole dataset is too big to fit in memory at once. This method has some performance overhead hence it is better to call partial_fit on chunks of data that are as large as possible (as long as fitting in the memory budget) to hide the overhead.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples]   Target values.   classes : array-like, shape = [n_classes] (default=None)   List of all the classes that can possibly appear in the y vector.  Must be provided at the first call to partial_fit, can be omitted in subsequent calls.   sample_weight : array-like, shape = [n_samples] (default=None)   Weights applied to individual samples (1. for unweighted).  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples]   Predicted target values for X  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Perform classification on an array of test vectors X.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "C : array-like, shape = [n_samples, n_classes]   Returns the log-probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Return log-probability estimates for the test vector X.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "C : array-like, shape = [n_samples, n_classes]   Returns the probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Return probability estimates for the test vector X.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["alpha", "fit_prior", "class_prior"], "notes": "For the rationale behind the names coef_ and intercept_ , i.e. naive Bayes as a linear classifier, see J. Rennie et al. (2003), Tackling the poor assumptions of naive Bayes text classifiers, ICML. C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html For the rationale behind the names coef_ and intercept_ , i.e. naive Bayes as a linear classifier, see J. Rennie et al. (2003), Tackling the poor assumptions of naive Bayes text classifiers, ICML. C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. http://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html", "funcName": "MultinomialNB", "allFuncAttributes": ["class_log_prior_", "intercept_", "feature_log_prob_", "coef_", "class_count_", "feature_count_"], "funcDesc": "Naive Bayes classifier for multinomial models", "funcParamBody": "alpha : float, optional (default=1.0) Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing). fit_prior : boolean, optional (default=True) Whether to learn class prior probabilities or not. If false, a uniform prior will be used. class_prior : array-like, size (n_classes,), optional (default=None) Prior probabilities of the classes. If specified the priors are not adjusted according to the data.", "funcAttrBody": "class_log_prior_ : array, shape (n_classes, ) Smoothed empirical log probability for each class. intercept_ : property Mirrors class_log_prior_ for interpreting MultinomialNB as a linear model. feature_log_prob_ : array, shape (n_classes, n_features) Empirical log probability of features given a class, P(x_i|y) . coef_ : property Mirrors feature_log_prob_ for interpreting MultinomialNB as a linear model. class_count_ : array, shape (n_classes,) Number of samples encountered for each class during fitting. This value is weighted by the sample weight when provided. feature_count_ : array, shape (n_classes, n_features) Number of samples encountered for each (class, feature) during fitting. This value is weighted by the sample weight when provided."},
{"libName": "sklearn.naive_bayes", "methods": [{"methodName": "__init__(priors=None)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit Gaussian Naive Bayes according to X, y", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training vectors, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape (n_samples,)   Target values.   sample_weight : array-like, shape (n_samples,), optional (default=None)   Weights applied to individual samples (1. for unweighted).   New in version 0.17: Gaussian Naive Bayes supports fitting with sample_weight .   "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "partial_fit(X, y, classes=None, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "classes", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Incremental fit on a batch of samples. This method is expected to be called several times consecutively on different chunks of a dataset so as to implement out-of-core or online learning. This is especially useful when the whole dataset is too big to fit in memory at once. This method has some performance and numerical stability overhead, hence it is better to call partial_fit on chunks of data that are as large as possible (as long as fitting in the memory budget) to hide the overhead.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training vectors, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape (n_samples,)   Target values.   classes : array-like, shape (n_classes,), optional (default=None)   List of all the classes that can possibly appear in the y vector.  Must be provided at the first call to partial_fit, can be omitted in subsequent calls.   sample_weight : array-like, shape (n_samples,), optional (default=None)   Weights applied to individual samples (1. for unweighted).   New in version 0.17.   "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples]   Predicted target values for X  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Perform classification on an array of test vectors X.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "C : array-like, shape = [n_samples, n_classes]   Returns the log-probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Return log-probability estimates for the test vector X.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "C : array-like, shape = [n_samples, n_classes]   Returns the probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Return probability estimates for the test vector X.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["priors"], "notes": "", "funcName": "GaussianNB", "allFuncAttributes": ["class_prior_", "class_count_", "theta_", "sigma_"], "funcDesc": "Gaussian Naive Bayes (GaussianNB)", "funcParamBody": "priors : array-like, shape (n_classes,) Prior probabilities of the classes. If specified the priors are not adjusted according to the data.", "funcAttrBody": "class_prior_ : array, shape (n_classes,) probability of each class. class_count_ : array, shape (n_classes,) number of training samples observed in each class. theta_ : array, shape (n_classes, n_features) mean of each feature per class sigma_ : array, shape (n_classes, n_features) variance of each feature per class"},
{"libName": "sklearn.naive_bayes", "methods": [{"methodName": "__init__(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit Naive Bayes classifier according to X, y", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples]   Target values.   sample_weight : array-like, shape = [n_samples], (default=None)   Weights applied to individual samples (1. for unweighted).  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "partial_fit(X, y, classes=None, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "classes", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Incremental fit on a batch of samples. This method is expected to be called several times consecutively on different chunks of a dataset so as to implement out-of-core or online learning. This is especially useful when the whole dataset is too big to fit in memory at once. This method has some performance overhead hence it is better to call partial_fit on chunks of data that are as large as possible (as long as fitting in the memory budget) to hide the overhead.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples]   Target values.   classes : array-like, shape = [n_classes] (default=None)   List of all the classes that can possibly appear in the y vector.  Must be provided at the first call to partial_fit, can be omitted in subsequent calls.   sample_weight : array-like, shape = [n_samples] (default=None)   Weights applied to individual samples (1. for unweighted).  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples]   Predicted target values for X  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Perform classification on an array of test vectors X.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "C : array-like, shape = [n_samples, n_classes]   Returns the log-probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Return log-probability estimates for the test vector X.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "C : array-like, shape = [n_samples, n_classes]   Returns the probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Return probability estimates for the test vector X.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["alpha", "binarize", "fit_prior", "class_prior"], "notes": "", "funcName": "BernoulliNB", "allFuncAttributes": ["class_log_prior_", "feature_log_prob_", "class_count_", "feature_count_"], "funcDesc": "Naive Bayes classifier for multivariate Bernoulli models.", "funcParamBody": "alpha : float, optional (default=1.0) Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing). binarize : float or None, optional (default=0.0) Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors. fit_prior : boolean, optional (default=True) Whether to learn class prior probabilities or not. If false, a uniform prior will be used. class_prior : array-like, size=[n_classes,], optional (default=None) Prior probabilities of the classes. If specified the priors are not adjusted according to the data.", "funcAttrBody": "class_log_prior_ : array, shape = [n_classes] Log probability of each class (smoothed). feature_log_prob_ : array, shape = [n_classes, n_features] Empirical log probability of features given a class, P(x_i|y). class_count_ : array, shape = [n_classes] Number of samples encountered for each class during fitting. This value is weighted by the sample weight when provided. feature_count_ : array, shape = [n_classes, n_features] Number of samples encountered for each (class, feature) during fitting. This value is weighted by the sample weight when provided."},
{"libName": "sklearn.multioutput", "methods": [{"methodName": "__init__(estimator, n_jobs=1)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the model to data. Fit a separate model for each output variable.", "methodParamsBody": "X : (sparse) array-like, shape (n_samples, n_features)   Data.   y : (sparse) array-like, shape (n_samples, n_outputs)   Multi-output targets. An indicator matrix turns on multilabel estimation.   sample_weight : array-like, shape = (n_samples) or None   Sample weights. If None, then samples are equally weighted. Only supported if the underlying regressor supports sample weights.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "partial_fit(X, y, classes=None, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "classes", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Incrementally fit the model to data. Fit a separate model for each output variable.", "methodParamsBody": "X : (sparse) array-like, shape (n_samples, n_features)   Data.   y : (sparse) array-like, shape (n_samples, n_outputs)   Multi-output targets.   classes : list of numpy arrays, shape (n_outputs)   Each array is unique classes for one output in str/int Can be obtained by via [np.unique(y[:,  i])  for  i  in  range(y.shape[1])] , where y is the target matrix of the entire dataset. This argument is required for the first call to partial_fit and can be omitted in the subsequent calls. Note that y doesnt need to contain all labels in classes .   sample_weight : array-like, shape = (n_samples) or None   Sample weights. If None, then samples are equally weighted. Only supported if the underlying regressor supports sample weights.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : (sparse) array-like, shape (n_samples, n_outputs)   Multi-output targets predicted across multiple predictors. Note: Separate models are generated for each predictor.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "", "methodParamsBody": "X : (sparse) array-like, shape (n_samples, n_features)   Data.  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "p : array of shape = [n_samples, n_classes], or a list of n_outputs             such arrays if n_outputs > 1.   The class probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Probability estimates. Returns prediction probabilities for each class of each output.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Data  "}, {"methodName": "score(X, y)", "methodReturnsBody": "scores : float   accuracy_score of self.predict(X) versus y  ", "methodParams": ["X", "y"], "methodReturns": ["scores"], "methodDesc": "Returns the mean accuracy on the given test data and labels.", "methodParamsBody": "X : array-like, shape [n_samples, n_features]   Test samples   y : array-like, shape [n_samples, n_outputs]   True values for X  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["estimator", "n_jobs"], "notes": "", "funcName": "MultiOutputClassifier", "allFuncAttributes": ["estimators_"], "funcDesc": "Multi target classification", "funcParamBody": "estimator : estimator object An estimator object implementing fit , score and predict_proba . n_jobs : int, optional, default=1 The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. The number of jobs to use for the computation. It does each target variable in y in parallel.", "funcAttrBody": "estimators_ : list of n_output estimators Estimators used for predictions."},
{"libName": "sklearn.multioutput", "methods": [{"methodName": "__init__(estimator, n_jobs=1)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the model to data. Fit a separate model for each output variable.", "methodParamsBody": "X : (sparse) array-like, shape (n_samples, n_features)   Data.   y : (sparse) array-like, shape (n_samples, n_outputs)   Multi-output targets. An indicator matrix turns on multilabel estimation.   sample_weight : array-like, shape = (n_samples) or None   Sample weights. If None, then samples are equally weighted. Only supported if the underlying regressor supports sample weights.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "partial_fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Incrementally fit the model to data. Fit a separate model for each output variable.", "methodParamsBody": "X : (sparse) array-like, shape (n_samples, n_features)   Data.   y : (sparse) array-like, shape (n_samples, n_outputs)   Multi-output targets.   sample_weight : array-like, shape = (n_samples) or None   Sample weights. If None, then samples are equally weighted. Only supported if the underlying regressor supports sample weights.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : (sparse) array-like, shape (n_samples, n_outputs)   Multi-output targets predicted across multiple predictors. Note: Separate models are generated for each predictor.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "", "methodParamsBody": "X : (sparse) array-like, shape (n_samples, n_features)   Data.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the regression sum of squares ((y_true - y_true.mean()) ** 2).sum(). Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Notes R^2 is calculated by weighting all the targets equally using multioutput=uniform_average .", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Test samples.   y : array-like, shape (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "R^2 is calculated by weighting all the targets equally using multioutput=uniform_average . R^2 is calculated by weighting all the targets equally using multioutput=uniform_average .", "funcName": "MultiOutputRegressor", "allFuncParams": ["estimator", "n_jobs"], "funcDesc": "Multi target regression", "funcParamBody": "estimator : estimator object An estimator object implementing fit and predict . n_jobs : int, optional, default=1 The number of jobs to run in parallel for fit . If -1, then the number of jobs is set to the number of cores. When individual estimators are fast to train or predict using n_jobs>1 can result in slower performance due to the overhead of spawning processes."},
{"libName": "sklearn.multioutput", "methods": [{"methodName": "__init__(base_estimator, order=None, cv=None, random_state=None)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "Y_decision : array-like, shape (n_samples, n_classes )   Returns the decision function of the sample for each model in the chain.  ", "methodParams": ["X"], "methodReturns": ["Y_decision"], "methodDesc": "Evaluate the decision_function of the models in the chain.", "methodParamsBody": "X : array-like, shape (n_samples, n_features) "}, {"methodName": "fit(X, Y)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "Y"], "methodReturns": ["self"], "methodDesc": "Fit the model to data matrix X and targets Y.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   The input data.   Y : array-like, shape (n_samples, n_classes)   The target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "Y_pred : array-like, shape (n_samples, n_classes)   The predicted values.  ", "methodParams": ["X"], "methodReturns": ["Y_pred"], "methodDesc": "Predict on the data matrix X using the ClassifierChain model.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   The input data.  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "Y_prob : array-like, shape (n_samples, n_classes)", "methodParams": [], "methodReturns": [], "methodDesc": "Predict probability estimates.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)"}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["base_estimator", "order", "cv", "default=None)", "random_state"], "notes": "", "funcName": "ClassifierChain", "allFuncAttributes": ["classes_", "estimators_", "order_"], "funcDesc": "A multi-label model that arranges binary classifiers into a chain.", "funcParamBody": "base_estimator : estimator The base estimator from which the classifier chain is built. order : array-like, shape=[n_outputs] or random, optional By default the order will be determined by the order of columns in the label matrix Y.: order  =  [ 0 ,  1 ,  2 ,  ... ,  Y . shape [ 1 ]  -  1 ] The order of the chain can be explicitly set by providing a list of integers. For example, for a chain of length 5.: order  =  [ 1 ,  3 ,  2 ,  4 ,  0 ] means that the first model in the chain will make predictions for column 1 in the Y matrix, the second model will make predictions for column 3, etc. If order is random a random ordering will be used. cv : int, cross-validation generator or an iterable, optional ( default=None) : Determines whether to use cross validated predictions or true labels for the results of previous estimators in the chain. If cv is None the true labels are used when fitting. Otherwise possible inputs for cv are: integer, to specify the number of folds in a (Stratified)KFold, An object to be used as a cross-validation generator. An iterable yielding train, test splits. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . The random number generator is used to generate random chain orders.", "funcAttrBody": "classes_ : list A list of arrays of length len(estimators_) containing the class labels for each estimator in the chain. estimators_ : list A list of clones of base_estimator. order_ : list The order of labels in the classifier chain."},
{"funcName": "OutputCodeClassifier", "notes": "", "libName": "sklearn.multiclass", "methods": [{"methodName": "__init__(estimator, code_size=1.5, random_state=None, n_jobs=1)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit underlying estimators.", "methodParamsBody": "X : (sparse) array-like, shape = [n_samples, n_features]   Data.   y : numpy array of shape [n_samples]   Multi-class targets.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : numpy array of shape [n_samples]   Predicted multi-class targets.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict multi-class targets using underlying estimators.", "methodParamsBody": "X : (sparse) array-like, shape = [n_samples, n_features]   Data.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "(Error-Correcting) Output-Code multiclass strategy"},
{"libName": "sklearn.multiclass", "methods": [{"methodName": "__init__(estimator, n_jobs=1)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "Y : array-like, shape = [n_samples, n_classes]", "methodParams": [], "methodReturns": [], "methodDesc": "Decision function for the OneVsOneClassifier. The decision values for the samples are computed by adding the normalized sum of pair-wise classification confidence levels to the votes in order to disambiguate between the decision values when the votes for all the classes are equal leading to a tie.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]"}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit underlying estimators.", "methodParamsBody": "X : (sparse) array-like, shape = [n_samples, n_features]   Data.   y : array-like, shape = [n_samples]   Multi-class targets.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "partial_fit(X, y, classes=None)", "methodReturnsBody": "self : ", "methodParams": ["X", "y", "classes"], "methodReturns": ["self"], "methodDesc": "Partially fit underlying estimators Should be used when memory is inefficient to train all data. Chunks of data can be passed in several iteration, where the first call should have an array of all target variables.", "methodParamsBody": "X : (sparse) array-like, shape = [n_samples, n_features]   Data.   y : array-like, shape = [n_samples]   Multi-class targets.   classes : array, shape (n_classes, )   Classes across all calls to partial_fit. Can be obtained via np.unique(y_all) , where y_all is the target vector of the entire dataset. This argument is only required in the first call of partial_fit and can be omitted in the subsequent calls.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : numpy array of shape [n_samples]   Predicted multi-class targets.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Estimate the best class label for each sample in X. This is implemented as argmax(decision_function(X),  axis=1) which will return the label of the class with most votes by estimators predicting the outcome of a decision for each possible class pair.", "methodParamsBody": "X : (sparse) array-like, shape = [n_samples, n_features]   Data.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["estimator", "n_jobs"], "notes": "", "funcName": "OneVsOneClassifier", "allFuncAttributes": ["estimators_", "classes_"], "funcDesc": "One-vs-one multiclass strategy", "funcParamBody": "estimator : estimator object An estimator object implementing fit and one of decision_function predict_proba . n_jobs : int, optional, default: 1 The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.", "funcAttrBody": "estimators_ : list of n_classes * (n_classes - 1) / 2 estimators Estimators used for predictions. classes_ : numpy array of shape [n_classes] Array containing labels."},
{"libName": "sklearn.multiclass", "methods": [{"methodName": "__init__(estimator, n_jobs=1)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "T : array-like, shape = [n_samples, n_classes]", "methodParams": [], "methodReturns": [], "methodDesc": "Returns the distance of each sample from the decision boundary for each class. This can only be used with estimators which implement the decision_function method.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]"}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit underlying estimators.", "methodParamsBody": "X : (sparse) array-like, shape = [n_samples, n_features]   Data.   y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]   Multi-class targets. An indicator matrix turns on multilabel classification.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "partial_fit(X, y, classes=None)", "methodReturnsBody": "self : ", "methodParams": ["X", "y", "classes"], "methodReturns": ["self"], "methodDesc": "Partially fit underlying estimators Should be used when memory is inefficient to train all data. Chunks of data can be passed in several iteration.", "methodParamsBody": "X : (sparse) array-like, shape = [n_samples, n_features]   Data.   y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes]   Multi-class targets. An indicator matrix turns on multilabel classification.   classes : array, shape (n_classes, )   Classes across all calls to partial_fit. Can be obtained via np.unique(y_all) , where y_all is the target vector of the entire dataset. This argument is only required in the first call of partial_fit and can be omitted in the subsequent calls.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : (sparse) array-like, shape = [n_samples, ], [n_samples, n_classes].   Predicted multi-class targets.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict multi-class targets using underlying estimators.", "methodParamsBody": "X : (sparse) array-like, shape = [n_samples, n_features]   Data.  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "T : (sparse) array-like, shape = [n_samples, n_classes]   Returns the probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ .  ", "methodParams": ["X"], "methodReturns": ["T"], "methodDesc": "Probability estimates. The returned estimates for all classes are ordered by label of classes. Note that in the multilabel case, each sample can have any number of labels. This returns the marginal probability that the given sample has the label in question. For example, it is entirely consistent that two labels both have a 90% probability of applying to a given sample. In the single label multiclass case, the rows of the returned matrix sum to 1.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["estimator", "n_jobs"], "notes": "", "funcName": "OneVsRestClassifier", "allFuncAttributes": ["estimators_", "classes_", "label_binarizer_", "multilabel_"], "funcDesc": "One-vs-the-rest (OvR) multiclass/multilabel strategy", "funcParamBody": "estimator : estimator object An estimator object implementing fit and one of decision_function predict_proba . n_jobs : int, optional, default: 1 The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.", "funcAttrBody": "estimators_ : list of n_classes estimators Estimators used for predictions. classes_ : array, shape = [ n_classes ] Class labels. label_binarizer_ : LabelBinarizer object Object used to transform multiclass labels to binary labels and vice-versa. multilabel_ : boolean Whether a OneVsRestClassifier is a multilabel classifier."},
{"allReturnParams": ["train_scores", "test_scores"], "libName": "sklearn.model_selection", "methods": [], "notes": "See Plotting Validation Curves See Plotting Validation Curves", "funcName": "validation_curve", "allFuncParams": ["estimator", "X", "y", "param_name", "param_range", "groups", "cv", "scoring", "n_jobs", "pre_dispatch", "verbose"], "funcDesc": "Validation curve.", "funcParamBody": "estimator : object type that implements the fit and predict methods An object of that type which is cloned for each validation. X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape (n_samples) or (n_samples, n_features), optional Target relative to X for classification or regression; None for unsupervised learning. param_name : string Name of the parameter that will be varied. param_range : array-like, shape (n_values,) The values of the parameter that will be evaluated. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold , An object to be used as a cross-validation generator. An iterable yielding train, test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. scoring : string, callable or None, optional, default: None A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator,  X,  y) . n_jobs : integer, optional Number of jobs to run in parallel (default 1). pre_dispatch : integer or string, optional Number of predispatched jobs for parallel execution (default is all). The option can reduce the allocated memory. The string can be an expression like 2*n_jobs. verbose : integer, optional Controls the verbosity: the higher, the more messages.", "funcReturnBody": "train_scores : array, shape (n_ticks, n_cv_folds) Scores on training sets. test_scores : array, shape (n_ticks, n_cv_folds) Scores on test set."},
{"allReturnParams": ["score", "permutation_scores", "pvalue"], "libName": "sklearn.model_selection", "methods": [], "notes": "This function implements Test 1 in: This function implements Test 1 in:", "funcName": "permutation_test_score", "allFuncParams": ["estimator", "X", "y", "groups", "scoring", "cv", "n_permutations", "n_jobs", "random_state", "verbose"], "funcDesc": "Evaluate the significance of a cross-validated score with permutations", "funcParamBody": "estimator : estimator object implementing fit The object to use to fit the data. X : array-like of shape at least 2D The data to fit. y : array-like The target variable to try to predict in the case of supervised learning. groups : array-like, with shape (n_samples,), optional Labels to constrain permutation within groups, i.e. y values are permuted among samples with the same group identifier. When not specified, y values are permuted among all samples. When a grouped cross-validator is used, the group labels are also passed on to the split method of the cross-validator. The cross-validator uses them for grouping the samples  while splitting the dataset into train/test set. scoring : string, callable or None, optional, default: None A single string (see The scoring parameter: defining model evaluation rules ) or a callable (see Defining your scoring strategy from metric functions ) to evaluate the predictions on the test set. If None the estimators default scorer, if available, is used. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold , An object to be used as a cross-validation generator. An iterable yielding train, test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. n_permutations : integer, optional Number of times to permute y . n_jobs : integer, optional The number of CPUs to use to do the computation. -1 means all CPUs. random_state : int, RandomState instance or None, optional (default=0) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . verbose : integer, optional The verbosity level.", "funcReturnBody": "score : float The true score without permuting targets. permutation_scores : array, shape (n_permutations,) The scores obtained for each permutations. pvalue : float The p-value, which approximates the probability that the score would be obtained by chance. This is calculated as: (C + 1) / (n_permutations + 1) Where C is the number of permutations whose score >= the true score. The best possible p-value is 1/(n_permutations + 1), the worst is 1.0."},
{"allReturnParams": ["train_sizes_abs", "train_scores", "test_scores"], "libName": "sklearn.model_selection", "methods": [], "notes": "See examples/model_selection/plot_learning_curve.py", "funcName": "learning_curve", "allFuncParams": ["estimator", "X", "y", "groups", "train_sizes", "cv", "scoring", "exploit_incremental_learning", "n_jobs", "pre_dispatch", "verbose", "shuffle", "random_state"], "funcDesc": "Learning curve.", "funcParamBody": "estimator : object type that implements the fit and predict methods An object of that type which is cloned for each validation. X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. y : array-like, shape (n_samples) or (n_samples, n_features), optional Target relative to X for classification or regression; None for unsupervised learning. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. train_sizes : array-like, shape (n_ticks,), dtype float or int Relative or absolute numbers of training examples that will be used to generate the learning curve. If the dtype is float, it is regarded as a fraction of the maximum size of the training set (that is determined by the selected validation method), i.e. it has to be within (0, 1]. Otherwise it is interpreted as absolute sizes of the training sets. Note that for classification the number of samples usually have to be big enough to contain at least one sample from each class. (default: np.linspace(0.1, 1.0, 5)) cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold , An object to be used as a cross-validation generator. An iterable yielding train, test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. scoring : string, callable or None, optional, default: None A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator,  X,  y) . exploit_incremental_learning : boolean, optional, default: False If the estimator supports incremental learning, this will be used to speed up fitting for different training set sizes. n_jobs : integer, optional Number of jobs to run in parallel (default 1). pre_dispatch : integer or string, optional Number of predispatched jobs for parallel execution (default is all). The option can reduce the allocated memory. The string can be an expression like 2*n_jobs. verbose : integer, optional Controls the verbosity: the higher, the more messages. shuffle : boolean, optional Whether to shuffle training data before taking prefixes of it based on``train_sizes``. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when shuffle is True.", "funcReturnBody": "train_sizes_abs : array, shape = (n_unique_ticks,), dtype int Numbers of training examples that has been used to generate the learning curve. Note that the number of ticks might be less than n_ticks because duplicate entries will be removed. train_scores : array, shape (n_ticks, n_cv_folds) Scores on training sets. test_scores : array, shape (n_ticks, n_cv_folds) Scores on test set."},
{"allReturnParams": ["scores"], "libName": "sklearn.model_selection", "methods": [], "notes": "", "funcName": "cross_val_score", "allFuncParams": ["estimator", "X", "y", "groups", "scoring", "cv", "n_jobs", "verbose", "fit_params", "pre_dispatch"], "funcDesc": "Evaluate a score by cross-validation", "funcParamBody": "estimator : estimator object implementing fit The object to use to fit the data. X : array-like The data to fit. Can be for example a list, or an array. y : array-like, optional, default: None The target variable to try to predict in the case of supervised learning. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. scoring : string, callable or None, optional, default: None A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator,  X,  y) . cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold , An object to be used as a cross-validation generator. An iterable yielding train, test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. n_jobs : integer, optional The number of CPUs to use to do the computation. -1 means all CPUs. verbose : integer, optional The verbosity level. fit_params : dict, optional Parameters to pass to the fit method of the estimator. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs", "funcReturnBody": "scores : array of float, shape=(len(list(cv)),) Array of scores of the estimator for each run of the cross validation."},
{"allReturnParams": ["predictions"], "libName": "sklearn.model_selection", "methods": [], "notes": "In the case that one or more classes are absent in a training portion, a default score needs to be assigned to all instances for that class if method produces columns per class, as in {decision_function, predict_proba, predict_log_proba}.  For predict_proba this value is 0.  In order to ensure finite output, we approximate negative infinity by the minimum finite float value for the dtype in other cases. In the case that one or more classes are absent in a training portion, a default score needs to be assigned to all instances for that class if method produces columns per class, as in {decision_function, predict_proba, predict_log_proba}.  For predict_proba this value is 0.  In order to ensure finite output, we approximate negative infinity by the minimum finite float value for the dtype in other cases.", "funcName": "cross_val_predict", "allFuncParams": ["estimator", "X", "y", "groups", "cv", "n_jobs", "verbose", "fit_params", "pre_dispatch", "method"], "funcDesc": "Generate cross-validated estimates for each input data point", "funcParamBody": "estimator : estimator object implementing fit and predict The object to use to fit the data. X : array-like The data to fit. Can be, for example a list, or an array at least 2d. y : array-like, optional, default: None The target variable to try to predict in the case of supervised learning. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold , An object to be used as a cross-validation generator. An iterable yielding train, test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. n_jobs : integer, optional The number of CPUs to use to do the computation. -1 means all CPUs. verbose : integer, optional The verbosity level. fit_params : dict, optional Parameters to pass to the fit method of the estimator. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs method : string, optional, default: predict Invokes the passed method name of the passed estimator. For method=predict_proba, the columns correspond to the classes in sorted order.", "funcReturnBody": "predictions : ndarray This is the result of calling method"},
{"allReturnParams": ["scores"], "libName": "sklearn.model_selection", "methods": [], "notes": "", "funcName": "cross_validate", "allFuncParams": ["estimator", "X", "y", "groups", "scoring", "cv", "n_jobs", "verbose", "fit_params", "pre_dispatch", "return_train_score"], "funcDesc": "Evaluate metric(s) by cross-validation and also record fit/score times.", "funcParamBody": "estimator : estimator object implementing fit The object to use to fit the data. X : array-like The data to fit. Can be for example a list, or an array. y : array-like, optional, default: None The target variable to try to predict in the case of supervised learning. groups : array-like, with shape (n_samples,), optional Group labels for the samples used while splitting the dataset into train/test set. scoring : string, callable, list/tuple, dict or None, default: None A single string (see The scoring parameter: defining model evaluation rules ) or a callable (see Defining your scoring strategy from metric functions ) to evaluate the predictions on the test set. For evaluating multiple metrics, either give a list of (unique) strings or a dict with names as keys and callables as values. NOTE that when using custom scorers, each scorer should return a single value. Metric functions returning a list/array of values can be wrapped into multiple scorers that return one value each. See Specifying multiple metrics for evaluation for an example. If None, the estimators default scorer (if available) is used. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold , An object to be used as a cross-validation generator. An iterable yielding train, test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. n_jobs : integer, optional The number of CPUs to use to do the computation. -1 means all CPUs. verbose : integer, optional The verbosity level. fit_params : dict, optional Parameters to pass to the fit method of the estimator. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs return_train_score : boolean, optional Whether to include train scores. Current default is 'warn' , which behaves as True in addition to raising a warning when a training score is looked up. That default will be changed to False in 0.21. Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off. However computing the scores on the training set can be computationally expensive and is not strictly required to select the parameters that yield the best generalization performance.", "funcReturnBody": "scores : dict of float arrays of shape=(n_splits,) Array of scores of the estimator for each run of the cross validation. A dict of arrays containing the score/time arrays for each scorer is returned. The possible keys for this dict are: test_score The score array for test scores on each cv split. train_score The score array for train scores on each cv split. This is available only if return_train_score parameter is True . fit_time The time for fitting the estimator on the train set for each cv split. score_time The time for scoring the estimator on the test set for each cv split. (Note time for scoring on the train set is not included even if return_train_score is set to True"},
{"allReturnParams": ["score", "parameters", "n_samples_test"], "libName": "sklearn.model_selection", "methods": [], "notes": "", "funcName": "fit_grid_point", "allFuncParams": ["X", "y", "estimator", "parameters", "train", "test", "scorer", "verbose", "**fit_params", "error_score"], "funcDesc": "Run fit on one set of parameters.", "funcParamBody": "X : array-like, sparse matrix or list Input data. y : array-like or None Targets for input data. estimator : estimator object A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a score function, or scoring must be passed. parameters : dict Parameters to be set on estimator for this grid point. train : ndarray, dtype int or bool Boolean mask or indices for training set. test : ndarray, dtype int or bool Boolean mask or indices for test set. scorer : callable or None The scorer callable object / function must have its signature as scorer(estimator,  X,  y) . If None the estimators default scorer is used. verbose : int Verbosity level. **fit_params : kwargs Additional parameter passed to the fit function of the estimator. error_score : raise (default) or numeric Value to assign to the score if an error occurs in estimator fitting. If set to raise, the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error.", "funcReturnBody": "score : float Score of this parameter setting on given training / test split. parameters : dict The parameters that have been evaluated. n_samples_test : int Number of test samples in this split."},
{"libName": "sklearn.model_selection", "methods": [{"methodName": "__init__(estimator, param_distributions, n_iter=10, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=2*n_jobs, random_state=None, error_score=raise, return_train_score=warn)", "methodDesc": ""}, {"methodParams": ["X"], "methodName": "decision_function(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call decision_function on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports decision_function ."}, {"methodParams": ["X", "y", "groups", "**fit_params"], "methodName": "fit(X, y=None, groups=None, **fit_params)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vector, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples] or [n_samples, n_output], optional   Target relative to X for classification or regression; None for unsupervised learning.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set.   **fit_params : dict of string -> object   Parameters passed to the fit method of the estimator  ", "methodDesc": "Run fit with all sets of parameters."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodParams": ["Xt"], "methodName": "inverse_transform(Xt)", "methodParamsBody": "Xt : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call inverse_transform on the estimator with the best found params. Only available if the underlying estimator implements inverse_transform and refit=True ."}, {"methodParams": ["X"], "methodName": "predict(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call predict on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict ."}, {"methodParams": ["X"], "methodName": "predict_log_proba(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call predict_log_proba on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict_log_proba ."}, {"methodParams": ["X"], "methodName": "predict_proba(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call predict_proba on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict_proba ."}, {"methodName": "score(X, y=None)", "methodReturnsBody": "score : float ", "methodParams": ["X", "y"], "methodReturns": ["score"], "methodDesc": "Returns the score on the given data, if the estimator has been refit. This uses the score defined by scoring where provided, and the best_estimator_.score method otherwise.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Input data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples] or [n_samples, n_output], optional   Target relative to X for classification or regression; None for unsupervised learning.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodParams": ["X"], "methodName": "transform(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call transform on the estimator with the best found parameters. Only available if the underlying estimator supports transform and refit=True ."}], "allFuncParams": ["estimator", "param_distributions", "n_iter", "scoring", "fit_params", "n_jobs", "pre_dispatch", "iid", "cv", "refit", "verbose", "random_state", "error_score", "return_train_score"], "notes": "The parameters selected are those that maximize the score of the held-out data, according to the scoring parameter. If n_jobs was set to a value higher than one, the data is copied for each parameter setting(and not n_jobs times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available.  A workaround in this case is to set pre_dispatch . Then, the memory is copied only pre_dispatch many times. A reasonable value for pre_dispatch is 2 * n_jobs . The parameters selected are those that maximize the score of the held-out data, according to the scoring parameter. If n_jobs was set to a value higher than one, the data is copied for each parameter setting(and not n_jobs times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available.  A workaround in this case is to set pre_dispatch . Then, the memory is copied only pre_dispatch many times. A reasonable value for pre_dispatch is 2 * n_jobs .", "funcName": "RandomizedSearchCV", "allFuncAttributes": ["cv_results_", "best_estimator_", "best_score_", "best_params_", "best_index_", "scorer_", "n_splits_"], "funcDesc": "Randomized search on hyper parameters.", "funcParamBody": "estimator : estimator object. A object of that type is instantiated for each grid point. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a score function, or scoring must be passed. param_distributions : dict Dictionary with parameters names (string) as keys and distributions or lists of parameters to try. Distributions must provide a rvs n_iter : int, default=10 Number of parameter settings that are sampled. n_iter trades off runtime vs quality of the solution. scoring : string, callable, list/tuple, dict or None, default: None A single string (see The scoring parameter: defining model evaluation rules ) or a callable (see Defining your scoring strategy from metric functions ) to evaluate the predictions on the test set. For evaluating multiple metrics, either give a list of (unique) strings or a dict with names as keys and callables as values. NOTE that when using custom scorers, each scorer should return a single value. Metric functions returning a list/array of values can be wrapped into multiple scorers that return one value each. See Specifying multiple metrics for evaluation for an example. If None, the estimators default scorer (if available) is used. fit_params : dict, optional Parameters to pass to the fit method. Deprecated since version 0.19: fit_params as a constructor argument was deprecated in version 0.19 and will be removed in version 0.21. Pass fit parameters to the fit method instead. n_jobs : int, default=1 Number of jobs to run in parallel. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs iid : boolean, default=True If True, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold , An object to be used as a cross-validation generator. An iterable yielding train, test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. refit : boolean, or string default=True Refit an estimator using the best found parameters on the whole dataset. For multiple metric evaluation, this needs to be a string denoting the scorer that would be used to find the best parameters for refitting the estimator at the end. The refitted estimator is made available at the best_estimator_ predict directly on this RandomizedSearchCV instance. Also for multiple metric evaluation, the attributes best_index_ , best_score_ and best_parameters_ will only be available if refit is set and all of them will be determined w.r.t this specific scorer. See scoring parameter to know more about multiple metric evaluation. verbose : integer Controls the verbosity: the higher, the more messages. random_state : int, RandomState instance or None, optional, default=None Pseudo random number generator state used for random uniform sampling from lists of possible values instead of scipy.stats distributions. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . error_score : raise (default) or numeric Value to assign to the score if an error occurs in estimator fitting. If set to raise, the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. return_train_score : boolean, optional If False , the cv_results_ attribute will not include training scores. Current default is 'warn' , which behaves as True in addition to raising a warning when a training score is looked up. That default will be changed to False in 0.21. Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off. However computing the scores on the training set can be computationally expensive and is not strictly required to select the parameters that yield the best generalization performance.", "funcAttrBody": "cv_results_ : dict of numpy (masked) ndarrays A dict with keys as column headers and values as columns, that can be imported into a pandas DataFrame . For instance the below given table param_kernel param_gamma split0_test_score ... rank_test_score rbf 0.1 0.8 ... 2 rbf 0.2 0.9 ... 1 rbf 0.3 0.7 ... 1 will be represented by a cv_results_ dict of: { 'param_kernel'  :  masked_array ( data  =  [ 'rbf' ,  'rbf' ,  'rbf' ], mask  =  False ), 'param_gamma'  :  masked_array ( data  =  [ 0.1  0.2  0.3 ],  mask  =  False ), 'split0_test_score'  :  [ 0.8 ,  0.9 ,  0.7 ], 'split1_test_score'  :  [ 0.82 ,  0.5 ,  0.7 ], 'mean_test_score'  :  [ 0.81 ,  0.7 ,  0.7 ], 'std_test_score'  :  [ 0.02 ,  0.2 ,  0. ], 'rank_test_score'  :  [ 3 ,  1 ,  1 ], 'split0_train_score'  :  [ 0.8 ,  0.9 ,  0.7 ], 'split1_train_score'  :  [ 0.82 ,  0.5 ,  0.7 ], 'mean_train_score'  :  [ 0.81 ,  0.7 ,  0.7 ], 'std_train_score'  :  [ 0.03 ,  0.03 ,  0.04 ], 'mean_fit_time'  :  [ 0.73 ,  0.63 ,  0.43 ,  0.49 ], 'std_fit_time'  :  [ 0.01 ,  0.02 ,  0.01 ,  0.01 ], 'mean_score_time'  :  [ 0.007 ,  0.06 ,  0.04 ,  0.04 ], 'std_score_time'  :  [ 0.001 ,  0.002 ,  0.003 ,  0.005 ], 'params'  :  [{ 'kernel'  :  'rbf' ,  'gamma'  :  0.1 },  ... ], } NOTE The key 'params' is used to store a list of parameter settings dicts for all the parameter candidates. The mean_fit_time , std_fit_time , mean_score_time and std_score_time are all in seconds. For multi-metric evaluation, the scores for all the scorers are available in the cv_results_ dict at the keys ending with that scorers name ( '_<scorer_name>' ) instead of '_score' shown above. (split0_test_precision, mean_train_precision etc.) best_estimator_ : estimator or dict Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if refit=False . For multi-metric evaluation, this attribute is present only if refit is specified. See refit parameter for more information on allowed values. best_score_ : float Mean cross-validated score of the best_estimator. For multi-metric evaluation, this is not available if refit is False . See refit parameter for more information. best_params_ : dict Parameter setting that gave the best results on the hold out data. For multi-metric evaluation, this is not available if refit is False . See refit parameter for more information. best_index_ : int The index (of the cv_results_ arrays) which corresponds to the best candidate parameter setting. The dict at search.cv_results_['params'][search.best_index_] gives the parameter setting for the best model, that gives the highest mean score ( search.best_score_ ). For multi-metric evaluation, this is not available if refit is False . See refit parameter for more information. scorer_ : function or a dict Scorer function used on the held out data to choose the best parameters for the model. For multi-metric evaluation, this attribute holds the validated scoring dict which maps the scorer key to the scorer callable. n_splits_ : int The number of cross-validation splits (folds/iterations)."},
{"allReturnParams": ["params", "Yields"], "libName": "sklearn.model_selection", "methods": [], "notes": "", "funcName": "ParameterSampler", "allFuncParams": ["param_distributions", "n_iter", "random_state"], "funcDesc": "Generator on parameters sampled from given distributions.", "funcParamBody": "param_distributions : dict Dictionary where the keys are parameters and values are distributions from which a parameter is to be sampled. Distributions either have to provide a rvs function to sample from them, or can be given as a list of values, where a uniform distribution is assumed. n_iter : integer Number of parameter settings that are produced. random_state : int, RandomState instance or None, optional (default=None) Pseudo random number generator state used for random uniform sampling from lists of possible values instead of scipy.stats distributions. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "params : dict of string to any Yields dictionaries mapping each estimator parameter to as sampled value."},
{"libName": "sklearn.model_selection", "methods": [], "notes": "", "funcName": "ParameterGrid", "allFuncParams": ["param_grid"], "funcDesc": "Grid of parameters with a discrete number of values for each.", "funcParamBody": "param_grid : dict of string to sequence, or sequence of such The parameter grid to explore, as a dictionary mapping estimator parameters to sequences of allowed values. An empty dict signifies default parameters. A sequence of dicts signifies a sequence of grids to search, and is useful to avoid exploring parameter combinations that make no sense or have no effect. See the examples below."},
{"libName": "sklearn.model_selection", "methods": [{"methodName": "__init__(estimator, param_grid, scoring=None, fit_params=None, n_jobs=1, iid=True, refit=True, cv=None, verbose=0, pre_dispatch=2*n_jobs, error_score=raise, return_train_score=warn)", "methodDesc": ""}, {"methodParams": ["X"], "methodName": "decision_function(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call decision_function on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports decision_function ."}, {"methodParams": ["X", "y", "groups", "**fit_params"], "methodName": "fit(X, y=None, groups=None, **fit_params)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vector, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples] or [n_samples, n_output], optional   Target relative to X for classification or regression; None for unsupervised learning.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set.   **fit_params : dict of string -> object   Parameters passed to the fit method of the estimator  ", "methodDesc": "Run fit with all sets of parameters."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodParams": ["Xt"], "methodName": "inverse_transform(Xt)", "methodParamsBody": "Xt : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call inverse_transform on the estimator with the best found params. Only available if the underlying estimator implements inverse_transform and refit=True ."}, {"methodParams": ["X"], "methodName": "predict(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call predict on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict ."}, {"methodParams": ["X"], "methodName": "predict_log_proba(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call predict_log_proba on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict_log_proba ."}, {"methodParams": ["X"], "methodName": "predict_proba(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call predict_proba on the estimator with the best found parameters. Only available if refit=True and the underlying estimator supports predict_proba ."}, {"methodName": "score(X, y=None)", "methodReturnsBody": "score : float ", "methodParams": ["X", "y"], "methodReturns": ["score"], "methodDesc": "Returns the score on the given data, if the estimator has been refit. This uses the score defined by scoring where provided, and the best_estimator_.score method otherwise.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Input data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples] or [n_samples, n_output], optional   Target relative to X for classification or regression; None for unsupervised learning.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodParams": ["X"], "methodName": "transform(X)", "methodParamsBody": "X : indexable, length n_samples   Must fulfill the input assumptions of the underlying estimator.  ", "methodDesc": "Call transform on the estimator with the best found parameters. Only available if the underlying estimator supports transform and refit=True ."}], "allFuncParams": ["estimator", "param_grid", "scoring", "fit_params", "n_jobs", "pre_dispatch", "iid", "cv", "refit", "verbose", "error_score", "return_train_score"], "notes": "The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed in which case it is used instead. If n_jobs was set to a value higher than one, the data is copied for each point in the grid (and not n_jobs times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available.  A workaround in this case is to set pre_dispatch . Then, the memory is copied only pre_dispatch many times. A reasonable value for pre_dispatch is 2 * n_jobs . The parameters selected are those that maximize the score of the left out data, unless an explicit score is passed in which case it is used instead. If n_jobs was set to a value higher than one, the data is copied for each point in the grid (and not n_jobs times). This is done for efficiency reasons if individual jobs take very little time, but may raise errors if the dataset is large and not enough memory is available.  A workaround in this case is to set pre_dispatch . Then, the memory is copied only pre_dispatch many times. A reasonable value for pre_dispatch is 2 * n_jobs .", "funcName": "GridSearchCV", "allFuncAttributes": ["cv_results_", "best_estimator_", "best_score_", "best_params_", "best_index_", "scorer_", "n_splits_"], "funcDesc": "Exhaustive search over specified parameter values for an estimator.", "funcParamBody": "estimator : estimator object. This is assumed to implement the scikit-learn estimator interface. Either estimator needs to provide a score function, or scoring must be passed. param_grid : dict or list of dictionaries Dictionary with parameters names (string) as keys and lists of parameter settings to try as values, or a list of such dictionaries, in which case the grids spanned by each dictionary in the list are explored. This enables searching over any sequence of parameter settings. scoring : string, callable, list/tuple, dict or None, default: None A single string (see The scoring parameter: defining model evaluation rules ) or a callable (see Defining your scoring strategy from metric functions ) to evaluate the predictions on the test set. For evaluating multiple metrics, either give a list of (unique) strings or a dict with names as keys and callables as values. NOTE that when using custom scorers, each scorer should return a single value. Metric functions returning a list/array of values can be wrapped into multiple scorers that return one value each. See Specifying multiple metrics for evaluation for an example. If None, the estimators default scorer (if available) is used. fit_params : dict, optional Parameters to pass to the fit method. Deprecated since version 0.19: fit_params as a constructor argument was deprecated in version 0.19 and will be removed in version 0.21. Pass fit parameters to the fit method instead. n_jobs : int, default=1 Number of jobs to run in parallel. pre_dispatch : int, or string, optional Controls the number of jobs that get dispatched during parallel execution. Reducing this number can be useful to avoid an explosion of memory consumption when more jobs get dispatched than CPUs can process. This parameter can be: None, in which case all the jobs are immediately created and spawned. Use this for lightweight and fast-running jobs, to avoid delays due to on-demand spawning of the jobs An int, giving the exact number of total jobs that are spawned A string, giving an expression as a function of n_jobs, as in 2*n_jobs iid : boolean, default=True If True, the data is assumed to be identically distributed across the folds, and the loss minimized is the total loss per sample, and not the mean loss across the folds. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross validation, integer, to specify the number of folds in a (Stratified)KFold , An object to be used as a cross-validation generator. An iterable yielding train, test splits. For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. refit : boolean, or string, default=True Refit an estimator using the best found parameters on the whole dataset. For multiple metric evaluation, this needs to be a string denoting the scorer is used to find the best parameters for refitting the estimator at the end. The refitted estimator is made available at the best_estimator_ predict directly on this GridSearchCV instance. Also for multiple metric evaluation, the attributes best_index_ , best_score_ and best_parameters_ will only be available if refit is set and all of them will be determined w.r.t this specific scorer. See scoring parameter to know more about multiple metric evaluation. verbose : integer Controls the verbosity: the higher, the more messages. error_score : raise (default) or numeric Value to assign to the score if an error occurs in estimator fitting. If set to raise, the error is raised. If a numeric value is given, FitFailedWarning is raised. This parameter does not affect the refit step, which will always raise the error. return_train_score : boolean, optional If False , the cv_results_ attribute will not include training scores. Current default is 'warn' , which behaves as True in addition to raising a warning when a training score is looked up. That default will be changed to False in 0.21. Computing training scores is used to get insights on how different parameter settings impact the overfitting/underfitting trade-off. However computing the scores on the training set can be computationally expensive and is not strictly required to select the parameters that yield the best generalization performance.", "funcAttrBody": "cv_results_ : dict of numpy (masked) ndarrays A dict with keys as column headers and values as columns, that can be imported into a pandas DataFrame . For instance the below given table param_kernel param_gamma param_degree split0_test_score ... rank_t... poly  2 0.8 ... 2 poly  3 0.7 ... 4 rbf 0.1  0.8 ... 3 rbf 0.2  0.9 ... 1 will be represented by a cv_results_ dict of: { 'param_kernel' :  masked_array ( data  =  [ 'poly' ,  'poly' ,  'rbf' ,  'rbf' ], mask  =  [ False  False  False  False ] ... ) 'param_gamma' :  masked_array ( data  =  [ --  --  0.1  0.2 ], mask  =  [  True  True  False  False ] ... ), 'param_degree' :  masked_array ( data  =  [ 2.0  3.0  --  -- ], mask  =  [ False  False  True  True ] ... ), 'split0_test_score'  :  [ 0.8 ,  0.7 ,  0.8 ,  0.9 ], 'split1_test_score'  :  [ 0.82 ,  0.5 ,  0.7 ,  0.78 ], 'mean_test_score'  :  [ 0.81 ,  0.60 ,  0.75 ,  0.82 ], 'std_test_score'  :  [ 0.02 ,  0.01 ,  0.03 ,  0.03 ], 'rank_test_score'  :  [ 2 ,  4 ,  3 ,  1 ], 'split0_train_score'  :  [ 0.8 ,  0.9 ,  0.7 ], 'split1_train_score'  :  [ 0.82 ,  0.5 ,  0.7 ], 'mean_train_score'  :  [ 0.81 ,  0.7 ,  0.7 ], 'std_train_score'  :  [ 0.03 ,  0.03 ,  0.04 ], 'mean_fit_time'  :  [ 0.73 ,  0.63 ,  0.43 ,  0.49 ], 'std_fit_time'  :  [ 0.01 ,  0.02 ,  0.01 ,  0.01 ], 'mean_score_time'  :  [ 0.007 ,  0.06 ,  0.04 ,  0.04 ], 'std_score_time'  :  [ 0.001 ,  0.002 ,  0.003 ,  0.005 ], 'params'  :  [{ 'kernel' :  'poly' ,  'degree' :  2 },  ... ], } NOTE The key 'params' is used to store a list of parameter settings dicts for all the parameter candidates. The mean_fit_time , std_fit_time , mean_score_time and std_score_time are all in seconds. For multi-metric evaluation, the scores for all the scorers are available in the cv_results_ dict at the keys ending with that scorers name ( '_<scorer_name>' ) instead of '_score' shown above. (split0_test_precision, mean_train_precision etc.) best_estimator_ : estimator or dict Estimator that was chosen by the search, i.e. estimator which gave highest score (or smallest loss if specified) on the left out data. Not available if refit=False . See refit parameter for more information on allowed values. best_score_ : float Mean cross-validated score of the best_estimator For multi-metric evaluation, this is present only if refit is specified. best_params_ : dict Parameter setting that gave the best results on the hold out data. For multi-metric evaluation, this is present only if refit is specified. best_index_ : int The index (of the cv_results_ arrays) which corresponds to the best candidate parameter setting. The dict at search.cv_results_['params'][search.best_index_] gives the parameter setting for the best model, that gives the highest mean score ( search.best_score_ ). For multi-metric evaluation, this is present only if refit is specified. scorer_ : function or a dict Scorer function used on the held out data to choose the best parameters for the model. For multi-metric evaluation, this attribute holds the validated scoring dict which maps the scorer key to the scorer callable. n_splits_ : int The number of cross-validation splits (folds/iterations)."},
{"allReturnParams": ["splitting"], "libName": "sklearn.model_selection", "methods": [], "notes": "", "funcName": "train_test_split", "allFuncParams": ["*arrays", "test_size", "train_size", "random_state", "shuffle", "stratify"], "funcDesc": "Split arrays or matrices into random train and test subsets", "funcParamBody": "*arrays : sequence of indexables with same length / shape[0] Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes. test_size : float, int, None, optional If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.25. The default will change in version 0.21. It will remain 0.25 only if train_size is unspecified, otherwise it will complement the specified train_size . train_size : float, int, or None, default None If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . shuffle : boolean, optional (default=True) Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None. stratify : array-like or None (default is None) If not None, data is split in a stratified fashion, using this as the class labels.", "funcReturnBody": "splitting : list, length=2 * len(arrays) List containing train-test split of inputs. New in version 0.16: If the input is sparse, the output will be a scipy.sparse.csr_matrix . Else, output type is the same as the input type."},
{"allReturnParams": ["checked_cv"], "libName": "sklearn.model_selection", "methods": [], "notes": "", "funcName": "check_cv", "allFuncParams": ["cv", "y", "classifier"], "funcDesc": "Input checker utility for building a cross-validator", "funcParamBody": "cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, if classifier is True and y is either binary or multiclass, StratifiedKFold is used. In all other cases, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. y : array-like, optional The target variable for supervised learning problems. classifier : boolean, optional, default False Whether the task is a classification task, in which case stratified KFold will be used.", "funcReturnBody": "checked_cv : a cross-validator instance. The return value is a cross-validator which generates the train/test splits via the split method."},
{"libName": "sklearn.model_selection", "methods": [{"methodName": "__init__(n_splits=3, max_train_size=None)", "methodDesc": ""}, {"methodName": "get_n_splits(X=None, y=None, groups=None)", "methodReturnsBody": "n_splits : int   Returns the number of splitting iterations in the cross-validator.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["n_splits"], "methodDesc": "Returns the number of splitting iterations in the cross-validator", "methodParamsBody": "X : object   Always ignored, exists for compatibility.   y : object   Always ignored, exists for compatibility.   groups : object   Always ignored, exists for compatibility.  "}, {"methodName": "split(X, y=None, groups=None)", "methodReturnsBody": "train : ndarray   The training set indices for that split.   test : ndarray   The testing set indices for that split.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["train", "test"], "methodDesc": "Generate indices to split data into training and test set. Notes Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape (n_samples,)   Always ignored, exists for compatibility.   groups : array-like, with shape (n_samples,), optional   Always ignored, exists for compatibility.  "}], "notes": "The training set has size i  *  n_samples  //  (n_splits  +  1)  +  n_samples  %  (n_splits  +  1) in the i``th  split,  with  a  test  set  of  size  ``n_samples//(n_splits  +  1) , where n_samples is the number of samples. Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "funcName": "TimeSeriesSplit", "allFuncParams": ["n_splits", "max_train_size"], "funcDesc": "Time Series cross-validator", "funcParamBody": "n_splits : int, default=3 Number of splits. Must be at least 1. max_train_size : int, optional Maximum size for a single training set."},
{"libName": "sklearn.model_selection", "methods": [{"methodName": "__init__(n_splits=10, test_size=default, train_size=None, random_state=None)", "methodDesc": ""}, {"methodName": "get_n_splits(X=None, y=None, groups=None)", "methodReturnsBody": "n_splits : int   Returns the number of splitting iterations in the cross-validator.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["n_splits"], "methodDesc": "Returns the number of splitting iterations in the cross-validator", "methodParamsBody": "X : object   Always ignored, exists for compatibility.   y : object   Always ignored, exists for compatibility.   groups : object   Always ignored, exists for compatibility.  "}, {"methodName": "split(X, y, groups=None)", "methodReturnsBody": "train : ndarray   The training set indices for that split.   test : ndarray   The testing set indices for that split.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["train", "test"], "methodDesc": "Generate indices to split data into training and test set. Notes Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.  Note that providing y is sufficient to generate the splits and hence np.zeros(n_samples) may be used as a placeholder for X instead of actual training data.   y : array-like, shape (n_samples,)   The target variable for supervised learning problems. Stratification is done based on the y labels.   groups : object   Always ignored, exists for compatibility.  "}], "notes": "Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer. Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "funcName": "StratifiedShuffleSplit", "allFuncParams": ["n_splits", "test_size", "train_size", "random_state"], "funcDesc": "Stratified ShuffleSplit cross-validator", "funcParamBody": "n_splits : int, default 10 Number of re-shuffling & splitting iterations. test_size : float, int, None, optional If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.1. The default will change in version 0.21. It will remain 0.1 only if train_size is unspecified, otherwise it will complement the specified train_size . train_size : float, int, or None, default is None If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random ."},
{"libName": "sklearn.model_selection", "methods": [{"methodName": "__init__(n_splits=3, shuffle=False, random_state=None)", "methodDesc": ""}, {"methodName": "get_n_splits(X=None, y=None, groups=None)", "methodReturnsBody": "n_splits : int   Returns the number of splitting iterations in the cross-validator.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["n_splits"], "methodDesc": "Returns the number of splitting iterations in the cross-validator", "methodParamsBody": "X : object   Always ignored, exists for compatibility.   y : object   Always ignored, exists for compatibility.   groups : object   Always ignored, exists for compatibility.  "}, {"methodName": "split(X, y, groups=None)", "methodReturnsBody": "train : ndarray   The training set indices for that split.   test : ndarray   The testing set indices for that split.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["train", "test"], "methodDesc": "Generate indices to split data into training and test set. Notes Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.  Note that providing y is sufficient to generate the splits and hence np.zeros(n_samples) may be used as a placeholder for X instead of actual training data.   y : array-like, shape (n_samples,)   The target variable for supervised learning problems. Stratification is done based on the y labels.   groups : object   Always ignored, exists for compatibility.  "}], "notes": "All the folds have size trunc(n_samples  /  n_splits) , the last one has the complementary. Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer. All the folds have size trunc(n_samples  /  n_splits) , the last one has the complementary. Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "funcName": "StratifiedKFold", "allFuncParams": ["n_splits", "shuffle", "random_state"], "funcDesc": "Stratified K-Folds cross-validator", "funcParamBody": "n_splits : int, default=3 Number of folds. Must be at least 2. shuffle : boolean, optional Whether to shuffle each stratification of the data before splitting into batches. random_state : int, RandomState instance or None, optional, default=None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when shuffle == True."},
{"libName": "sklearn.model_selection", "methods": [{"methodName": "__init__(n_splits=10, test_size=default, train_size=None, random_state=None)", "methodDesc": ""}, {"methodName": "get_n_splits(X=None, y=None, groups=None)", "methodReturnsBody": "n_splits : int   Returns the number of splitting iterations in the cross-validator.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["n_splits"], "methodDesc": "Returns the number of splitting iterations in the cross-validator", "methodParamsBody": "X : object   Always ignored, exists for compatibility.   y : object   Always ignored, exists for compatibility.   groups : object   Always ignored, exists for compatibility.  "}, {"methodName": "split(X, y=None, groups=None)", "methodReturnsBody": "train : ndarray   The training set indices for that split.   test : ndarray   The testing set indices for that split.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["train", "test"], "methodDesc": "Generate indices to split data into training and test set. Notes Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape (n_samples,)   The target variable for supervised learning problems.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set.  "}], "notes": "Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer. Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "funcName": "ShuffleSplit", "allFuncParams": ["n_splits", "test_size", "train_size", "random_state"], "funcDesc": "Random permutation cross-validator", "funcParamBody": "n_splits : int, default 10 Number of re-shuffling & splitting iterations. test_size : float, int, None, default=0.1 If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default (the is parameter unspecified), the value is set to 0.1. The default will change in version 0.21. It will remain 0.1 only if train_size is unspecified, otherwise it will complement the specified train_size . train_size : float, int, or None, default=None If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random ."},
{"libName": "sklearn.model_selection", "methods": [{"methodName": "__init__(n_splits=5, n_repeats=10, random_state=None)", "methodDesc": ""}, {"methodName": "get_n_splits(X=None, y=None, groups=None)", "methodReturnsBody": "n_splits : int   Returns the number of splitting iterations in the cross-validator.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["n_splits"], "methodDesc": "Returns the number of splitting iterations in the cross-validator", "methodParamsBody": "X : object   Always ignored, exists for compatibility. np.zeros(n_samples) may be used as a placeholder.   y : object   Always ignored, exists for compatibility. np.zeros(n_samples) may be used as a placeholder.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set.  "}, {"methodName": "split(X, y=None, groups=None)", "methodReturnsBody": "train : ndarray   The training set indices for that split.   test : ndarray   The testing set indices for that split.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["train", "test"], "methodDesc": "Generates indices to split data into training and test set.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, of length n_samples   The target variable for supervised learning problems.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set.  "}], "notes": "", "funcName": "RepeatedStratifiedKFold", "allFuncParams": ["n_splits", "n_repeats", "random_state"], "funcDesc": "Repeated Stratified K-Fold cross validator.", "funcParamBody": "n_splits : int, default=5 Number of folds. Must be at least 2. n_repeats : int, default=10 Number of times cross-validator needs to be repeated. random_state : None, int or RandomState, default=None Random state to be used to generate random state for each repetition."},
{"libName": "sklearn.model_selection", "methods": [{"methodName": "__init__(n_splits=5, n_repeats=10, random_state=None)", "methodDesc": ""}, {"methodName": "get_n_splits(X=None, y=None, groups=None)", "methodReturnsBody": "n_splits : int   Returns the number of splitting iterations in the cross-validator.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["n_splits"], "methodDesc": "Returns the number of splitting iterations in the cross-validator", "methodParamsBody": "X : object   Always ignored, exists for compatibility. np.zeros(n_samples) may be used as a placeholder.   y : object   Always ignored, exists for compatibility. np.zeros(n_samples) may be used as a placeholder.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set.  "}, {"methodName": "split(X, y=None, groups=None)", "methodReturnsBody": "train : ndarray   The training set indices for that split.   test : ndarray   The testing set indices for that split.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["train", "test"], "methodDesc": "Generates indices to split data into training and test set.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, of length n_samples   The target variable for supervised learning problems.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set.  "}], "notes": "", "funcName": "RepeatedKFold", "allFuncParams": ["n_splits", "n_repeats", "random_state"], "funcDesc": "Repeated K-Fold cross validator.", "funcParamBody": "n_splits : int, default=5 Number of folds. Must be at least 2. n_repeats : int, default=10 Number of times cross-validator needs to be repeated. random_state : int, RandomState instance or None, optional, default=None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random ."},
{"libName": "sklearn.model_selection", "methods": [{"methodName": "__init__(test_fold)", "methodDesc": ""}, {"methodName": "get_n_splits(X=None, y=None, groups=None)", "methodReturnsBody": "n_splits : int   Returns the number of splitting iterations in the cross-validator.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["n_splits"], "methodDesc": "Returns the number of splitting iterations in the cross-validator", "methodParamsBody": "X : object   Always ignored, exists for compatibility.   y : object   Always ignored, exists for compatibility.   groups : object   Always ignored, exists for compatibility.  "}, {"methodName": "split(X=None, y=None, groups=None)", "methodReturnsBody": "train : ndarray   The training set indices for that split.   test : ndarray   The testing set indices for that split.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["train", "test"], "methodDesc": "Generate indices to split data into training and test set.", "methodParamsBody": "X : object   Always ignored, exists for compatibility.   y : object   Always ignored, exists for compatibility.   groups : object   Always ignored, exists for compatibility.  "}], "notes": "", "funcName": "PredefinedSplit", "allFuncParams": ["test_fold"], "funcDesc": "Predefined split cross-validator", "funcParamBody": "test_fold : array-like, shape (n_samples,) The entry test_fold[i] represents the index of the test set that sample i belongs to. It is possible to exclude sample i from any test set (i.e. include sample i in every training set) by setting test_fold[i] equal to -1."},
{"libName": "sklearn.model_selection", "methods": [{"methodName": "__init__(p)", "methodDesc": ""}, {"methodParams": ["X", "y", "groups"], "methodName": "get_n_splits(X, y=None, groups=None)", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : object   Always ignored, exists for compatibility.   groups : object   Always ignored, exists for compatibility.  ", "methodDesc": "Returns the number of splitting iterations in the cross-validator"}, {"methodName": "split(X, y=None, groups=None)", "methodReturnsBody": "train : ndarray   The training set indices for that split.   test : ndarray   The testing set indices for that split.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["train", "test"], "methodDesc": "Generate indices to split data into training and test set. Notes Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, of length n_samples   The target variable for supervised learning problems.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set.  "}], "notes": "Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "funcName": "LeavePOut", "allFuncParams": ["p"], "funcDesc": "Leave-P-Out cross-validator", "funcParamBody": "p : int Size of the test sets."},
{"funcName": "LeaveOneOut", "notes": "Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "libName": "sklearn.model_selection", "methods": [{"methodName": "__init__()", "methodDesc": ""}, {"methodName": "get_n_splits(X, y=None, groups=None)", "methodReturnsBody": "n_splits : int   Returns the number of splitting iterations in the cross-validator.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["n_splits"], "methodDesc": "Returns the number of splitting iterations in the cross-validator", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : object   Always ignored, exists for compatibility.   groups : object   Always ignored, exists for compatibility.  "}, {"methodName": "split(X, y=None, groups=None)", "methodReturnsBody": "train : ndarray   The training set indices for that split.   test : ndarray   The testing set indices for that split.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["train", "test"], "methodDesc": "Generate indices to split data into training and test set. Notes Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, of length n_samples   The target variable for supervised learning problems.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set.  "}], "funcDesc": "Leave-One-Out cross-validator"},
{"libName": "sklearn.model_selection", "methods": [{"methodName": "__init__(n_groups)", "methodDesc": ""}, {"methodName": "get_n_splits(X=None, y=None, groups=None)", "methodReturnsBody": "n_splits : int   Returns the number of splitting iterations in the cross-validator.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["n_splits"], "methodDesc": "Returns the number of splitting iterations in the cross-validator", "methodParamsBody": "X : object, optional   Always ignored, exists for compatibility.   y : object, optional   Always ignored, exists for compatibility.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set. This groups parameter must always be specified to calculate the number of splits, though the other parameters can be omitted.  "}, {"methodName": "split(X, y=None, groups=None)", "methodReturnsBody": "train : ndarray   The training set indices for that split.   test : ndarray   The testing set indices for that split.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["train", "test"], "methodDesc": "Generate indices to split data into training and test set. Notes Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, of length n_samples   The target variable for supervised learning problems.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set.  "}], "notes": "Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "funcName": "LeavePGroupsOut", "allFuncParams": ["n_groups"], "funcDesc": "Leave P Group(s) Out cross-validator", "funcParamBody": "n_groups : int Number of groups ( p ) to leave out in the test split."},
{"funcName": "LeaveOneGroupOut", "notes": "Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "libName": "sklearn.model_selection", "methods": [{"methodName": "__init__()", "methodDesc": ""}, {"methodName": "get_n_splits(X=None, y=None, groups=None)", "methodReturnsBody": "n_splits : int   Returns the number of splitting iterations in the cross-validator.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["n_splits"], "methodDesc": "Returns the number of splitting iterations in the cross-validator", "methodParamsBody": "X : object, optional   Always ignored, exists for compatibility.   y : object, optional   Always ignored, exists for compatibility.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set. This groups parameter must always be specified to calculate the number of splits, though the other parameters can be omitted.  "}, {"methodName": "split(X, y=None, groups=None)", "methodReturnsBody": "train : ndarray   The training set indices for that split.   test : ndarray   The testing set indices for that split.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["train", "test"], "methodDesc": "Generate indices to split data into training and test set. Notes Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, of length n_samples   The target variable for supervised learning problems.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set.  "}], "funcDesc": "Leave One Group Out cross-validator"},
{"libName": "sklearn.model_selection", "methods": [{"methodName": "__init__(n_splits=3, shuffle=False, random_state=None)", "methodDesc": ""}, {"methodName": "get_n_splits(X=None, y=None, groups=None)", "methodReturnsBody": "n_splits : int   Returns the number of splitting iterations in the cross-validator.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["n_splits"], "methodDesc": "Returns the number of splitting iterations in the cross-validator", "methodParamsBody": "X : object   Always ignored, exists for compatibility.   y : object   Always ignored, exists for compatibility.   groups : object   Always ignored, exists for compatibility.  "}, {"methodName": "split(X, y=None, groups=None)", "methodReturnsBody": "train : ndarray   The training set indices for that split.   test : ndarray   The testing set indices for that split.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["train", "test"], "methodDesc": "Generate indices to split data into training and test set. Notes Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape (n_samples,)   The target variable for supervised learning problems.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set.  "}], "notes": "The first n_samples  %  n_splits folds have size n_samples  //  n_splits  +  1 , other folds have size n_samples  //  n_splits , where n_samples is the number of samples. Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer. The first n_samples  %  n_splits folds have size n_samples  //  n_splits  +  1 , other folds have size n_samples  //  n_splits , where n_samples is the number of samples. Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "funcName": "KFold", "allFuncParams": ["n_splits", "shuffle", "random_state"], "funcDesc": "K-Folds cross-validator", "funcParamBody": "n_splits : int, default=3 Number of folds. Must be at least 2. shuffle : boolean, optional Whether to shuffle the data before splitting into batches. random_state : int, RandomState instance or None, optional, default=None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when shuffle == True."},
{"libName": "sklearn.model_selection", "methods": [{"methodName": "__init__(n_splits=5, test_size=default, train_size=None, random_state=None)", "methodDesc": ""}, {"methodName": "get_n_splits(X=None, y=None, groups=None)", "methodReturnsBody": "n_splits : int   Returns the number of splitting iterations in the cross-validator.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["n_splits"], "methodDesc": "Returns the number of splitting iterations in the cross-validator", "methodParamsBody": "X : object   Always ignored, exists for compatibility.   y : object   Always ignored, exists for compatibility.   groups : object   Always ignored, exists for compatibility.  "}, {"methodName": "split(X, y=None, groups=None)", "methodReturnsBody": "train : ndarray   The training set indices for that split.   test : ndarray   The testing set indices for that split.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["train", "test"], "methodDesc": "Generate indices to split data into training and test set. Notes Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape (n_samples,)   The target variable for supervised learning problems.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set.  "}], "notes": "Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "funcName": "GroupShuffleSplit", "allFuncParams": ["n_splits", "test_size", "train_size", "random_state"], "funcDesc": "Shuffle-Group(s)-Out cross-validation iterator", "funcParamBody": "n_splits : int (default 5) Number of re-shuffling & splitting iterations. test_size : float, int, None, optional If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size. By default, the value is set to 0.2. The default will change in version 0.21. It will remain 0.2 only if train_size is unspecified, otherwise it will complement the specified train_size . train_size : float, int, or None, default is None If float, should be between 0.0 and 1.0 and represent the proportion of the groups to include in the train split. If int, represents the absolute number of train groups. If None, the value is automatically set to the complement of the test size. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random ."},
{"libName": "sklearn.model_selection", "methods": [{"methodName": "__init__(n_splits=3)", "methodDesc": ""}, {"methodName": "get_n_splits(X=None, y=None, groups=None)", "methodReturnsBody": "n_splits : int   Returns the number of splitting iterations in the cross-validator.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["n_splits"], "methodDesc": "Returns the number of splitting iterations in the cross-validator", "methodParamsBody": "X : object   Always ignored, exists for compatibility.   y : object   Always ignored, exists for compatibility.   groups : object   Always ignored, exists for compatibility.  "}, {"methodName": "split(X, y=None, groups=None)", "methodReturnsBody": "train : ndarray   The training set indices for that split.   test : ndarray   The testing set indices for that split.  ", "methodParams": ["X", "y", "groups"], "methodReturns": ["train", "test"], "methodDesc": "Generate indices to split data into training and test set. Notes Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape (n_samples,)   The target variable for supervised learning problems.   groups : array-like, with shape (n_samples,), optional   Group labels for the samples used while splitting the dataset into train/test set.  "}], "notes": "Randomized CV splitters may return different results for each call of split. You can make the results identical by setting random_state to an integer.", "funcName": "GroupKFold", "allFuncParams": ["n_splits"], "funcDesc": "K-fold iterator variant with non-overlapping groups.", "funcParamBody": "n_splits : int, default=3 Number of folds. Must be at least 2."},
{"libName": "sklearn.mixture", "methods": [{"methodName": "__init__(n_components=1, covariance_type=full, tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params=kmeans, weights_init=None, means_init=None, precisions_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10)", "methodDesc": ""}, {"methodName": "aic(X)", "methodReturnsBody": "aic : float   The lower the better.  ", "methodParams": ["X"], "methodReturns": ["aic"], "methodDesc": "Akaike information criterion for the current model on the input X.", "methodParamsBody": "X : array of shape (n_samples, n_dimensions) "}, {"methodName": "bic(X)", "methodReturnsBody": "bic : float   The lower the better.  ", "methodParams": ["X"], "methodReturns": ["bic"], "methodDesc": "Bayesian information criterion for the current model on the input X.", "methodParamsBody": "X : array of shape (n_samples, n_dimensions) "}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Estimate model parameters with the EM algorithm. The method fit the model n_init times and set the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for max_iter times until the change of likelihood or lower bound is less than tol , otherwise, a ConvergenceWarning is raised.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "labels : array, shape (n_samples,)   Component labels.  ", "methodParams": ["X"], "methodReturns": ["labels"], "methodDesc": "Predict the labels for the data samples in X using trained model.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "resp : array, shape (n_samples, n_components)   Returns the probability each Gaussian (state) in the model given each sample.  ", "methodParams": ["X"], "methodReturns": ["resp"], "methodDesc": "Predict posterior probability of each component given the data.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "sample(n_samples=1)", "methodReturnsBody": "X : array, shape (n_samples, n_features)   Randomly generated sample   y : array, shape (nsamples,)   Component labels  ", "methodParams": ["n_samples"], "methodReturns": ["X", "y"], "methodDesc": "Generate random samples from the fitted Gaussian distribution.", "methodParamsBody": "n_samples : int, optional   Number of samples to generate. Defaults to 1.  "}, {"methodName": "score(X, y=None)", "methodReturnsBody": "log_likelihood : float   Log likelihood of the Gaussian mixture given X.  ", "methodParams": ["X"], "methodReturns": ["log_likelihood"], "methodDesc": "Compute the per-sample average log-likelihood of the given data X.", "methodParamsBody": "X : array-like, shape (n_samples, n_dimensions)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "score_samples(X)", "methodReturnsBody": "log_prob : array, shape (n_samples,)   Log probabilities of each data point in X.  ", "methodParams": ["X"], "methodReturns": ["log_prob"], "methodDesc": "Compute the weighted log probabilities for each sample.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["n_components", "covariance_type", "tol", "reg_covar", "max_iter", "n_init", "init_params", "weights_init", "means_init", "precisions_init", "random_state", "warm_start", "verbose", "verbose_interval"], "notes": "", "funcName": "GaussianMixture", "allFuncAttributes": ["weights_", "means_", "covariances_", "precisions_", "precisions_cholesky_", "converged_", "n_iter_", "lower_bound_"], "funcDesc": "Gaussian Mixture.", "funcParamBody": "n_components : int, defaults to 1. The number of mixture components. covariance_type : {full, tied, diag, spherical}, defaults to full. String describing the type of covariance parameters to use. Must be one of: 'full'  ( each  component  has  its  own  general  covariance  matrix ), 'tied'  ( all  components  share  the  same  general  covariance  matrix ), 'diag'  ( each  component  has  its  own  diagonal  covariance  matrix ), 'spherical'  ( each  component  has  its  own  single  variance ) . tol : float, defaults to 1e-3. The convergence threshold. EM iterations will stop when the lower bound average gain is below this threshold. reg_covar : float, defaults to 1e-6. Non-negative regularization added to the diagonal of covariance. Allows to assure that the covariance matrices are all positive. max_iter : int, defaults to 100. The number of EM iterations to perform. n_init : int, defaults to 1. The number of initializations to perform. The best results are kept. init_params : {kmeans, random}, defaults to kmeans. The method used to initialize the weights, the means and the precisions. Must be one of: 'kmeans'  :  responsibilities  are  initialized  using  kmeans . 'random'  :  responsibilities  are  initialized  randomly . weights_init : array-like, shape (n_components, ), optional The user-provided initial weights, defaults to None. If it None, weights are initialized using the init_params method. means_init : array-like, shape (n_components, n_features), optional The user-provided initial means, defaults to None, If it None, means are initialized using the init_params method. precisions_init : array-like, optional. The user-provided initial precisions (inverse of the covariance matrices), defaults to None. If it None, precisions are initialized using the init_params method. The shape depends on covariance_type: ( n_components ,)  if  'spherical' , ( n_features ,  n_features )  if  'tied' , ( n_components ,  n_features )  if  'diag' , ( n_components ,  n_features ,  n_features )  if  'full' random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . warm_start : bool, default to False. If warm_start is True, the solution of the last fitting is used as initialization for the next call of fit(). This can speed up convergence when fit is called several time on similar problems. verbose : int, default to 0. Enable verbose output. If 1 then it prints the current initialization and each iteration step. If greater than 1 then it prints also the log probability and the time needed for each step. verbose_interval : int, default to 10. Number of iteration done before the next print.", "funcAttrBody": "weights_ : array-like, shape (n_components,) The weights of each mixture components. means_ : array-like, shape (n_components, n_features) The mean of each mixture component. covariances_ : array-like The covariance of each mixture component. The shape depends on covariance_type : ( n_components ,)  if  'spherical' , ( n_features ,  n_features )  if  'tied' , ( n_components ,  n_features )  if  'diag' , ( n_components ,  n_features ,  n_features )  if  'full' precisions_ : array-like The precision matrices for each component in the mixture. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on covariance_type : ( n_components ,)  if  'spherical' , ( n_features ,  n_features )  if  'tied' , ( n_components ,  n_features )  if  'diag' , ( n_components ,  n_features ,  n_features )  if  'full' precisions_cholesky_ : array-like The cholesky decomposition of the precision matrices of each mixture component. A precision matrix is the inverse of a covariance matrix. A covariance matrix is symmetric positive definite so the mixture of Gaussian can be equivalently parameterized by the precision matrices. Storing the precision matrices instead of the covariance matrices makes it more efficient to compute the log-likelihood of new samples at test time. The shape depends on covariance_type : ( n_components ,)  if  'spherical' , ( n_features ,  n_features )  if  'tied' , ( n_components ,  n_features )  if  'diag' , ( n_components ,  n_features ,  n_features )  if  'full' converged_ : bool True when convergence was reached in fit(), False otherwise. n_iter_ : int Number of step used by the best fit of EM to reach the convergence. lower_bound_ : float Log-likelihood of the best fit of EM."},
{"funcName": "BayesianGaussianMixture", "notes": "", "libName": "sklearn.mixture", "methods": [{"methodName": "__init__(n_components=1, covariance_type=full, tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params=kmeans, weight_concentration_prior_type=dirichlet_process, weight_concentration_prior=None, mean_precision_prior=None, mean_prior=None, degrees_of_freedom_prior=None, covariance_prior=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Estimate model parameters with the EM algorithm. The method fit the model n_init times and set the parameters with which the model has the largest likelihood or lower bound. Within each trial, the method iterates between E-step and M-step for max_iter times until the change of likelihood or lower bound is less than tol , otherwise, a ConvergenceWarning is raised.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "labels : array, shape (n_samples,)   Component labels.  ", "methodParams": ["X"], "methodReturns": ["labels"], "methodDesc": "Predict the labels for the data samples in X using trained model.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "resp : array, shape (n_samples, n_components)   Returns the probability each Gaussian (state) in the model given each sample.  ", "methodParams": ["X"], "methodReturns": ["resp"], "methodDesc": "Predict posterior probability of each component given the data.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "sample(n_samples=1)", "methodReturnsBody": "X : array, shape (n_samples, n_features)   Randomly generated sample   y : array, shape (nsamples,)   Component labels  ", "methodParams": ["n_samples"], "methodReturns": ["X", "y"], "methodDesc": "Generate random samples from the fitted Gaussian distribution.", "methodParamsBody": "n_samples : int, optional   Number of samples to generate. Defaults to 1.  "}, {"methodName": "score(X, y=None)", "methodReturnsBody": "log_likelihood : float   Log likelihood of the Gaussian mixture given X.  ", "methodParams": ["X"], "methodReturns": ["log_likelihood"], "methodDesc": "Compute the per-sample average log-likelihood of the given data X.", "methodParamsBody": "X : array-like, shape (n_samples, n_dimensions)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "score_samples(X)", "methodReturnsBody": "log_prob : array, shape (n_samples,)   Log probabilities of each data point in X.  ", "methodParams": ["X"], "methodReturns": ["log_prob"], "methodDesc": "Compute the weighted log probabilities for each sample.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   List of n_features-dimensional data points. Each row corresponds to a single data point.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "Variational Bayesian estimation of a Gaussian mixture."},
{"allReturnParams": ["argmin", "distances"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "pairwise_distances_argmin_min", "allFuncParams": ["X", "Y", "axis", "metric", "batch_size", "metric_kwargs"], "funcDesc": "Compute minimum distances between one point and a set of points.", "funcParamBody": "X : {array-like, sparse matrix}, shape (n_samples1, n_features) Array containing points. Y : {array-like, sparse matrix}, shape (n_samples2, n_features) Arrays containing points. axis : int, optional, default 1 Axis along which the argmin and distances are to be computed. metric : string or callable, default euclidean metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used. If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipys metrics, but is less efficient than passing the metric name as a string. Distance matrices are not supported. Valid values for metric are: from scikit-learn: [cityblock, cosine, euclidean, l1, l2, manhattan] from scipy.spatial.distance: [braycurtis, canberra, chebyshev, correlation, dice, hamming, jaccard, kulsinski, mahalanobis, matching, minkowski, rogerstanimoto, russellrao, seuclidean, sokalmichener, sokalsneath, sqeuclidean, yule] See the documentation for scipy.spatial.distance for details on these metrics. batch_size : integer To reduce memory consumption over the naive solution, data are processed in batches, comprising batch_size rows of X and batch_size rows of Y. The default value is quite conservative, but can be changed for fine-tuning. The larger the number, the larger the memory usage. metric_kwargs : dict, optional Keyword arguments to pass to specified metric function.", "funcReturnBody": "argmin : numpy.ndarray Y[argmin[i], :] is the row in Y that is closest to X[i, :]. distances : numpy.ndarray distances[i] is the distance between the i-th row in X and the argmin[i]-th row in Y."},
{"allReturnParams": ["argmin"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "pairwise_distances_argmin", "allFuncParams": ["X", "Y", "axis", "metric", "batch_size", "metric_kwargs"], "funcDesc": "Compute minimum distances between one point and a set of points.", "funcParamBody": "X : array-like Arrays containing points. Respective shapes (n_samples1, n_features) and (n_samples2, n_features) Y : array-like Arrays containing points. Respective shapes (n_samples1, n_features) and (n_samples2, n_features) axis : int, optional, default 1 Axis along which the argmin and distances are to be computed. metric : string or callable metric to use for distance computation. Any metric from scikit-learn or scipy.spatial.distance can be used. If metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays as input and return one value indicating the distance between them. This works for Scipys metrics, but is less efficient than passing the metric name as a string. Distance matrices are not supported. Valid values for metric are: from scikit-learn: [cityblock, cosine, euclidean, l1, l2, manhattan] from scipy.spatial.distance: [braycurtis, canberra, chebyshev, correlation, dice, hamming, jaccard, kulsinski, mahalanobis, matching, minkowski, rogerstanimoto, russellrao, seuclidean, sokalmichener, sokalsneath, sqeuclidean, yule] See the documentation for scipy.spatial.distance for details on these metrics. batch_size : integer To reduce memory consumption over the naive solution, data are processed in batches, comprising batch_size rows of X and batch_size rows of Y. The default value is quite conservative, but can be changed for fine-tuning. The larger the number, the larger the memory usage. metric_kwargs : dict keyword arguments to pass to specified metric function.", "funcReturnBody": "argmin : numpy.ndarray Y[argmin[i], :] is the row in Y that is closest to X[i, :]."},
{"allReturnParams": ["D"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "pairwise_distances", "allFuncParams": ["X", "Y", "metric", "n_jobs", "**kwds"], "funcDesc": "Compute the distance matrix from a vector array X and optional Y.", "funcParamBody": "X : array [n_samples_a, n_samples_a] if metric == precomputed, or,              [n_samples_a, n_features] otherwise Array of pairwise distances between samples, or a feature array. Y : array [n_samples_b, n_features], optional An optional second feature array. Only allowed if metric != precomputed. metric : string, or callable The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is precomputed, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. n_jobs : int The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. **kwds : optional keyword parameters Any further parameters are passed directly to the distance function. If using a scipy.spatial.distance metric, the parameters are still metric dependent. See the scipy docs for usage examples.", "funcReturnBody": "D : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b] A distance matrix D such that D_{i, j} is the distance between the ith and jth vectors of the given matrix X, if Y is None. If Y is not None, then D_{i, j} is the distance between the ith array from X and the jth array from Y."},
{"allReturnParams": ["distances"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "", "funcName": "paired_distances", "allFuncParams": ["X", "Y", "metric"], "funcDesc": "Computes the paired distances between X and Y.", "funcParamBody": "X : ndarray (n_samples, n_features) Array 1 for distance computation. Y : ndarray (n_samples, n_features) Array 2 for distance computation. metric : string or callable The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options specified in PAIRED_DISTANCES, including euclidean, manhattan, or cosine. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them.", "funcReturnBody": "distances : ndarray (n_samples, )"},
{"allReturnParams": ["distances"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "The cosine distance is equivalent to the half the squared euclidean distance if each sample is normalized to unit norm", "funcName": "paired_cosine_distances", "allFuncParams": ["X", "Y"], "funcDesc": "Computes the paired cosine distances between X and Y", "funcParamBody": "X : array-like, shape (n_samples, n_features) Y : array-like, shape (n_samples, n_features)", "funcReturnBody": "distances : ndarray, shape (n_samples, )"},
{"allReturnParams": ["distances"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "", "funcName": "paired_manhattan_distances", "allFuncParams": ["X", "Y"], "funcDesc": "Compute the L1 distances between the vectors in X and Y.", "funcParamBody": "X : array-like, shape (n_samples, n_features) Y : array-like, shape (n_samples, n_features)", "funcReturnBody": "distances : ndarray (n_samples, )"},
{"allReturnParams": ["distances"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "", "funcName": "paired_euclidean_distances", "allFuncParams": ["X", "Y"], "funcDesc": "Computes the paired euclidean distances between X and Y", "funcParamBody": "X : array-like, shape (n_samples, n_features) Y : array-like, shape (n_samples, n_features)", "funcReturnBody": "distances : ndarray (n_samples, )"},
{"allReturnParams": ["Gram matrix"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "", "funcName": "sigmoid_kernel", "allFuncParams": ["X", "Y", "gamma", "coef0"], "funcDesc": "Compute the sigmoid kernel between X and Y:", "funcParamBody": "X : ndarray of shape (n_samples_1, n_features) Y : ndarray of shape (n_samples_2, n_features) gamma : float, default None If None, defaults to 1.0 / n_features coef0 : int, default 1", "funcReturnBody": "Gram matrix : array of shape (n_samples_1, n_samples_2)"},
{"allReturnParams": ["kernel_matrix"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "", "funcName": "rbf_kernel", "allFuncParams": ["X", "Y", "gamma"], "funcDesc": "Compute the rbf (gaussian) kernel between X and Y:", "funcParamBody": "X : array of shape (n_samples_X, n_features) Y : array of shape (n_samples_Y, n_features) gamma : float, default None If None, defaults to 1.0 / n_features", "funcReturnBody": "kernel_matrix : array of shape (n_samples_X, n_samples_Y)"},
{"allReturnParams": ["Gram matrix"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "", "funcName": "polynomial_kernel", "allFuncParams": ["X", "Y", "degree", "gamma", "coef0"], "funcDesc": "Compute the polynomial kernel between X and Y:", "funcParamBody": "X : ndarray of shape (n_samples_1, n_features) Y : ndarray of shape (n_samples_2, n_features) degree : int, default 3 gamma : float, default None if None, defaults to 1.0 / n_features coef0 : int, default 1", "funcReturnBody": "Gram matrix : array of shape (n_samples_1, n_samples_2)"},
{"allReturnParams": ["K"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "If metric is precomputed, Y is ignored and X is returned.", "funcName": "pairwise_kernels", "allFuncParams": ["X", "Y", "metric", "filter_params", "n_jobs", "**kwds"], "funcDesc": "Compute the kernel between arrays X and optional array Y.", "funcParamBody": "X : array [n_samples_a, n_samples_a] if metric == precomputed, or,              [n_samples_a, n_features] otherwise Array of pairwise kernels between samples, or a feature array. Y : array [n_samples_b, n_features] A second feature array only if X has shape [n_samples_a, n_features]. metric : string, or callable The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is precomputed, X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. filter_params : boolean Whether to filter invalid parameters or not. n_jobs : int The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. **kwds : optional keyword parameters Any further parameters are passed directly to the kernel function.", "funcReturnBody": "K : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b] A kernel matrix K such that K_{i, j} is the kernel between the ith and jth vectors of the given matrix X, if Y is None. If Y is not None, then K_{i, j} is the kernel between the ith array from X and the jth array from Y."},
{"allReturnParams": ["D"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "", "funcName": "pairwise_distances", "allFuncParams": ["X", "Y", "metric", "n_jobs", "**kwds"], "funcDesc": "Compute the distance matrix from a vector array X and optional Y.", "funcParamBody": "X : array [n_samples_a, n_samples_a] if metric == precomputed, or,              [n_samples_a, n_features] otherwise Array of pairwise distances between samples, or a feature array. Y : array [n_samples_b, n_features], optional An optional second feature array. Only allowed if metric != precomputed. metric : string, or callable The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is precomputed, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. n_jobs : int The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. **kwds : optional keyword parameters Any further parameters are passed directly to the distance function. If using a scipy.spatial.distance metric, the parameters are still metric dependent. See the scipy docs for usage examples.", "funcReturnBody": "D : array [n_samples_a, n_samples_a] or [n_samples_a, n_samples_b] A distance matrix D such that D_{i, j} is the distance between the ith and jth vectors of the given matrix X, if Y is None. If Y is not None, then D_{i, j} is the distance between the ith array from X and the jth array from Y."},
{"allReturnParams": ["D"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "", "funcName": "manhattan_distances", "allFuncParams": ["X", "Y", "sum_over_features", "size_threshold"], "funcDesc": "Compute the L1 distances between the vectors in X and Y.", "funcParamBody": "X : array_like An array with shape (n_samples_X, n_features). Y : array_like, optional An array with shape (n_samples_Y, n_features). sum_over_features : bool, default=True If True the function returns the pairwise distance matrix else it returns the componentwise L1 pairwise-distances. Not supported for sparse matrix inputs. size_threshold : int, default=5e8 Unused parameter.", "funcReturnBody": "D : array If sum_over_features is False shape is (n_samples_X * n_samples_Y, n_features) and D contains the componentwise L1 pairwise-distances (ie. absolute difference), else shape is (n_samples_X, n_samples_Y) and D contains the pairwise L1 distances."},
{"allReturnParams": ["Gram matrix"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "", "funcName": "linear_kernel", "allFuncParams": ["X", "Y"], "funcDesc": "Compute the linear kernel between X and Y.", "funcParamBody": "X : array of shape (n_samples_1, n_features) Y : array of shape (n_samples_2, n_features)", "funcReturnBody": "Gram matrix : array of shape (n_samples_1, n_samples_2)"},
{"allReturnParams": ["kernel_matrix"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "", "funcName": "laplacian_kernel", "allFuncParams": ["X", "Y", "gamma"], "funcDesc": "Compute the laplacian kernel between X and Y.", "funcParamBody": "X : array of shape (n_samples_X, n_features) Y : array of shape (n_samples_Y, n_features) gamma : float, default None If None, defaults to 1.0 / n_features", "funcReturnBody": "kernel_matrix : array of shape (n_samples_X, n_samples_Y)"},
{"funcName": "kernel_metrics", "notes": "", "libName": "sklearn.metrics.pairwise", "methods": [], "funcDesc": "Valid metrics for pairwise_kernels"},
{"allReturnParams": ["distances"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "", "funcName": "euclidean_distances", "allFuncParams": ["X", "Y", "Y_norm_squared", "squared", "X_norm_squared"], "funcDesc": "Considering the rows of X (and Y=X) as vectors, compute the distance matrix between each pair of vectors.", "funcParamBody": "X : {array-like, sparse matrix}, shape (n_samples_1, n_features) Y : {array-like, sparse matrix}, shape (n_samples_2, n_features) Y_norm_squared : array-like, shape (n_samples_2, ), optional Pre-computed dot-products of vectors in Y (e.g., (Y**2).sum(axis=1) ) squared : boolean, optional Return squared Euclidean distances. X_norm_squared : array-like, shape = [n_samples_1], optional Pre-computed dot-products of vectors in X (e.g., (X**2).sum(axis=1) )", "funcReturnBody": "distances : {array, sparse matrix}, shape (n_samples_1, n_samples_2)"},
{"funcName": "distance_metrics", "notes": "", "libName": "sklearn.metrics.pairwise", "methods": [], "funcDesc": "Valid metrics for pairwise_distances."},
{"allReturnParams": ["distance matrix"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "", "funcName": "cosine_distances", "allFuncParams": ["X", "Y"], "funcDesc": "Compute cosine distance between samples in X and Y.", "funcParamBody": "X : array_like, sparse matrix with shape (n_samples_X, n_features). Y : array_like, sparse matrix (optional) with shape (n_samples_Y, n_features).", "funcReturnBody": "distance matrix : array An array with shape (n_samples_X, n_samples_Y)."},
{"allReturnParams": ["kernel matrix"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "", "funcName": "cosine_similarity", "allFuncParams": ["X", "Y", "dense_output"], "funcDesc": "Compute cosine similarity between samples in X and Y.", "funcParamBody": "X : ndarray or sparse array, shape: (n_samples_X, n_features) Input data. Y : ndarray or sparse array, shape: (n_samples_Y, n_features) Input data. If None , the output will be the pairwise similarities between all samples in X . dense_output : boolean (optional), default True Whether to return dense output even when the input is sparse. If False , the output is sparse if both input arrays are sparse. New in version 0.17: parameter dense_output for dense output.", "funcReturnBody": "kernel matrix : array An array with shape (n_samples_X, n_samples_Y)."},
{"allReturnParams": ["kernel_matrix"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "", "funcName": "chi2_kernel", "allFuncParams": ["X", "Y", "gamma"], "funcDesc": "Computes the exponential chi-squared kernel X and Y.", "funcParamBody": "X : array-like of shape (n_samples_X, n_features) Y : array of shape (n_samples_Y, n_features) gamma : float, default=1. Scaling parameter of the chi2 kernel.", "funcReturnBody": "kernel_matrix : array of shape (n_samples_X, n_samples_Y)"},
{"allReturnParams": ["kernel_matrix"], "libName": "sklearn.metrics.pairwise", "methods": [], "notes": "As the negative of a distance, this kernel is only conditionally positive definite.", "funcName": "additive_chi2_kernel", "allFuncParams": ["X", "Y"], "funcDesc": "Computes the additive chi-squared kernel between observations in X and Y", "funcParamBody": "X : array-like of shape (n_samples_X, n_features) Y : array of shape (n_samples_Y, n_features)", "funcReturnBody": "kernel_matrix : array of shape (n_samples_X, n_samples_Y)"},
{"libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "consensus_score", "allFuncParams": ["a", "b", "similarity"], "funcDesc": "The similarity of two sets of biclusters.", "funcParamBody": "a : (rows, columns) Tuple of row and column indicators for a set of biclusters. b : (rows, columns) Another set of biclusters like a . similarity : string or function, optional, default: jaccard May be the string jaccard to use the Jaccard coefficient, or any function that takes four arguments, each of which is a 1d indicator vector: (a_rows, a_columns, b_rows, b_columns)."},
{"allReturnParams": ["v_measure"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "v_measure_score", "allFuncParams": ["labels_true", "labels_pred"], "funcDesc": "V-measure cluster labeling given a ground truth.", "funcParamBody": "labels_true : int array, shape = [n_samples] ground truth class labels to be used as a reference labels_pred : array, shape = [n_samples] cluster labels to evaluate", "funcReturnBody": "v_measure : float score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling"},
{"allReturnParams": ["silhouette"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "silhouette_samples", "allFuncParams": ["X", "labels", "metric", "**kwds"], "funcDesc": "Compute the Silhouette Coefficient for each sample.", "funcParamBody": "X : array [n_samples_a, n_samples_a] if metric == precomputed, or,              [n_samples_a, n_features] otherwise Array of pairwise distances between samples, or a feature array. labels : array, shape = [n_samples] label values for each sample metric : string, or callable The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by sklearn.metrics.pairwise.pairwise_distances . If X is the distance array itself, use precomputed as the metric. **kwds : optional keyword parameters Any further parameters are passed directly to the distance function. If using a scipy.spatial.distance metric, the parameters are still metric dependent. See the scipy docs for usage examples.", "funcReturnBody": "silhouette : array, shape = [n_samples] Silhouette Coefficient for each samples."},
{"allReturnParams": ["silhouette"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "silhouette_score", "allFuncParams": ["X", "labels", "metric", "sample_size", "random_state", "**kwds"], "funcDesc": "Compute the mean Silhouette Coefficient of all samples.", "funcParamBody": "X : array [n_samples_a, n_samples_a] if metric == precomputed, or,              [n_samples_a, n_features] otherwise Array of pairwise distances between samples, or a feature array. labels : array, shape = [n_samples] Predicted labels for each sample. metric : string, or callable The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by metrics.pairwise.pairwise_distances . If X is the distance array itself, use metric=\"precomputed\" . sample_size : int or None The size of the sample to use when computing the Silhouette Coefficient on a random subset of the data. If sample_size  is  None , no sampling is used. random_state : int, RandomState instance or None, optional (default=None) The generator used to randomly select a subset of samples.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when sample_size  is  not  None . **kwds : optional keyword parameters Any further parameters are passed directly to the distance function. If using a scipy.spatial.distance metric, the parameters are still metric dependent. See the scipy docs for usage examples.", "funcReturnBody": "silhouette : float Mean Silhouette Coefficient for all samples."},
{"allReturnParams": ["nmi"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "normalized_mutual_info_score", "allFuncParams": ["labels_true", "labels_pred"], "funcDesc": "Normalized Mutual Information between two clusterings.", "funcParamBody": "labels_true : int array, shape = [n_samples] A clustering of the data into disjoint subsets. labels_pred : array, shape = [n_samples] A clustering of the data into disjoint subsets.", "funcReturnBody": "nmi : float score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling"},
{"allReturnParams": ["mi"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "mutual_info_score", "allFuncParams": ["labels_true", "labels_pred", "contingency"], "funcDesc": "Mutual Information between two clusterings.", "funcParamBody": "labels_true : int array, shape = [n_samples] A clustering of the data into disjoint subsets. labels_pred : array, shape = [n_samples] A clustering of the data into disjoint subsets. contingency : {None, array, sparse matrix}, shape = [n_classes_true, n_classes_pred] A contingency matrix given by the contingency_matrix function. If value is None , it will be computed, otherwise the given value is used, with labels_true and labels_pred ignored.", "funcReturnBody": "mi : float Mutual information, a non-negative value"},
{"allReturnParams": ["homogeneity"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "homogeneity_score", "allFuncParams": ["labels_true", "labels_pred"], "funcDesc": "Homogeneity metric of a cluster labeling given a ground truth.", "funcParamBody": "labels_true : int array, shape = [n_samples] ground truth class labels to be used as a reference labels_pred : array, shape = [n_samples] cluster labels to evaluate", "funcReturnBody": "homogeneity : float score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling"},
{"allReturnParams": ["homogeneity", "completeness", "v_measure"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "homogeneity_completeness_v_measure", "allFuncParams": ["labels_true", "labels_pred"], "funcDesc": "Compute the homogeneity and completeness and V-Measure scores at once.", "funcParamBody": "labels_true : int array, shape = [n_samples] ground truth class labels to be used as a reference labels_pred : array, shape = [n_samples] cluster labels to evaluate", "funcReturnBody": "homogeneity : float score between 0.0 and 1.0. 1.0 stands for perfectly homogeneous labeling completeness : float score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling v_measure : float harmonic mean of the first two"},
{"allReturnParams": ["score"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "fowlkes_mallows_score", "allFuncParams": ["labels_true", "labels_pred", "sparse"], "funcDesc": "Measure the similarity of two clusterings of a set of points.", "funcParamBody": "labels_true : int array, shape = ( n_samples ,) A clustering of the data into disjoint subsets. labels_pred : array, shape = ( n_samples , ) A clustering of the data into disjoint subsets. sparse : bool Compute contingency matrix internally with sparse matrix.", "funcReturnBody": "score : float The resulting Fowlkes-Mallows score."},
{"allReturnParams": ["completeness"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "completeness_score", "allFuncParams": ["labels_true", "labels_pred"], "funcDesc": "Completeness metric of a cluster labeling given a ground truth.", "funcParamBody": "labels_true : int array, shape = [n_samples] ground truth class labels to be used as a reference labels_pred : array, shape = [n_samples] cluster labels to evaluate", "funcReturnBody": "completeness : float score between 0.0 and 1.0. 1.0 stands for perfectly complete labeling"},
{"allReturnParams": ["score"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "calinski_harabaz_score", "allFuncParams": ["X", "labels"], "funcDesc": "Compute the Calinski and Harabaz score.", "funcParamBody": "X : array-like, shape ( n_samples , n_features ) List of n_features -dimensional data points. Each row corresponds to a single data point. labels : array-like, shape ( n_samples ,) Predicted labels for each sample.", "funcReturnBody": "score : float The resulting Calinski-Harabaz score."},
{"allReturnParams": ["ari"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "adjusted_rand_score", "allFuncParams": ["labels_true", "labels_pred"], "funcDesc": "Rand index adjusted for chance.", "funcParamBody": "labels_true : int array, shape = [n_samples] Ground truth class labels to be used as a reference labels_pred : array, shape = [n_samples] Cluster labels to evaluate", "funcReturnBody": "ari : float Similarity score between -1.0 and 1.0. Random labelings have an ARI close to 0.0. 1.0 stands for perfect match."},
{"allReturnParams": ["ami: float(upperlimited by 1.0)"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "adjusted_mutual_info_score", "allFuncParams": ["labels_true", "labels_pred"], "funcDesc": "Adjusted Mutual Information between two clusterings.", "funcParamBody": "labels_true : int array, shape = [n_samples] A clustering of the data into disjoint subsets. labels_pred : array, shape = [n_samples] A clustering of the data into disjoint subsets.", "funcReturnBody": "ami: float(upperlimited by 1.0) : The AMI returns a value of 1 when the two partitions are identical (ie perfectly matched). Random partitions (independent labellings) have an expected AMI around 0 on average hence can be negative."},
{"allReturnParams": ["loss"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "label_ranking_loss", "allFuncParams": ["y_true", "y_score", "sample_weight"], "funcDesc": "Compute Ranking loss measure", "funcParamBody": "y_true : array or sparse matrix, shape = [n_samples, n_labels] True binary labels in binary indicator format. y_score : array, shape = [n_samples, n_labels] Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by decision_function on some classifiers). sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "loss : float"},
{"allReturnParams": ["score"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "label_ranking_average_precision_score", "allFuncParams": ["y_true", "y_score"], "funcDesc": "Compute ranking-based average precision", "funcParamBody": "y_true : array or sparse matrix, shape = [n_samples, n_labels] True binary labels in binary indicator format. y_score : array, shape = [n_samples, n_labels] Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by decision_function on some classifiers).", "funcReturnBody": "score : float"},
{"allReturnParams": ["coverage_error"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "coverage_error", "allFuncParams": ["y_true", "y_score", "sample_weight"], "funcDesc": "Coverage error measure", "funcParamBody": "y_true : array, shape = [n_samples, n_labels] True binary labels in binary indicator format. y_score : array, shape = [n_samples, n_labels] Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by decision_function on some classifiers). sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "coverage_error : float"},
{"allReturnParams": ["z"], "libName": "sklearn.metrics", "methods": [], "notes": "This is not a symmetric function. Unlike most other scores, R^2 score may be negative (it need not actually be the square of a quantity R). This is not a symmetric function. Unlike most other scores, R^2 score may be negative (it need not actually be the square of a quantity R).", "funcName": "r2_score", "allFuncParams": ["y_true", "y_pred", "sample_weight", "multioutput"], "funcDesc": "R^2 (coefficient of determination) regression score function.", "funcParamBody": "y_true : array-like of shape = (n_samples) or (n_samples, n_outputs) Ground truth (correct) target values. y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs) Estimated target values. sample_weight : array-like of shape = (n_samples), optional Sample weights. multioutput : string in [raw_values, uniform_average, variance_weighted] or None or array-like of shape (n_outputs) Defines aggregating of multiple output scores. Array-like value defines weights used to average scores. Default is uniform_average. raw_values : Returns a full set of scores in case of multioutput input. uniform_average : Scores of all outputs are averaged with uniform weight. variance_weighted : Scores of all outputs are averaged, weighted by the variances of each individual output. Changed in version 0.19: Default value of multioutput is uniform_average.", "funcReturnBody": "z : float or ndarray of floats The R^2 score or ndarray of scores if multioutput is raw_values."},
{"allReturnParams": ["loss"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "median_absolute_error", "allFuncParams": ["y_true", "y_pred"], "funcDesc": "Median absolute error regression loss", "funcParamBody": "y_true : array-like of shape = (n_samples) Ground truth (correct) target values. y_pred : array-like of shape = (n_samples) Estimated target values.", "funcReturnBody": "loss : float A positive floating point value (the best value is 0.0)."},
{"allReturnParams": ["loss"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "mean_squared_log_error", "allFuncParams": ["y_true", "y_pred", "sample_weight", "multioutput"], "funcDesc": "Mean squared logarithmic error regression loss", "funcParamBody": "y_true : array-like of shape = (n_samples) or (n_samples, n_outputs) Ground truth (correct) target values. y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs) Estimated target values. sample_weight : array-like of shape = (n_samples), optional Sample weights. multioutput : string in [raw_values, uniform_average]             or array-like of shape = (n_outputs) Defines aggregating of multiple output values. Array-like value defines weights used to average errors. raw_values : Returns a full set of errors when the input is of multioutput format. uniform_average : Errors of all outputs are averaged with uniform weight.", "funcReturnBody": "loss : float or ndarray of floats A non-negative floating point value (the best value is 0.0), or an array of floating point values, one for each individual target."},
{"allReturnParams": ["loss"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "mean_squared_error", "allFuncParams": ["y_true", "y_pred", "sample_weight", "multioutput"], "funcDesc": "Mean squared error regression loss", "funcParamBody": "y_true : array-like of shape = (n_samples) or (n_samples, n_outputs) Ground truth (correct) target values. y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs) Estimated target values. sample_weight : array-like of shape = (n_samples), optional Sample weights. multioutput : string in [raw_values, uniform_average] or array-like of shape (n_outputs) Defines aggregating of multiple output values. Array-like value defines weights used to average errors. raw_values : Returns a full set of errors in case of multioutput input. uniform_average : Errors of all outputs are averaged with uniform weight.", "funcReturnBody": "loss : float or ndarray of floats A non-negative floating point value (the best value is 0.0), or an array of floating point values, one for each individual target."},
{"allReturnParams": ["loss"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "mean_absolute_error", "allFuncParams": ["y_true", "y_pred", "sample_weight", "multioutput"], "funcDesc": "Mean absolute error regression loss", "funcParamBody": "y_true : array-like of shape = (n_samples) or (n_samples, n_outputs) Ground truth (correct) target values. y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs) Estimated target values. sample_weight : array-like of shape = (n_samples), optional Sample weights. multioutput : string in [raw_values, uniform_average] or array-like of shape (n_outputs) Defines aggregating of multiple output values. Array-like value defines weights used to average errors. raw_values : Returns a full set of errors in case of multioutput input. uniform_average : Errors of all outputs are averaged with uniform weight.", "funcReturnBody": "loss : float or ndarray of floats If multioutput is raw_values, then mean absolute error is returned for each output separately. If multioutput is uniform_average or an ndarray of weights, then the weighted average of all output errors is returned. MAE output is non-negative floating point. The best value is 0.0."},
{"allReturnParams": ["score"], "libName": "sklearn.metrics", "methods": [], "notes": "This is not a symmetric function.", "funcName": "explained_variance_score", "allFuncParams": ["y_true", "y_pred", "sample_weight", "multioutput"], "funcDesc": "Explained variance regression score function", "funcParamBody": "y_true : array-like of shape = (n_samples) or (n_samples, n_outputs) Ground truth (correct) target values. y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs) Estimated target values. sample_weight : array-like of shape = (n_samples), optional Sample weights. multioutput : string in [raw_values, uniform_average,                 variance_weighted] or array-like of shape (n_outputs) Defines aggregating of multiple output scores. Array-like value defines weights used to average scores. raw_values : Returns a full set of scores in case of multioutput input. uniform_average : Scores of all outputs are averaged with uniform weight. variance_weighted : Scores of all outputs are averaged, weighted by the variances of each individual output.", "funcReturnBody": "score : float or ndarray of floats The explained variance or ndarray if multioutput is raw_values."},
{"allReturnParams": ["loss"], "libName": "sklearn.metrics", "methods": [], "notes": "In multilabel classification, the zero_one_loss function corresponds to the subset zero-one loss: for each sample, the entire set of labels must be correctly predicted, otherwise the loss for that sample is equal to one. In the multilabel case with binary label indicators: In multilabel classification, the zero_one_loss function corresponds to the subset zero-one loss: for each sample, the entire set of labels must be correctly predicted, otherwise the loss for that sample is equal to one. In the multilabel case with binary label indicators:", "funcName": "zero_one_loss", "allFuncParams": ["y_true", "y_pred", "normalize", "sample_weight"], "funcDesc": "Zero-one classification loss.", "funcParamBody": "y_true : 1d array-like, or label indicator array / sparse matrix Ground truth (correct) labels. y_pred : 1d array-like, or label indicator array / sparse matrix Predicted labels, as returned by a classifier. normalize : bool, optional (default=True) If False , return the number of misclassifications. Otherwise, return the fraction of misclassifications. sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "loss : float or int, If normalize  ==  True , return the fraction of misclassifications (float), else it returns the number of misclassifications (int)."},
{"allReturnParams": ["fpr", "tpr", "thresholds"], "libName": "sklearn.metrics", "methods": [], "notes": "Since the thresholds are sorted from low to high values, they are reversed upon returning them to ensure they correspond to both fpr and tpr , which are sorted in reversed order during their calculation. Since the thresholds are sorted from low to high values, they are reversed upon returning them to ensure they correspond to both fpr and tpr , which are sorted in reversed order during their calculation.", "funcName": "roc_curve", "allFuncParams": ["y_true", "y_score", "pos_label", "sample_weight", "drop_intermediate"], "funcDesc": "Compute Receiver operating characteristic (ROC)", "funcParamBody": "y_true : array, shape = [n_samples] True binary labels in range {0, 1} or {-1, 1}.  If labels are not binary, pos_label should be explicitly given. y_score : array, shape = [n_samples] Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by decision_function on some classifiers). pos_label : int or str, default=None Label considered as positive and others are considered negative. sample_weight : array-like of shape = [n_samples], optional Sample weights. drop_intermediate : boolean, optional (default=True) Whether to drop some suboptimal thresholds which would not appear on a plotted ROC curve. This is useful in order to create lighter ROC curves. New in version 0.17: parameter drop_intermediate .", "funcReturnBody": "fpr : array, shape = [>2] Increasing false positive rates such that element i is the false positive rate of predictions with score >= thresholds[i]. tpr : array, shape = [>2] Increasing true positive rates such that element i is the true positive rate of predictions with score >= thresholds[i]. thresholds : array, shape = [n_thresholds] Decreasing thresholds on the decision function used to compute fpr and tpr. thresholds[0] represents no instances being predicted and is arbitrarily set to max(y_score) + 1 ."},
{"allReturnParams": ["auc"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "roc_auc_score", "allFuncParams": ["y_true", "y_score", "average", "sample_weight"], "funcDesc": "Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.", "funcParamBody": "y_true : array, shape = [n_samples] or [n_samples, n_classes] True binary labels in binary label indicators. y_score : array, shape = [n_samples] or [n_samples, n_classes] Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by decision_function on some classifiers). average : string, [None, micro, macro (default), samples, weighted] If None , the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data: 'micro' : Calculate metrics globally by considering each element of the label indicator matrix as a label. 'macro' : Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account. 'weighted' : Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). 'samples' : Calculate metrics for each instance, and find their average. sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "auc : float"},
{"allReturnParams": ["recall"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "recall_score", "allFuncParams": ["y_true", "y_pred", "labels", "pos_label", "average", "sample_weight"], "funcDesc": "Compute the recall", "funcParamBody": "y_true : 1d array-like, or label indicator array / sparse matrix Ground truth (correct) target values. y_pred : 1d array-like, or label indicator array / sparse matrix Estimated targets as returned by a classifier. labels : list, optional The set of labels to include when average  !=  'binary' , and their order if average  is  None . Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in y_true and y_pred are used in sorted order. Changed in version 0.17: parameter labels improved for multiclass problem. pos_label : str or int, 1 by default The class to report if average='binary' and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting labels=[pos_label] and average  !=  'binary' will report scores for that label only. average : string, [None, binary (default), micro, macro, samples,                        weighted] This parameter is required for multiclass/multilabel targets. If None , the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data: 'binary' : Only report results for the class specified by pos_label . This is applicable only if targets ( y_{true,pred} ) are binary. 'micro' : Calculate metrics globally by counting the total true positives, false negatives and false positives. 'macro' : Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account. 'weighted' : Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters macro to account for label imbalance; it can result in an F-score that is not between precision and recall. 'samples' : Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score ). sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "recall : float (if average is not None) or array of float, shape =        [n_unique_labels] Recall of the positive class in binary classification or weighted average of the recall of each class for the multiclass task."},
{"allReturnParams": ["precision"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "precision_score", "allFuncParams": ["y_true", "y_pred", "labels", "pos_label", "average", "sample_weight"], "funcDesc": "Compute the precision", "funcParamBody": "y_true : 1d array-like, or label indicator array / sparse matrix Ground truth (correct) target values. y_pred : 1d array-like, or label indicator array / sparse matrix Estimated targets as returned by a classifier. labels : list, optional The set of labels to include when average  !=  'binary' , and their order if average  is  None . Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in y_true and y_pred are used in sorted order. Changed in version 0.17: parameter labels improved for multiclass problem. pos_label : str or int, 1 by default The class to report if average='binary' and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting labels=[pos_label] and average  !=  'binary' will report scores for that label only. average : string, [None, binary (default), micro, macro, samples,                        weighted] This parameter is required for multiclass/multilabel targets. If None , the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data: 'binary' : Only report results for the class specified by pos_label . This is applicable only if targets ( y_{true,pred} ) are binary. 'micro' : Calculate metrics globally by counting the total true positives, false negatives and false positives. 'macro' : Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account. 'weighted' : Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters macro to account for label imbalance; it can result in an F-score that is not between precision and recall. 'samples' : Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score ). sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "precision : float (if average is not None) or array of float, shape =        [n_unique_labels] Precision of the positive class in binary classification or weighted average of the precision of each class for the multiclass task."},
{"allReturnParams": ["precision", "recall", "fbeta_score", "support"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "precision_recall_fscore_support", "allFuncParams": ["y_true", "y_pred", "beta", "labels", "pos_label", "average", "warn_for", "sample_weight"], "funcDesc": "Compute precision, recall, F-measure and support for each class", "funcParamBody": "y_true : 1d array-like, or label indicator array / sparse matrix Ground truth (correct) target values. y_pred : 1d array-like, or label indicator array / sparse matrix Estimated targets as returned by a classifier. beta : float, 1.0 by default The strength of recall versus precision in the F-score. labels : list, optional The set of labels to include when average  !=  'binary' , and their order if average  is  None . Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in y_true and y_pred are used in sorted order. pos_label : str or int, 1 by default The class to report if average='binary' and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting labels=[pos_label] and average  !=  'binary' will report scores for that label only. average : string, [None (default), binary, micro, macro, samples,                        weighted] If None , the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data: 'binary' : Only report results for the class specified by pos_label . This is applicable only if targets ( y_{true,pred} ) are binary. 'micro' : Calculate metrics globally by counting the total true positives, false negatives and false positives. 'macro' : Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account. 'weighted' : Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters macro to account for label imbalance; it can result in an F-score that is not between precision and recall. 'samples' : Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score ). warn_for : tuple or set, for internal use This determines which warnings will be made in the case that this function is being used to return only one of its metrics. sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "precision : float (if average is not None) or array of float, shape =        [n_unique_labels] recall : float (if average is not None) or array of float, , shape =        [n_unique_labels] fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels] support : int (if average is not None) or array of int, shape =        [n_unique_labels] The number of occurrences of each label in y_true ."},
{"allReturnParams": ["precision", "recall", "thresholds"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "precision_recall_curve", "allFuncParams": ["y_true", "probas_pred", "pos_label", "sample_weight"], "funcDesc": "Compute precision-recall pairs for different probability thresholds", "funcParamBody": "y_true : array, shape = [n_samples] True targets of binary classification in range {-1, 1} or {0, 1}. probas_pred : array, shape = [n_samples] Estimated probabilities or decision function. pos_label : int or str, default=None The label of the positive class sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "precision : array, shape = [n_thresholds + 1] Precision values such that element i is the precision of predictions with score >= thresholds[i] and the last element is 1. recall : array, shape = [n_thresholds + 1] Decreasing recall values such that element i is the recall of predictions with score >= thresholds[i] and the last element is 0. thresholds : array, shape = [n_thresholds <= len(np.unique(probas_pred))] Increasing thresholds on the decision function used to compute precision and recall."},
{"allReturnParams": ["mcc"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "matthews_corrcoef", "allFuncParams": ["y_true", "y_pred", "sample_weight"], "funcDesc": "Compute the Matthews correlation coefficient (MCC)", "funcParamBody": "y_true : array, shape = [n_samples] Ground truth (correct) target values. y_pred : array, shape = [n_samples] Estimated targets as returned by a classifier. sample_weight : array-like of shape = [n_samples], default None Sample weights.", "funcReturnBody": "mcc : float The Matthews correlation coefficient (+1 represents a perfect prediction, 0 an average random prediction and -1 and inverse prediction)."},
{"allReturnParams": ["loss"], "libName": "sklearn.metrics", "methods": [], "notes": "The logarithm used is the natural logarithm (base-e). C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer, p. 209. The logarithm used is the natural logarithm (base-e). C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer, p. 209.", "funcName": "log_loss", "allFuncParams": ["y_true", "y_pred", "eps", "normalize", "sample_weight", "labels"], "funcDesc": "Log loss, aka logistic loss or cross-entropy loss.", "funcParamBody": "y_true : array-like or label indicator matrix Ground truth (correct) labels for n_samples samples. y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,) Predicted probabilities, as returned by a classifiers predict_proba method. If y_pred.shape  =  (n_samples,) y_pred are assumed to be ordered alphabetically, as done by preprocessing.LabelBinarizer . eps : float Log loss is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p)). normalize : bool, optional (default=True) If true, return the mean loss per sample. Otherwise, return the sum of the per-sample losses. sample_weight : array-like of shape = [n_samples], optional Sample weights. labels : array-like, optional (default=None) If not provided, labels will be inferred from y_true. If labels None and y_pred has shape (n_samples,) the labels are assumed to be binary and are inferred from y_true . .. versionadded:: 0.18", "funcReturnBody": "loss : float"},
{"allReturnParams": ["score"], "libName": "sklearn.metrics", "methods": [], "notes": "In binary and multiclass classification, this function is equivalent to the accuracy_score . It differs in the multilabel classification problem. In the multilabel case with binary label indicators: In binary and multiclass classification, this function is equivalent to the accuracy_score . It differs in the multilabel classification problem. In the multilabel case with binary label indicators:", "funcName": "jaccard_similarity_score", "allFuncParams": ["y_true", "y_pred", "normalize", "sample_weight"], "funcDesc": "Jaccard similarity coefficient score", "funcParamBody": "y_true : 1d array-like, or label indicator array / sparse matrix Ground truth (correct) labels. y_pred : 1d array-like, or label indicator array / sparse matrix Predicted labels, as returned by a classifier. normalize : bool, optional (default=True) If False , return the sum of the Jaccard similarity coefficient over the sample set. Otherwise, return the average of Jaccard similarity coefficient. sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "score : float If normalize  ==  True , return the average Jaccard similarity coefficient, else it returns the sum of the Jaccard similarity coefficient over the sample set. The best performance is 1 with normalize  ==  True and the number of samples with normalize  ==  False ."},
{"allReturnParams": ["loss"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "hinge_loss", "allFuncParams": ["y_true", "pred_decision", "labels", "sample_weight"], "funcDesc": "Average hinge loss (non-regularized)", "funcParamBody": "y_true : array, shape = [n_samples] True target, consisting of integers of two values. The positive label must be greater than the negative label. pred_decision : array, shape = [n_samples] or [n_samples, n_classes] Predicted decisions, as output by decision_function (floats). labels : array, optional, default None Contains all the labels for the problem. Used in multiclass hinge loss. sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "loss : float"},
{"allReturnParams": ["loss"], "libName": "sklearn.metrics", "methods": [], "notes": "In multiclass classification, the Hamming loss correspond to the Hamming distance between y_true and y_pred which is equivalent to the subset zero_one_loss function. In multilabel classification, the Hamming loss is different from the subset zero-one loss. The zero-one loss considers the entire set of labels for a given sample incorrect if it does entirely match the true set of labels. Hamming loss is more forgiving in that it penalizes the individual labels. The Hamming loss is upperbounded by the subset zero-one loss. When normalized over samples, the Hamming loss is always between 0 and 1. In the multilabel case with binary label indicators: In multiclass classification, the Hamming loss correspond to the Hamming distance between y_true and y_pred which is equivalent to the subset zero_one_loss function. In multilabel classification, the Hamming loss is different from the subset zero-one loss. The zero-one loss considers the entire set of labels for a given sample incorrect if it does entirely match the true set of labels. Hamming loss is more forgiving in that it penalizes the individual labels. The Hamming loss is upperbounded by the subset zero-one loss. When normalized over samples, the Hamming loss is always between 0 and 1. In the multilabel case with binary label indicators:", "funcName": "hamming_loss", "allFuncParams": ["y_true", "y_pred", "labels", "sample_weight", "classes"], "funcDesc": "Compute the average Hamming loss.", "funcParamBody": "y_true : 1d array-like, or label indicator array / sparse matrix Ground truth (correct) labels. y_pred : 1d array-like, or label indicator array / sparse matrix Predicted labels, as returned by a classifier. labels : array, shape = [n_labels], optional (default=None) Integer array of labels. If not provided, labels will be inferred from y_true and y_pred. New in version 0.18. sample_weight : array-like of shape = [n_samples], optional Sample weights. New in version 0.18. classes : array, shape = [n_labels], optional Integer array of labels. Deprecated since version 0.18: This parameter has been deprecated in favor of labels in version 0.18 and will be removed in 0.20. Use labels instead.", "funcReturnBody": "loss : float or int, Return the average Hamming loss between element of y_true and y_pred ."},
{"allReturnParams": ["fbeta_score"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "fbeta_score", "allFuncParams": ["y_true", "y_pred", "beta", "labels", "pos_label", "average", "sample_weight"], "funcDesc": "Compute the F-beta score", "funcParamBody": "y_true : 1d array-like, or label indicator array / sparse matrix Ground truth (correct) target values. y_pred : 1d array-like, or label indicator array / sparse matrix Estimated targets as returned by a classifier. beta : float Weight of precision in harmonic mean. labels : list, optional The set of labels to include when average  !=  'binary' , and their order if average  is  None . Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in y_true and y_pred are used in sorted order. Changed in version 0.17: parameter labels improved for multiclass problem. pos_label : str or int, 1 by default The class to report if average='binary' and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting labels=[pos_label] and average  !=  'binary' will report scores for that label only. average : string, [None, binary (default), micro, macro, samples,                        weighted] This parameter is required for multiclass/multilabel targets. If None , the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data: 'binary' : Only report results for the class specified by pos_label . This is applicable only if targets ( y_{true,pred} ) are binary. 'micro' : Calculate metrics globally by counting the total true positives, false negatives and false positives. 'macro' : Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account. 'weighted' : Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters macro to account for label imbalance; it can result in an F-score that is not between precision and recall. 'samples' : Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score ). sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "fbeta_score : float (if average is not None) or array of float, shape =        [n_unique_labels] F-beta score of the positive class in binary classification or weighted average of the F-beta score of each class for the multiclass task."},
{"allReturnParams": ["f1_score"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "f1_score", "allFuncParams": ["y_true", "y_pred", "labels", "pos_label", "average", "sample_weight"], "funcDesc": "Compute the F1 score, also known as balanced F-score or F-measure", "funcParamBody": "y_true : 1d array-like, or label indicator array / sparse matrix Ground truth (correct) target values. y_pred : 1d array-like, or label indicator array / sparse matrix Estimated targets as returned by a classifier. labels : list, optional The set of labels to include when average  !=  'binary' , and their order if average  is  None . Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in y_true and y_pred are used in sorted order. Changed in version 0.17: parameter labels improved for multiclass problem. pos_label : str or int, 1 by default The class to report if average='binary' and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting labels=[pos_label] and average  !=  'binary' will report scores for that label only. average : string, [None, binary (default), micro, macro, samples,                        weighted] This parameter is required for multiclass/multilabel targets. If None , the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data: 'binary' : Only report results for the class specified by pos_label . This is applicable only if targets ( y_{true,pred} ) are binary. 'micro' : Calculate metrics globally by counting the total true positives, false negatives and false positives. 'macro' : Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account. 'weighted' : Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters macro to account for label imbalance; it can result in an F-score that is not between precision and recall. 'samples' : Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score ). sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "f1_score : float or array of float, shape = [n_unique_labels] F1 score of the positive class in binary classification or weighted average of the F1 scores of each class for the multiclass task."},
{"allReturnParams": ["C"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "confusion_matrix", "allFuncParams": ["y_true", "y_pred", "labels", "sample_weight"], "funcDesc": "Compute confusion matrix to evaluate the accuracy of a classification", "funcParamBody": "y_true : array, shape = [n_samples] Ground truth (correct) target values. y_pred : array, shape = [n_samples] Estimated targets as returned by a classifier. labels : array, shape = [n_classes], optional List of labels to index the matrix. This may be used to reorder or select a subset of labels. If none is given, those that appear at least once in y_true or y_pred are used in sorted order. sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "C : array, shape = [n_classes, n_classes] Confusion matrix"},
{"allReturnParams": ["kappa"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "cohen_kappa_score", "allFuncParams": ["y1", "y2", "labels", "weights", "sample_weight"], "funcDesc": "Cohens kappa: a statistic that measures inter-annotator agreement.", "funcParamBody": "y1 : array, shape = [n_samples] Labels assigned by the first annotator. y2 : array, shape = [n_samples] Labels assigned by the second annotator. The kappa statistic is symmetric, so swapping y1 and y2 doesnt change the value. labels : array, shape = [n_classes], optional List of labels to index the matrix. This may be used to select a subset of labels. If None, all labels that appear at least once in y1 or y2 are used. weights : str, optional List of weighting type to calculate the score. None means no weighted; linear means linear weighted; quadratic means quadratic weighted. sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "kappa : float The kappa statistic, which is a number between -1 and 1. The maximum value means complete agreement; zero or lower means chance agreement."},
{"allReturnParams": ["report"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "classification_report", "allFuncParams": ["y_true", "y_pred", "labels", "target_names", "sample_weight", "digits"], "funcDesc": "Build a text report showing the main classification metrics", "funcParamBody": "y_true : 1d array-like, or label indicator array / sparse matrix Ground truth (correct) target values. y_pred : 1d array-like, or label indicator array / sparse matrix Estimated targets as returned by a classifier. labels : array, shape = [n_labels] Optional list of label indices to include in the report. target_names : list of strings Optional display names matching the labels (same order). sample_weight : array-like of shape = [n_samples], optional Sample weights. digits : int Number of digits for formatting output floating point values", "funcReturnBody": "report : string Text summary of the precision, recall, F1 score for each class. The reported averages are a prevalence-weighted macro-average across classes (equivalent to precision_recall_fscore_support with average='weighted' ). Note that in binary classification, recall of the positive class is also known as sensitivity; recall of the negative class is specificity."},
{"allReturnParams": ["score"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "brier_score_loss", "allFuncParams": ["y_true", "y_prob", "sample_weight", "pos_label"], "funcDesc": "Compute the Brier score.", "funcParamBody": "y_true : array, shape (n_samples,) True targets. y_prob : array, shape (n_samples,) Probabilities of the positive class. sample_weight : array-like of shape = [n_samples], optional Sample weights. pos_label : int or str, default=None Label of the positive class. If None, the maximum label is used as positive class", "funcReturnBody": "score : float Brier score"},
{"allReturnParams": ["average_precision"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "average_precision_score", "allFuncParams": ["y_true", "y_score", "average", "sample_weight"], "funcDesc": "Compute average precision (AP) from prediction scores", "funcParamBody": "y_true : array, shape = [n_samples] or [n_samples, n_classes] True binary labels in binary label indicators. y_score : array, shape = [n_samples] or [n_samples, n_classes] Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by decision_function on some classifiers). average : string, [None, micro, macro (default), samples, weighted] If None , the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data: 'micro' : Calculate metrics globally by considering each element of the label indicator matrix as a label. 'macro' : Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account. 'weighted' : Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). 'samples' : Calculate metrics for each instance, and find their average. sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "average_precision : float"},
{"allReturnParams": ["auc"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "auc", "allFuncParams": ["x", "y", "reorder"], "funcDesc": "Compute Area Under the Curve (AUC) using the trapezoidal rule", "funcParamBody": "x : array, shape = [n] x coordinates. y : array, shape = [n] y coordinates. reorder : boolean, optional (default=False) If True, assume that the curve is ascending in the case of ties, as for an ROC curve. If the curve is non-ascending, the result will be wrong.", "funcReturnBody": "auc : float"},
{"allReturnParams": ["score"], "libName": "sklearn.metrics", "methods": [], "notes": "In binary and multiclass classification, this function is equal to the jaccard_similarity_score function. In the multilabel case with binary label indicators: In binary and multiclass classification, this function is equal to the jaccard_similarity_score function. In the multilabel case with binary label indicators:", "funcName": "accuracy_score", "allFuncParams": ["y_true", "y_pred", "normalize", "sample_weight"], "funcDesc": "Accuracy classification score.", "funcParamBody": "y_true : 1d array-like, or label indicator array / sparse matrix Ground truth (correct) labels. y_pred : 1d array-like, or label indicator array / sparse matrix Predicted labels, as returned by a classifier. normalize : bool, optional (default=True) If False , return the number of correctly classified samples. Otherwise, return the fraction of correctly classified samples. sample_weight : array-like of shape = [n_samples], optional Sample weights.", "funcReturnBody": "score : float If normalize  ==  True , return the correctly classified samples (float), else it returns the number of correctly classified samples (int). The best performance is 1 with normalize  ==  True and the number of samples with normalize  ==  False ."},
{"allReturnParams": ["scorer"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "make_scorer", "allFuncParams": ["score_func", "greater_is_better", "needs_proba", "needs_threshold", "**kwargs"], "funcDesc": "Make a scorer from a performance metric or loss function.", "funcParamBody": "score_func : callable, Score function (or loss function) with signature score_func(y,  y_pred,  **kwargs) . greater_is_better : boolean, default=True Whether score_func is a score function (default), meaning high is good, or a loss function, meaning low is good. In the latter case, the scorer object will sign-flip the outcome of the score_func. needs_proba : boolean, default=False Whether score_func requires predict_proba to get probability estimates out of a classifier. needs_threshold : boolean, default=False Whether score_func takes a continuous decision certainty. This only works for binary classification using estimators that have either a decision_function or predict_proba method. For example average_precision or the area under the roc curve can not be computed using discrete predictions alone. **kwargs : additional arguments Additional parameters to be passed to score_func.", "funcReturnBody": "scorer : callable Callable object that returns a scalar score; greater is better."},
{"allReturnParams": ["scorer"], "libName": "sklearn.metrics", "methods": [], "notes": "", "funcName": "get_scorer", "allFuncParams": ["scoring"], "funcDesc": "Get a scorer from string", "funcParamBody": "scoring : str | callable scoring method as string. If callable it is returned as is.", "funcReturnBody": "scorer : callable The scorer."},
{"allReturnParams": ["embedding"], "libName": "sklearn.manifold", "methods": [], "notes": "Spectral Embedding (Laplacian Eigenmaps) is most useful when the graph has one connected component. If there graph has many components, the first few eigenvectors will simply uncover the connected components of the graph.", "funcName": "spectral_embedding", "allFuncParams": ["adjacency", "n_components", "eigen_solver", "random_state", "eigen_tol", "norm_laplacian", "drop_first"], "funcDesc": "Project the sample on the first eigenvectors of the graph Laplacian.", "funcParamBody": "adjacency : array-like or sparse matrix, shape: (n_samples, n_samples) The adjacency matrix of the graph to embed. n_components : integer, optional, default 8 The dimension of the projection subspace. eigen_solver : {None, arpack, lobpcg, or amg}, default None The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities. random_state : int, RandomState instance or None, optional, default: None A pseudo random number generator used for the initialization of the lobpcg eigenvectors decomposition.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when solver == amg. eigen_tol : float, optional, default=0.0 Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack eigen_solver. norm_laplacian : bool, optional, default=True If True, then compute normalized Laplacian. drop_first : bool, optional, default=True Whether to drop the first eigenvector. For spectral embedding, this should be True as the first eigenvector should be constant vector for connected graph, but for spectral clustering, this should be kept as False to retain the first eigenvector.", "funcReturnBody": "embedding : array, shape=(n_samples, n_components) The reduced samples."},
{"allReturnParams": ["X", "stress", "n_iter"], "libName": "sklearn.manifold", "methods": [], "notes": "Modern Multidimensional Scaling - Theory and Applications Borg, I.; Groenen P. Springer Series in Statistics (1997) Nonmetric multidimensional scaling: a numerical method Kruskal, J. Psychometrika, 29 (1964) Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis Kruskal, J. Psychometrika, 29, (1964)", "funcName": "smacof", "allFuncParams": ["dissimilarities", "metric", "n_components", "init", "n_init", "n_jobs", "max_iter", "verbose", "eps", "random_state", "return_n_iter"], "funcDesc": "Computes multidimensional scaling using the SMACOF algorithm.", "funcParamBody": "dissimilarities : ndarray, shape (n_samples, n_samples) Pairwise dissimilarities between the points. Must be symmetric. metric : boolean, optional, default: True Compute metric or nonmetric SMACOF algorithm. n_components : int, optional, default: 2 Number of dimensions in which to immerse the dissimilarities. If an init array is provided, this option is overridden and the shape of init is used to determine the dimensionality of the embedding space. init : ndarray, shape (n_samples, n_components), optional, default: None Starting configuration of the embedding to initialize the algorithm. By default, the algorithm is initialized with a randomly chosen array. n_init : int, optional, default: 8 Number of times the SMACOF algorithm will be run with different initializations. The final results will be the best output of the runs, determined by the run with the smallest final stress. If init is provided, this option is overridden and a single run is performed. n_jobs : int, optional, default: 1 The number of jobs to use for the computation. If multiple initializations are used ( n_init ), each run of the algorithm is computed in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, ( n_cpus  +  1  +  n_jobs ) are used. Thus for n_jobs  =  -2 , all CPUs but one are used. max_iter : int, optional, default: 300 Maximum number of iterations of the SMACOF algorithm for a single run. verbose : int, optional, default: 0 Level of verbosity. eps : float, optional, default: 1e-3 Relative tolerance with respect to stress at which to declare convergence. random_state : int, RandomState instance or None, optional, default: None The generator used to initialize the centers.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . return_n_iter : bool, optional, default: False Whether or not to return the number of iterations.", "funcReturnBody": "X : ndarray, shape (n_samples, n_components) Coordinates of the points in a n_components -space. stress : float The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points). n_iter : int The number of iterations corresponding to the best stress. Returned only if return_n_iter is set to True ."},
{"allReturnParams": ["Y", "squared_error"], "libName": "sklearn.manifold", "methods": [], "notes": "", "funcName": "locally_linear_embedding", "allFuncParams": ["X", "n_neighbors", "n_components", "reg", "eigen_solver", "tol", "max_iter", "method", "hessian_tol", "modified_tol", "random_state", "n_jobs"], "funcDesc": "Perform a Locally Linear Embedding analysis on the data.", "funcParamBody": "X : {array-like, sparse matrix, BallTree, KDTree, NearestNeighbors} Sample data, shape = (n_samples, n_features), in the form of a numpy array, sparse array, precomputed tree, or NearestNeighbors object. n_neighbors : integer number of neighbors to consider for each point. n_components : integer number of coordinates for the manifold. reg : float regularization constant, multiplies the trace of the local covariance matrix of the distances. eigen_solver : string, {auto, arpack, dense} auto : algorithm will attempt to choose the best method for input data arpack :  use arnoldi iteration in shift-invert mode. For this method, M may be a dense matrix, sparse matrix, or general linear operator. Warning: ARPACK can be unstable for some problems.  It is best to try several random seeds in order to check results. dense :  use standard dense matrix operations for the eigenvalue decomposition.  For this method, M must be an array or matrix type.  This method should be avoided for large problems. tol : float, optional Tolerance for arpack method Not used if eigen_solver==dense. max_iter : integer maximum number of iterations for the arpack solver. method : {standard, hessian, modified, ltsa} standard :  use the standard locally linear embedding algorithm. see reference [R193] hessian :  use the Hessian eigenmap method.  This method requires n_neighbors > n_components * (1 + (n_components + 1) / 2. see reference [R194] modified :  use the modified locally linear embedding algorithm. see reference [R195] ltsa :  use local tangent space alignment algorithm see reference [R196] hessian_tol : float, optional Tolerance for Hessian eigenmapping method. Only used if method == hessian modified_tol : float, optional Tolerance for modified LLE method. Only used if method == modified random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when solver == arpack. n_jobs : int, optional (default = 1) The number of parallel jobs to run for neighbors search. If -1 , then the number of jobs is set to the number of CPU cores.", "funcReturnBody": "Y : array-like, shape [n_samples, n_components] Embedding vectors. squared_error : float Reconstruction error for the embedding vectors. Equivalent to norm(Y  -  W  Y,  'fro')**2 , where W are the reconstruction weights."},
{"libName": "sklearn.manifold", "methods": [{"methodName": "__init__(n_components=2, perplexity=30.0, early_exaggeration=12.0, learning_rate=200.0, n_iter=1000, n_iter_without_progress=300, min_grad_norm=1e-07, metric=euclidean, init=random, verbose=0, random_state=None, method=barnes_hut, angle=0.5)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array, shape (n_samples, n_features) or (n_samples, n_samples)   If the metric is precomputed X must be a square distance matrix. Otherwise it contains a sample per row. If the method is exact, X may be a sparse matrix of type csr, csc or coo.   y : Ignored. ", "methodDesc": "Fit X into an embedded space."}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "X_new : array, shape (n_samples, n_components)   Embedding of the training data in low-dimensional space.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit X into an embedded space and return that transformed output.", "methodParamsBody": "X : array, shape (n_samples, n_features) or (n_samples, n_samples)   If the metric is precomputed X must be a square distance matrix. Otherwise it contains a sample per row.   y : Ignored. "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["n_components", "perplexity", "early_exaggeration", "learning_rate", "n_iter", "n_iter_without_progress", "min_grad_norm", "metric", "init", "verbose", "random_state", "method", "angle"], "notes": "", "funcName": "TSNE", "allFuncAttributes": ["embedding_", "kl_divergence_", "n_iter_"], "funcDesc": "t-distributed Stochastic Neighbor Embedding.", "funcParamBody": "n_components : int, optional (default: 2) Dimension of the embedded space. perplexity : float, optional (default: 30) The perplexity is related to the number of nearest neighbors that is used in other manifold learning algorithms. Larger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. The choice is not extremely critical since t-SNE is quite insensitive to this parameter. early_exaggeration : float, optional (default: 12.0) Controls how tight natural clusters in the original space are in the embedded space and how much space will be between them. For larger values, the space between natural clusters will be larger in the embedded space. Again, the choice of this parameter is not very critical. If the cost function increases during initial optimization, the early exaggeration factor or the learning rate might be too high. learning_rate : float, optional (default: 200.0) The learning rate for t-SNE is usually in the range [10.0, 1000.0]. If the learning rate is too high, the data may look like a ball with any point approximately equidistant from its nearest neighbours. If the learning rate is too low, most points may look compressed in a dense cloud with few outliers. If the cost function gets stuck in a bad local minimum increasing the learning rate may help. n_iter : int, optional (default: 1000) Maximum number of iterations for the optimization. Should be at least 250. n_iter_without_progress : int, optional (default: 300) Maximum number of iterations without progress before we abort the optimization, used after 250 initial iterations with early exaggeration. Note that progress is only checked every 50 iterations so this value is rounded to the next multiple of 50. New in version 0.17: parameter n_iter_without_progress to control stopping criteria. min_grad_norm : float, optional (default: 1e-7) If the gradient norm is below this threshold, the optimization will be stopped. metric : string or callable, optional The metric to use when calculating distance between instances in a feature array. If metric is a string, it must be one of the options allowed by scipy.spatial.distance.pdist for its metric parameter, or a metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS. If metric is precomputed, X is assumed to be a distance matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. The default is euclidean which is interpreted as squared euclidean distance. init : string or numpy array, optional (default: random) Initialization of embedding. Possible options are random, pca, and a numpy array of shape (n_samples, n_components). PCA initialization cannot be used with precomputed distances and is usually more globally stable than random initialization. verbose : int, optional (default: 0) Verbosity level. random_state : int, RandomState instance or None, optional (default: None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .  Note that different initializations might result in different local minima of the cost function. method : string (default: barnes_hut) By default the gradient calculation algorithm uses Barnes-Hut approximation running in O(NlogN) time. method=exact will run on the slower, but exact, algorithm in O(N^2) time. The exact algorithm should be used when nearest-neighbor errors need to be better than 3%. However, the exact method cannot scale to millions of examples. New in version 0.17: Approximate optimization method via the Barnes-Hut. angle : float (default: 0.5) Only used if method=barnes_hut This is the trade-off between speed and accuracy for Barnes-Hut T-SNE. angle is the angular size (referred to as theta in [3]) of a distant node as measured from a point. If this size is below angle then it is used as a summary node of all points contained within it. This method is not very sensitive to changes in this parameter in the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasing computation time and angle greater 0.8 has quickly increasing error.", "funcAttrBody": "embedding_ : array-like, shape (n_samples, n_components) Stores the embedding vectors. kl_divergence_ : float Kullback-Leibler divergence after optimization. n_iter_ : int Number of iterations run."},
{"libName": "sklearn.manifold", "methods": [{"methodName": "__init__(n_components=2, affinity=nearest_neighbors, gamma=None, random_state=None, eigen_solver=None, n_neighbors=None, n_jobs=1)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns the instance itself.  ", "methodParams": ["X", "Y: Ignored."], "methodReturns": ["self"], "methodDesc": "Fit the model from data in X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training vector, where n_samples is the number of samples and n_features is the number of features.  If affinity is precomputed X : array-like, shape (n_samples, n_samples), Interpret X as precomputed adjacency graph computed from samples.   Y: Ignored. : "}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components) ", "methodParams": ["X", "Y: Ignored."], "methodReturns": ["X_new"], "methodDesc": "Fit the model from data in X and transform X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training vector, where n_samples is the number of samples and n_features is the number of features.  If affinity is precomputed X : array-like, shape (n_samples, n_samples), Interpret X as precomputed adjacency graph computed from samples.   Y: Ignored. : "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["n_components", "affinity", "gamma", "random_state", "eigen_solver", "n_neighbors", "n_jobs"], "notes": "", "funcName": "SpectralEmbedding", "allFuncAttributes": ["embedding_", "affinity_matrix_"], "funcDesc": "Spectral embedding for non-linear dimensionality reduction.", "funcParamBody": "n_components : integer, default: 2 The dimension of the projected subspace. affinity : string or callable, default How to construct the affinity matrix. nearest_neighbors : construct affinity matrix by knn graph rbf : construct affinity matrix by rbf kernel precomputed : interpret X as precomputed affinity matrix callable : use passed in function as affinity the function takes in data matrix (n_samples, n_features) and return affinity matrix (n_samples, n_samples). gamma : float, optional, default Kernel coefficient for rbf kernel. random_state : int, RandomState instance or None, optional, default: None A pseudo random number generator used for the initialization of the lobpcg eigenvectors.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when solver == amg. eigen_solver : {None, arpack, lobpcg, or amg} The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities. n_neighbors : int, default Number of nearest neighbors for nearest_neighbors graph building. n_jobs : int, optional (default = 1) The number of parallel jobs to run. If -1 , then the number of jobs is set to the number of CPU cores.", "funcAttrBody": "embedding_ : array, shape = (n_samples, n_components) Spectral embedding of the training matrix. affinity_matrix_ : array, shape = (n_samples, n_samples) Affinity_matrix constructed from samples or precomputed."},
{"libName": "sklearn.manifold", "methods": [{"methodName": "__init__(n_components=2, metric=True, n_init=4, max_iter=300, verbose=0, eps=0.001, n_jobs=1, random_state=None, dissimilarity=euclidean)", "methodDesc": ""}, {"methodParams": ["X", "y: Ignored.", "init"], "methodName": "fit(X, y=None, init=None)", "methodParamsBody": "X : array, shape (n_samples, n_features) or (n_samples, n_samples)   Input data. If dissimilarity=='precomputed' , the input should be the dissimilarity matrix.   y: Ignored. :  init : ndarray, shape (n_samples,), optional, default: None   Starting configuration of the embedding to initialize the SMACOF algorithm. By default, the algorithm is initialized with a randomly chosen array.  ", "methodDesc": "Computes the position of the points in the embedding space"}, {"methodParams": ["X", "y: Ignored.", "init"], "methodName": "fit_transform(X, y=None, init=None)", "methodParamsBody": "X : array, shape (n_samples, n_features) or (n_samples, n_samples)   Input data. If dissimilarity=='precomputed' , the input should be the dissimilarity matrix.   y: Ignored. :  init : ndarray, shape (n_samples,), optional, default: None   Starting configuration of the embedding to initialize the SMACOF algorithm. By default, the algorithm is initialized with a randomly chosen array.  ", "methodDesc": "Fit the data from X, and returns the embedded coordinates"}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["n_components", "metric", "n_init", "max_iter", "verbose", "eps", "n_jobs", "random_state", "dissimilarity"], "notes": "", "funcName": "MDS", "allFuncAttributes": ["embedding_", "stress_"], "funcDesc": "Multidimensional scaling", "funcParamBody": "n_components : int, optional, default: 2 Number of dimensions in which to immerse the dissimilarities. metric : boolean, optional, default: True If True , perform metric MDS; otherwise, perform nonmetric MDS. n_init : int, optional, default: 4 Number of times the SMACOF algorithm will be run with different initializations. The final results will be the best output of the runs, determined by the run with the smallest final stress. max_iter : int, optional, default: 300 Maximum number of iterations of the SMACOF algorithm for a single run. verbose : int, optional, default: 0 Level of verbosity. eps : float, optional, default: 1e-3 Relative tolerance with respect to stress at which to declare convergence. n_jobs : int, optional, default: 1 The number of jobs to use for the computation. If multiple initializations are used ( n_init ), each run of the algorithm is computed in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, ( n_cpus  +  1  +  n_jobs ) are used. Thus for n_jobs  =  -2 , all CPUs but one are used. random_state : int, RandomState instance or None, optional, default: None The generator used to initialize the centers.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . dissimilarity : euclidean | precomputed, optional, default: euclidean Dissimilarity measure to use: euclidean: Pairwise Euclidean distances between points in the dataset. precomputed: Pre-computed dissimilarities are passed directly to fit and fit_transform .", "funcAttrBody": "embedding_ : array-like, shape (n_components, n_samples) Stores the position of the dataset in the embedding space. stress_ : float The final value of the stress (sum of squared distance of the disparities and the distances for all constrained points)."},
{"funcName": "LocallyLinearEmbedding", "notes": "Because of scaling performed by this method, it is discouraged to use it together with methods that are not scale-invariant (like SVMs) Because of scaling performed by this method, it is discouraged to use it together with methods that are not scale-invariant (like SVMs)", "libName": "sklearn.manifold", "methods": [{"methodName": "__init__(n_neighbors=5, n_components=2, reg=0.001, eigen_solver=auto, tol=1e-06, max_iter=100, method=standard, hessian_tol=0.0001, modified_tol=1e-12, neighbors_algorithm=auto, random_state=None, n_jobs=1)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y: Ignored."], "methodReturns": ["self"], "methodDesc": "Compute the embedding vectors for data X", "methodParamsBody": "X : array-like of shape [n_samples, n_features]   training set.   y: Ignored. : "}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components) ", "methodParams": ["X", "y: Ignored."], "methodReturns": ["X_new"], "methodDesc": "Compute the embedding vectors for data X and transform X.", "methodParamsBody": "X : array-like of shape [n_samples, n_features]   training set.   y: Ignored. : "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array, shape = [n_samples, n_components]", "methodParams": [], "methodReturns": [], "methodDesc": "Transform new points into embedding space. Notes Because of scaling performed by this method, it is discouraged to use it together with methods that are not scale-invariant (like SVMs)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]"}], "funcDesc": "Locally Linear Embedding"},
{"funcName": "Isomap", "notes": "The cost function of an isomap embedding is E  =  frobenius_norm[K(D)  -  K(D_fit)]  /  n_samples Where D is the matrix of distances for the input data X, D_fit is the matrix of distances for the output embedding X_fit, and K is the isomap kernel: K(D)  =  -0.5  *  (I  -  1/n_samples)  *  D^2  *  (I  -  1/n_samples) The cost function of an isomap embedding is E  =  frobenius_norm[K(D)  -  K(D_fit)]  /  n_samples Where D is the matrix of distances for the input data X, D_fit is the matrix of distances for the output embedding X_fit, and K is the isomap kernel: K(D)  =  -0.5  *  (I  -  1/n_samples)  *  D^2  *  (I  -  1/n_samples)", "libName": "sklearn.manifold", "methods": [{"methodName": "__init__(n_neighbors=5, n_components=2, eigen_solver=auto, tol=0, max_iter=None, path_method=auto, neighbors_algorithm=auto, n_jobs=1)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y: Ignored."], "methodReturns": ["self"], "methodDesc": "Compute the embedding vectors for data X", "methodParamsBody": "X : {array-like, sparse matrix, BallTree, KDTree, NearestNeighbors}   Sample data, shape = (n_samples, n_features), in the form of a numpy array, precomputed tree, or NearestNeighbors object.   y: Ignored. : "}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components) ", "methodParams": ["X", "y: Ignored."], "methodReturns": ["X_new"], "methodDesc": "Fit the model from data in X and transform X.", "methodParamsBody": "X : {array-like, sparse matrix, BallTree, KDTree}   Training vector, where n_samples in the number of samples and n_features is the number of features.   y: Ignored. : "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "reconstruction_error()", "methodReturns": [], "methodDesc": "Compute the reconstruction error for the embedding. Notes The cost function of an isomap embedding is E  =  frobenius_norm[K(D)  -  K(D_fit)]  /  n_samples Where D is the matrix of distances for the input data X, D_fit is the matrix of distances for the output embedding X_fit, and K is the isomap kernel: K(D)  =  -0.5  *  (I  -  1/n_samples)  *  D^2  *  (I  -  1/n_samples)", "methodReturnsBody": "reconstruction_error : float"}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components)", "methodParams": [], "methodReturns": [], "methodDesc": "Transform X. This is implemented by linking the points X into the graph of geodesic distances of the training data. First the n_neighbors nearest neighbors of X are found in the training data, and from these the shortest geodesic distances from each point in X to each point in the training data are computed in order to construct the kernel. The embedding of X is the projection of this kernel onto the embedding vectors of the training set.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)"}], "funcDesc": "Isomap Embedding"},
{"allReturnParams": ["coef", "n_iters"], "libName": "sklearn.linear_model", "methods": [], "notes": "Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang, Matching pursuits with time-frequency dictionaries, IEEE Transactions on Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415. ( http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf ) This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008. http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf", "funcName": "orthogonal_mp_gram", "allFuncParams": ["Gram", "Xy", "n_nonzero_coefs", "tol", "norms_squared", "copy_Gram", "copy_Xy", "return_path", "return_n_iter"], "funcDesc": "Gram Orthogonal Matching Pursuit (OMP)", "funcParamBody": "Gram : array, shape (n_features, n_features) Gram matrix of the input data: X.T * X Xy : array, shape (n_features,) or (n_features, n_targets) Input targets multiplied by X: X.T * y n_nonzero_coefs : int Desired number of non-zero entries in the solution. If None (by default) this value is set to 10% of n_features. tol : float Maximum norm of the residual. If not None, overrides n_nonzero_coefs. norms_squared : array-like, shape (n_targets,) Squared L2 norms of the lines of y. Required if tol is not None. copy_Gram : bool, optional Whether the gram matrix must be copied by the algorithm. A false value is only helpful if it is already Fortran-ordered, otherwise a copy is made anyway. copy_Xy : bool, optional Whether the covariance vector Xy must be copied by the algorithm. If False, it may be overwritten. return_path : bool, optional. Default: False Whether to return every value of the nonzero coefficients along the forward path. Useful for cross-validation. return_n_iter : bool, optional default False Whether or not to return the number of iterations.", "funcReturnBody": "coef : array, shape (n_features,) or (n_features, n_targets) Coefficients of the OMP solution. If return_path=True , this contains the whole coefficient path. In this case its shape is (n_features, n_features) or (n_features, n_targets, n_features) and iterating over the last axis yields coefficients in increasing order of active features. n_iters : array-like or int Number of active features across every target. Returned only if return_n_iter is set to True."},
{"allReturnParams": ["coef", "n_iters"], "libName": "sklearn.linear_model", "methods": [], "notes": "Orthogonal matching pursuit was introduced in S. Mallat, Z. Zhang, Matching pursuits with time-frequency dictionaries, IEEE Transactions on Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415. ( http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf ) This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008. http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf", "funcName": "orthogonal_mp", "allFuncParams": ["X", "y", "n_nonzero_coefs", "tol", "precompute", "copy_X", "return_path", "return_n_iter"], "funcDesc": "Orthogonal Matching Pursuit (OMP)", "funcParamBody": "X : array, shape (n_samples, n_features) Input data. Columns are assumed to have unit norm. y : array, shape (n_samples,) or (n_samples, n_targets) Input targets n_nonzero_coefs : int Desired number of non-zero entries in the solution. If None (by default) this value is set to 10% of n_features. tol : float Maximum norm of the residual. If not None, overrides n_nonzero_coefs. precompute : {True, False, auto}, Whether to perform precomputations. Improves performance when n_targets or n_samples is very large. copy_X : bool, optional Whether the design matrix X must be copied by the algorithm. A false value is only helpful if X is already Fortran-ordered, otherwise a copy is made anyway. return_path : bool, optional. Default: False Whether to return every value of the nonzero coefficients along the forward path. Useful for cross-validation. return_n_iter : bool, optional default False Whether or not to return the number of iterations.", "funcReturnBody": "coef : array, shape (n_features,) or (n_features, n_targets) Coefficients of the OMP solution. If return_path=True , this contains the whole coefficient path. In this case its shape is (n_features, n_features) or (n_features, n_targets, n_features) and iterating over the last axis yields coefficients in increasing order of active features. n_iters : array-like or int Number of active features across every target. Returned only if return_n_iter is set to True."},
{"allReturnParams": ["coefs", "Cs", "n_iter"], "libName": "sklearn.linear_model", "methods": [], "notes": "You might get slightly different results with the solver liblinear than with the others since this uses LIBLINEAR which penalizes the intercept.", "funcName": "logistic_regression_path", "allFuncParams": ["X", "y", "pos_class", "Cs", "fit_intercept", "max_iter", "tol", "verbose", "solver", "coef", "class_weight", "dual", "penalty", "intercept_scaling", "multi_class", "random_state", "check_input", "max_squared_sum", "sample_weight"], "funcDesc": "Compute a Logistic Regression model for a list of regularization parameters.", "funcParamBody": "X : array-like or sparse matrix, shape (n_samples, n_features) Input data. y : array-like, shape (n_samples,) Input data, target values. pos_class : int, None The class with respect to which we perform a one-vs-all fit. If None, then it is assumed that the given problem is binary. Cs : int | array-like, shape (n_cs,) List of values for the regularization parameter or integer specifying the number of regularization parameters that should be used. In this case, the parameters will be chosen in a logarithmic scale between 1e-4 and 1e4. fit_intercept : bool Whether to fit an intercept for the model. In this case the shape of the returned array is (n_cs, n_features + 1). max_iter : int Maximum number of iterations for the solver. tol : float Stopping criterion. For the newton-cg and lbfgs solvers, the iteration will stop when max{|g_i  |  i  =  1,  ...,  n}  <=  tol g_i is the i-th component of the gradient. verbose : int For the liblinear and lbfgs solvers set verbose to any positive number for verbosity. solver : {lbfgs, newton-cg, liblinear, sag, saga} Numerical solver to use. coef : array-like, shape (n_features,), default None Initialization value for coefficients of logistic regression. Useless for liblinear solver. class_weight : dict or balanced, optional Weights associated with classes in the form {class_label:  weight} . If not given, all classes are supposed to have weight one. The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples  /  (n_classes  *  np.bincount(y)) . Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. dual : bool Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features. penalty : str, l1 or l2 Used to specify the norm used in the penalization. The newton-cg, sag and lbfgs solvers support only l2 penalties. intercept_scaling : float, default 1. Useful only when the solver liblinear is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a synthetic feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling  *  synthetic_feature_weight . Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. multi_class : str, {ovr, multinomial} Multiclass option can be either ovr or multinomial. If the option chosen is ovr, then a binary problem is fit for each label. Else the loss minimised is the multinomial loss fit across the entire probability distribution. Works only for the lbfgs and newton-cg solvers. random_state : int, RandomState instance or None, optional, default None The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when solver == sag or liblinear. check_input : bool, default True If False, the input arrays X and y will not be checked. max_squared_sum : float, default None Maximum squared sum of X over samples. Used only in SAG solver. If None, it will be computed, going through all the samples. The value should be precomputed to speed up cross validation. sample_weight : array-like, shape(n_samples,) optional Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.", "funcReturnBody": "coefs : ndarray, shape (n_cs, n_features) or (n_cs, n_features + 1) List of coefficients for the Logistic Regression model. If fit_intercept is set to True then the second dimension will be n_features + 1, where the last item represents the intercept. Cs : ndarray Grid of Cs used for cross-validation. n_iter : array, shape (n_cs,) Actual number of iteration for each Cs."},
{"allReturnParams": ["alphas_grid"], "libName": "sklearn.linear_model", "methods": [], "notes": "", "funcName": "lasso_stability_path", "allFuncParams": ["X"], "funcDesc": "DEPRECATED: The function lasso_stability_path is deprecated in 0.19 and will be removed in 0.21.", "funcParamBody": "X : array-like, shape = [n_samples, n_features] training data. y :  array-like, shape = [n_samples] target values. scaling :  float, optional, default=0.5 The alpha parameter in the stability selection article used to randomly scale the features. Should be between 0 and 1. random_state :  int, RandomState instance or None, optional, default=None The generator used to randomize the design.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_resampling :  int, optional, default=200 Number of randomized models. n_grid :  int, optional, default=100 Number of grid points. The path is linearly reinterpolated on a grid between 0 and 1 before computing the scores. sample_fraction :  float, optional, default=0.75 The fraction of samples to be used in each randomized design. Should be between 0 and 1. If 1, all samples are used. eps :  float, optional Smallest value of alpha / alpha_max considered n_jobs :  integer, optional Number of CPUs to use during the resampling. If -1, use all the CPUs verbose :  boolean or integer, optional Sets the verbosity amount", "funcReturnBody": "alphas_grid : array, shape ~ [n_grid] The grid points between 0 and 1: alpha/alpha_max scores_path :  array, shape = [n_features, n_grid] The scores for each feature along the path."},
{"allReturnParams": ["alphas", "coefs", "dual_gaps", "n_iters"], "libName": "sklearn.linear_model", "methods": [], "notes": "For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py . To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. Note that in certain cases, the Lars solver may be significantly faster to implement this functionality. In particular, linear interpolation can be used to retrieve model coefficients between the values output by lars_path Comparing lasso_path and lars_path with interpolation: For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py . To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. Note that in certain cases, the Lars solver may be significantly faster to implement this functionality. In particular, linear interpolation can be used to retrieve model coefficients between the values output by lars_path Comparing lasso_path and lars_path with interpolation:", "funcName": "lasso_path", "allFuncParams": ["X", "y", "eps", "n_alphas", "alphas", "precompute", "Xy", "copy_X", "coef_init", "verbose", "return_n_iter", "positive", "**params"], "funcDesc": "Compute Lasso path with coordinate descent", "funcParamBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features) Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output then X y : ndarray, shape (n_samples,), or (n_samples, n_outputs) Target values eps : float, optional Length of the path. eps=1e-3 means that alpha_min  /  alpha_max  =  1e-3 n_alphas : int, optional Number of alphas along the regularization path alphas : ndarray, optional List of alphas where to compute the models. If None alphas are set automatically precompute : True | False | auto | array-like Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. Xy : array-like, optional Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed. copy_X : boolean, optional, default True If True , X will be copied; else, it may be overwritten. coef_init : array, shape (n_features, ) | None The initial values of the coefficients. verbose : bool or integer Amount of verbosity. return_n_iter : bool whether to return the number of iterations or not. positive : bool, default False If set to True, forces coefficients to be positive. (Only allowed when y.ndim  ==  1 ). **params : kwargs keyword arguments passed to the coordinate descent solver.", "funcReturnBody": "alphas : array, shape (n_alphas,) The alphas along the path where models are computed. coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas) Coefficients along the path. dual_gaps : array, shape (n_alphas,) The dual gaps at the end of the optimization for each alpha. n_iters : array-like, shape (n_alphas,) The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha."},
{"allReturnParams": ["alphas", "active", "coefs", "n_iter"], "libName": "sklearn.linear_model", "methods": [], "notes": "", "funcName": "lars_path", "allFuncParams": ["X", "y", "Xy", "Gram", "max_iter", "alpha_min", "method", "copy_X", "eps", "copy_Gram", "verbose", "return_path", "return_n_iter", "positive"], "funcDesc": "Compute Least Angle Regression or Lasso path using LARS algorithm [1]", "funcParamBody": "X : array, shape: (n_samples, n_features) Input data. y : array, shape: (n_samples) Input targets. Xy : array-like, shape (n_samples,) or (n_samples, n_targets),             optional Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed. Gram : None, auto, array, shape: (n_features, n_features), optional Precomputed Gram matrix (X * X), if 'auto' , the Gram matrix is precomputed from the given X, if there are more samples than features. max_iter : integer, optional (default=500) Maximum number of iterations to perform, set to infinity for no limit. alpha_min : float, optional (default=0) Minimum correlation along the path. It corresponds to the regularization parameter alpha parameter in the Lasso. method : {lar, lasso}, optional (default=lar) Specifies the returned model. Select 'lar' for Least Angle Regression, 'lasso' for the Lasso. copy_X : bool, optional (default=True) If False , X is overwritten. eps : float, optional (default=``np.finfo(np.float).eps``) The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. copy_Gram : bool, optional (default=True) If False , Gram is overwritten. verbose : int (default=0) Controls output verbosity. return_path : bool, optional (default=True) If return_path==True returns the entire path, else returns only the last point of the path. return_n_iter : bool, optional (default=False) Whether to return the number of iterations. positive : boolean (default=False) Restrict coefficients to be >= 0. When using this option together with method lasso the model coefficients will not converge to the ordinary-least-squares solution for small values of alpha (neither will they when using method lar ..). Only coefficients up to the smallest alpha value ( alphas_[alphas_  >  0.].min() when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent lasso_path function.", "funcReturnBody": "alphas : array, shape: [n_alphas + 1] Maximum of covariances (in absolute value) at each iteration. n_alphas is either max_iter , n_features or the number of nodes in the path with alpha  >=  alpha_min , whichever is smaller. active : array, shape [n_alphas] Indices of active variables at the end of the path. coefs : array, shape (n_features, n_alphas + 1) Coefficients along the path n_iter : int Number of iterations run. Returned only if return_n_iter is set to True."},
{"allReturnParams": ["alphas", "coefs", "dual_gaps", "n_iters"], "libName": "sklearn.linear_model", "methods": [], "notes": "For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py . For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py .", "funcName": "enet_path", "allFuncParams": ["X", "y", "l1_ratio", "eps", "n_alphas", "alphas", "precompute", "Xy", "copy_X", "coef_init", "verbose", "return_n_iter", "positive", "check_input", "**params"], "funcDesc": "Compute elastic net path with coordinate descent", "funcParamBody": "X : {array-like}, shape (n_samples, n_features) Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output then X y : ndarray, shape (n_samples,) or (n_samples, n_outputs) Target values l1_ratio : float, optional float between 0 and 1 passed to elastic net (scaling between l1 and l2 penalties). l1_ratio=1 corresponds to the Lasso eps : float Length of the path. eps=1e-3 means that alpha_min  /  alpha_max  =  1e-3 n_alphas : int, optional Number of alphas along the regularization path alphas : ndarray, optional List of alphas where to compute the models. If None alphas are set automatically precompute : True | False | auto | array-like Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. Xy : array-like, optional Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed. copy_X : boolean, optional, default True If True , X will be copied; else, it may be overwritten. coef_init : array, shape (n_features, ) | None The initial values of the coefficients. verbose : bool or integer Amount of verbosity. return_n_iter : bool whether to return the number of iterations or not. positive : bool, default False If set to True, forces coefficients to be positive. (Only allowed when y.ndim  ==  1 ). check_input : bool, default True Skip input validation checks, including the Gram matrix when provided assuming there are handled by the caller when check_input=False. **params : kwargs keyword arguments passed to the coordinate descent solver.", "funcReturnBody": "alphas : array, shape (n_alphas,) The alphas along the path where models are computed. coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas) Coefficients along the path. dual_gaps : array, shape (n_alphas,) The dual gaps at the end of the optimization for each alpha. n_iters : array-like, shape (n_alphas,) The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha. (Is returned when return_n_iter is set to True)."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(fit_intercept=True, copy_X=True, max_subpopulation=10000.0, n_subsamples=None, max_iter=300, tol=0.001, random_state=None, n_jobs=1, verbose=False)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit linear model.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training data   y : numpy array of shape [n_samples]   Target values  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["fit_intercept", "copy_X", "max_subpopulation", "n_subsamples", "max_iter", "tol", "random_state", "n_jobs", "verbose"], "notes": "", "funcName": "TheilSenRegressor", "allFuncAttributes": ["coef_", "intercept_", "breakdown_", "n_iter_", "n_subpopulation_"], "funcDesc": "Theil-Sen Estimator: robust multivariate regression model.", "funcParamBody": "fit_intercept : boolean, optional, default True Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations. copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. max_subpopulation : int, optional, default 1e4 Instead of computing with a set of cardinality n choose k, where n is the number of samples and k is the number of subsamples (at least number of features), consider only a stochastic subpopulation of a given maximal size if n choose k is larger than max_subpopulation. For other than small problem sizes this parameter will determine memory usage and runtime if n_subsamples is not changed. n_subsamples : int, optional, default None Number of samples to calculate the parameters. This is at least the number of features (plus 1 if fit_intercept=True) and the number of samples as a maximum. A lower number leads to a higher breakdown point and a low efficiency while a high number leads to a low breakdown point and a high efficiency. If None, take the minimum number of subsamples leading to maximal robustness. If n_subsamples is set to n_samples, Theil-Sen is identical to least squares. max_iter : int, optional, default 300 Maximum number of iterations for the calculation of spatial median. tol : float, optional, default 1.e-3 Tolerance when calculating spatial median. random_state : int, RandomState instance or None, optional, default None A random number generator instance to define the state of the random permutations generator.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_jobs : integer, optional, default 1 Number of CPUs to use during the cross validation. If -1 , use all the CPUs. verbose : boolean, optional, default False Verbose mode when fitting the model.", "funcAttrBody": "coef_ : array, shape = (n_features) Coefficients of the regression model (median of distribution). intercept_ : float Estimated intercept of regression model. breakdown_ : float Approximated breakdown point. n_iter_ : int Number of iterations needed for the spatial median. n_subpopulation_ : int Number of combinations taken into account from n choose k, where n is the number of samples and k is the number of subsamples."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(loss=squared_loss, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, shuffle=True, verbose=0, epsilon=0.1, random_state=None, learning_rate=invscaling, eta0=0.01, power_t=0.25, warm_start=False, average=False, n_iter=None)", "methodDesc": ""}, {"methodName": "densify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to dense array format. Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.", "methodReturnsBody": "self : estimator"}, {"methodName": "fit(X, y, coef_init=None, intercept_init=None, sample_weight=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y", "coef_init", "intercept_init", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit linear model with Stochastic Gradient Descent.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Training data   y : numpy array, shape (n_samples,)   Target values   coef_init : array, shape (n_features,)   The initial coefficients to warm-start the optimization.   intercept_init : array, shape (1,)   The initial intercept to warm-start the optimization.   sample_weight : array-like, shape (n_samples,), optional   Weights applied to individual samples (1. for unweighted).  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "partial_fit(X, y, sample_weight=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit linear model with Stochastic Gradient Descent.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Subset of training data   y : numpy array of shape (n_samples,)   Subset of target values   sample_weight : array-like, shape (n_samples,), optional   Weights applied to individual samples. If not provided, uniform weights are assumed.  "}, {"methodName": "predict(X)", "methodReturnsBody": "array, shape (n_samples,) :   Predicted target values per element in X.  ", "methodParams": ["X"], "methodReturns": ["array, shape (n_samples,)"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features) "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "sparsify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to sparse format. Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation. The intercept_ member is not converted. Notes For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "methodReturnsBody": "self : estimator"}], "allFuncParams": ["loss", "penalty", "alpha", "l1_ratio", "fit_intercept", "max_iter", "tol", "shuffle", "verbose", "epsilon", "random_state", "learning_rate", "eta0", "power_t", "warm_start", "average", "n_iter"], "notes": "For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify. For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "funcName": "SGDRegressor", "allFuncAttributes": ["coef_", "intercept_", "average_coef_", "average_intercept_", "n_iter_"], "funcDesc": "Linear model fitted by minimizing a regularized empirical loss with SGD", "funcParamBody": "loss : str, default: squared_loss The loss function to be used. The possible values are squared_loss, huber, epsilon_insensitive, or squared_epsilon_insensitive The squared_loss refers to the ordinary least squares fit. huber modifies squared_loss to focus less on getting outliers correct by switching from squared to linear loss past a distance of epsilon. epsilon_insensitive ignores errors less than epsilon and is linear past that; this is the loss function used in SVR. squared_epsilon_insensitive is the same but becomes squared loss past a tolerance of epsilon. penalty : str, none, l2, l1, or elasticnet The penalty (aka regularization term) to be used. Defaults to l2 which is the standard regularizer for linear SVM models. l1 and elasticnet might bring sparsity to the model (feature selection) not achievable with l2. alpha : float Constant that multiplies the regularization term. Defaults to 0.0001 Also used to compute learning_rate when set to optimal. l1_ratio : float The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15. fit_intercept : bool Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True. max_iter : int, optional The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit . Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None. New in version 0.19. tol : float or None, optional The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21. New in version 0.19. shuffle : bool, optional Whether or not the training data should be shuffled after each epoch. Defaults to True. verbose : integer, optional The verbosity level. epsilon : float Epsilon in the epsilon-insensitive loss functions; only if loss is huber, epsilon_insensitive, or squared_epsilon_insensitive. For huber, determines the threshold at which it becomes less important to get the prediction exactly right. For epsilon-insensitive, any differences between the current prediction and the correct label are ignored if they are less than this threshold. random_state : int, RandomState instance or None, optional (default=None) The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . learning_rate : string, optional The learning rate schedule: constant: eta = eta0 optimal: eta = 1.0 / (alpha * (t + t0)) [default] invscaling: eta = eta0 / pow(t, power_t) where t0 is chosen by a heuristic proposed by Leon Bottou. eta0 : double, optional The initial learning rate [default 0.01]. power_t : double, optional The exponent for inverse scaling learning rate [default 0.25]. warm_start : bool, optional When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. average : bool or int, optional When set to True, computes the averaged SGD weights and stores the result in the coef_ attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples. n_iter : int, optional The number of passes over the training data (aka epochs). Defaults to None. Deprecated, will be removed in 0.21. Changed in version 0.19: Deprecated", "funcAttrBody": "coef_ : array, shape (n_features,) Weights assigned to the features. intercept_ : array, shape (1,) The intercept term. average_coef_ : array, shape (n_features,) Averaged weights assigned to the features. average_intercept_ : array, shape (1,) The averaged intercept term. n_iter_ : int The actual number of iterations to reach the stopping criterion."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(loss=hinge, penalty=l2, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=None, tol=None, shuffle=True, verbose=0, epsilon=0.1, n_jobs=1, random_state=None, learning_rate=optimal, eta0=0.0, power_t=0.5, class_weight=None, warm_start=False, average=False, n_iter=None)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :   Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted.  ", "methodParams": ["X"], "methodReturns": ["array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"], "methodDesc": "Predict confidence scores for samples. The confidence score for a sample is the signed distance of that sample to the hyperplane.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "densify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to dense array format. Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.", "methodReturnsBody": "self : estimator"}, {"methodName": "fit(X, y, coef_init=None, intercept_init=None, sample_weight=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y", "coef_init", "intercept_init", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit linear model with Stochastic Gradient Descent.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Training data   y : numpy array, shape (n_samples,)   Target values   coef_init : array, shape (n_classes, n_features)   The initial coefficients to warm-start the optimization.   intercept_init : array, shape (n_classes,)   The initial intercept to warm-start the optimization.   sample_weight : array-like, shape (n_samples,), optional   Weights applied to individual samples. If not provided, uniform weights are assumed. These weights will be multiplied with class_weight (passed through the constructor) if class_weight is specified  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "partial_fit(X, y, classes=None, sample_weight=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y", "classes", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit linear model with Stochastic Gradient Descent.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Subset of the training data   y : numpy array, shape (n_samples,)   Subset of the target values   classes : array, shape (n_classes,)   Classes across all calls to partial_fit. Can be obtained by via np.unique(y_all) , where y_all is the target vector of the entire dataset. This argument is required for the first call to partial_fit and can be omitted in the subsequent calls. Note that y doesnt need to contain all labels in classes .   sample_weight : array-like, shape (n_samples,), optional   Weights applied to individual samples. If not provided, uniform weights are assumed.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples]   Predicted class label per sample.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict class labels for samples in X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "sparsify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to sparse format. Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation. The intercept_ member is not converted. Notes For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "methodReturnsBody": "self : estimator"}], "allFuncParams": ["loss", "penalty", "alpha", "l1_ratio", "fit_intercept", "max_iter", "tol", "shuffle", "verbose", "epsilon", "n_jobs", "random_state", "learning_rate", "eta0", "power_t", "class_weight", "warm_start", "average", "n_iter"], "notes": "For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify. For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "funcName": "SGDClassifier", "allFuncAttributes": ["coef_", "intercept_", "n_iter_", "loss_function_"], "funcDesc": "Linear classifiers (SVM, logistic regression, a.o.) with SGD training.", "funcParamBody": "loss : str, default: hinge The loss function to be used. Defaults to hinge, which gives a linear SVM. The possible options are hinge, log, modified_huber, squared_hinge, perceptron, or a regression loss: squared_loss, huber, epsilon_insensitive, or squared_epsilon_insensitive. The log loss gives logistic regression, a probabilistic classifier. modified_huber is another smooth loss that brings tolerance to outliers as well as probability estimates. squared_hinge is like hinge but is quadratically penalized. perceptron is the linear loss used by the perceptron algorithm. The other losses are designed for regression but can be useful in classification as well; see SGDRegressor for a description. penalty : str, none, l2, l1, or elasticnet The penalty (aka regularization term) to be used. Defaults to l2 which is the standard regularizer for linear SVM models. l1 and elasticnet might bring sparsity to the model (feature selection) not achievable with l2. alpha : float Constant that multiplies the regularization term. Defaults to 0.0001 Also used to compute learning_rate when set to optimal. l1_ratio : float The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15. fit_intercept : bool Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True. max_iter : int, optional The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit . Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None. New in version 0.19. tol : float or None, optional The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21. New in version 0.19. shuffle : bool, optional Whether or not the training data should be shuffled after each epoch. Defaults to True. verbose : integer, optional The verbosity level epsilon : float Epsilon in the epsilon-insensitive loss functions; only if loss is huber, epsilon_insensitive, or squared_epsilon_insensitive. For huber, determines the threshold at which it becomes less important to get the prediction exactly right. For epsilon-insensitive, any differences between the current prediction and the correct label are ignored if they are less than this threshold. n_jobs : integer, optional The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. -1 means all CPUs. Defaults to 1. random_state : int, RandomState instance or None, optional (default=None) The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . learning_rate : string, optional The learning rate schedule: constant: eta = eta0 optimal: eta = 1.0 / (alpha * (t + t0)) [default] invscaling: eta = eta0 / pow(t, power_t) where t0 is chosen by a heuristic proposed by Leon Bottou. eta0 : double The initial learning rate for the constant or invscaling schedules. The default value is 0.0 as eta0 is not used by the default schedule optimal. power_t : double The exponent for inverse scaling learning rate [default 0.5]. class_weight : dict, {class_label: weight} or balanced or None, optional Preset for the class_weight fit parameter. Weights associated with classes. If not given, all classes are supposed to have weight one. The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples  /  (n_classes  *  np.bincount(y)) warm_start : bool, optional When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. average : bool or int, optional When set to True, computes the averaged SGD weights and stores the result in the coef_ attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples. n_iter : int, optional The number of passes over the training data (aka epochs). Defaults to None. Deprecated, will be removed in 0.21. Changed in version 0.19: Deprecated", "funcAttrBody": "coef_ : array, shape (1, n_features) if n_classes == 2 else (n_classes,            n_features) Weights assigned to the features. intercept_ : array, shape (1,) if n_classes == 2 else (n_classes,) Constants in decision function. n_iter_ : int The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit. loss_function_ : concrete LossFunction"},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, gcv_mode=None, store_cv_values=False)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : Returns self. ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit Ridge regression model", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training data   y : array-like, shape = [n_samples] or [n_samples, n_targets]   Target values. Will be cast to Xs dtype if necessary   sample_weight : float or array-like of shape [n_samples]   Sample weight  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["alphas", "fit_intercept", "normalize", "scoring", "cv", "gcv_mode", "store_cv_values"], "notes": "", "funcName": "RidgeCV", "allFuncAttributes": ["cv_values_", "coef_", "intercept_", "alpha_"], "funcDesc": "Ridge regression with built-in cross-validation.", "funcParamBody": "alphas : numpy array of shape [n_alphas] Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to C^-1 in other linear models such as LogisticRegression or LinearSVC. fit_intercept : boolean Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . scoring : string, callable or None, optional, default: None A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator,  X,  y) . cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the efficient Leave-One-Out cross-validation integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, if y is binary or multiclass, sklearn.model_selection.StratifiedKFold is used, else, sklearn.model_selection.KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. gcv_mode : {None, auto, svd, eigen}, optional Flag indicating which strategy to use when performing Generalized Cross-Validation. Options are: 'auto'  :  use  svd  if  n_samples  >  n_features  or  when  X  is  a  sparse matrix ,  otherwise  use  eigen 'svd'  :  force  computation  via  singular  value  decomposition  of  X ( does  not  work  for  sparse  matrices ) 'eigen'  :  force  computation  via  eigendecomposition  of  X ^ T  X The auto mode is the default and is intended to pick the cheaper option of the two depending upon the shape and format of the training data. store_cv_values : boolean, default=False Flag indicating if the cross-validation values corresponding to each alpha should be stored in the cv_values_ attribute (see below). This flag is only compatible with cv=None (i.e. using Generalized Cross-Validation).", "funcAttrBody": "cv_values_ : array, shape = [n_samples, n_alphas] or         shape = [n_samples, n_targets, n_alphas], optional Cross-validation values for each alpha (if store_cv_values=True and cv=None ). After fit() has been called, this attribute will         contain the mean squared errors (by default) or the values of the {loss,score}_func function (if provided in the constructor). coef_ : array, shape = [n_features] or [n_targets, n_features] Weight vector(s). intercept_ : float | array, shape = (n_targets,) Independent term in decision function. Set to 0.0 if fit_intercept  =  False . alpha_ : float Estimated regularization parameter."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(alphas=(0.1, 1.0, 10.0), fit_intercept=True, normalize=False, scoring=None, cv=None, class_weight=None)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :   Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted.  ", "methodParams": ["X"], "methodReturns": ["array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"], "methodDesc": "Predict confidence scores for samples. The confidence score for a sample is the signed distance of that sample to the hyperplane.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the ridge classifier.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training vectors, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape (n_samples,)   Target values. Will be cast to Xs dtype if necessary   sample_weight : float or numpy array of shape (n_samples,)   Sample weight.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples]   Predicted class label per sample.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict class labels for samples in X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["alphas", "fit_intercept", "normalize", "scoring", "cv", "class_weight"], "notes": "For multi-class classification, n_class classifiers are trained in a one-versus-all approach. Concretely, this is implemented by taking advantage of the multi-variate response support in Ridge.", "funcName": "RidgeClassifierCV", "allFuncAttributes": ["cv_values_", "`cv=None`). After `fit()` has been called, this attribute will contain     the mean squared errors (by default) or the values of the     `{loss,score}_func` function (if provided in the constructor).", "coef_", "intercept_", "alpha_"], "funcDesc": "Ridge classifier with built-in cross-validation.", "funcParamBody": "alphas : numpy array of shape [n_alphas] Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to C^-1 in other linear models such as LogisticRegression or LinearSVC. fit_intercept : boolean Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . scoring : string, callable or None, optional, default: None A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator,  X,  y) . cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the efficient Leave-One-Out cross-validation integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. Refer User Guide for the various cross-validation strategies that can be used here. class_weight : dict or balanced, optional Weights associated with classes in the form {class_label:  weight} . If not given, all classes are supposed to have weight one. The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples  /  (n_classes  *  np.bincount(y))", "funcAttrBody": "cv_values_ : array, shape = [n_samples, n_alphas] or     shape = [n_samples, n_responses, n_alphas], optional Cross-validation values for each alpha (if store_cv_values=True and `cv=None`). After `fit()` has been called, this attribute will contain     the mean squared errors (by default) or the values of the     `{loss,score}_func` function (if provided in the constructor). : coef_ : array, shape = [n_features] or [n_targets, n_features] Weight vector(s). intercept_ : float | array, shape = (n_targets,) Independent term in decision function. Set to 0.0 if fit_intercept  =  False . alpha_ : float Estimated regularization parameter"},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, class_weight=None, solver=auto, random_state=None)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :   Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted.  ", "methodParams": ["X"], "methodReturns": ["array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"], "methodDesc": "Predict confidence scores for samples. The confidence score for a sample is the signed distance of that sample to the hyperplane.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit Ridge regression model.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples,n_features]   Training data   y : array-like, shape = [n_samples]   Target values   sample_weight : float or numpy array of shape (n_samples,)   Sample weight.   New in version 0.17: sample_weight support to Classifier.   "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples]   Predicted class label per sample.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict class labels for samples in X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["alpha", "fit_intercept", "normalize", "copy_X", "max_iter", "tol", "class_weight", "solver", "random_state"], "notes": "For multi-class classification, n_class classifiers are trained in a one-versus-all approach. Concretely, this is implemented by taking advantage of the multi-variate response support in Ridge. For multi-class classification, n_class classifiers are trained in a one-versus-all approach. Concretely, this is implemented by taking advantage of the multi-variate response support in Ridge.", "funcName": "RidgeClassifier", "allFuncAttributes": ["coef_", "intercept_", "n_iter_"], "funcDesc": "Classifier using Ridge regression.", "funcParamBody": "alpha : float Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to C^-1 in other linear models such as LogisticRegression or LinearSVC. fit_intercept : boolean Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. max_iter : int, optional Maximum number of iterations for conjugate gradient solver. The default value is determined by scipy.sparse.linalg. tol : float Precision of the solution. class_weight : dict or balanced, optional Weights associated with classes in the form {class_label:  weight} . If not given, all classes are supposed to have weight one. The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples  /  (n_classes  *  np.bincount(y)) solver : {auto, svd, cholesky, lsqr, sparse_cg, sag, saga} Solver to use in the computational routines: auto chooses the solver automatically based on the type of data. svd uses a Singular Value Decomposition of X to compute the Ridge coefficients. More stable for singular matrices than cholesky. cholesky uses the standard scipy.linalg.solve function to obtain a closed-form solution. sparse_cg uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than cholesky for large-scale data (possibility to set tol and max_iter ). lsqr uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest but may not be available in old scipy versions. It also uses an iterative procedure. sag uses a Stochastic Average Gradient descent, and saga uses its unbiased and more flexible version named SAGA. Both methods use an iterative procedure, and are often faster than other solvers when both n_samples and n_features are large. Note that sag and saga fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing. New in version 0.17: Stochastic Average Gradient descent solver. New in version 0.19: SAGA solver. random_state : int, RandomState instance or None, optional, default None The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when solver == sag.", "funcAttrBody": "coef_ : array, shape (n_features,) or (n_classes, n_features) Weight vector(s). intercept_ : float | array, shape = (n_targets,) Independent term in decision function. Set to 0.0 if fit_intercept  =  False . n_iter_ : array or None, shape (n_targets,) Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return None."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=None, tol=0.001, solver=auto, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit Ridge regression model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training data   y : array-like, shape = [n_samples] or [n_samples, n_targets]   Target values   sample_weight : float or numpy array of shape [n_samples]   Individual weights for each sample  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["alpha", "fit_intercept", "normalize", "copy_X", "max_iter", "tol", "solver", "random_state"], "notes": "", "funcName": "Ridge", "allFuncAttributes": ["coef_", "intercept_", "n_iter_"], "funcDesc": "Linear least squares with l2 regularization.", "funcParamBody": "alpha : {float, array-like}, shape (n_targets) Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to C^-1 in other linear models such as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number. fit_intercept : boolean Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. max_iter : int, optional Maximum number of iterations for conjugate gradient solver. For sparse_cg and lsqr solvers, the default value is determined by scipy.sparse.linalg. For sag solver, the default value is 1000. tol : float Precision of the solution. solver : {auto, svd, cholesky, lsqr, sparse_cg, sag, saga} Solver to use in the computational routines: auto chooses the solver automatically based on the type of data. svd uses a Singular Value Decomposition of X to compute the Ridge coefficients. More stable for singular matrices than cholesky. cholesky uses the standard scipy.linalg.solve function to obtain a closed-form solution. sparse_cg uses the conjugate gradient solver as found in scipy.sparse.linalg.cg. As an iterative algorithm, this solver is more appropriate than cholesky for large-scale data (possibility to set tol and max_iter ). lsqr uses the dedicated regularized least-squares routine scipy.sparse.linalg.lsqr. It is the fastest but may not be available in old scipy versions. It also uses an iterative procedure. sag uses a Stochastic Average Gradient descent, and saga uses its improved, unbiased version named SAGA. Both methods also use an iterative procedure, and are often faster than other solvers when both n_samples and n_features are large. Note that sag and saga fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing. All last five solvers support both dense and sparse data. However, only sag and saga supports sparse input when fit_intercept is True. New in version 0.17: Stochastic Average Gradient descent solver. New in version 0.19: SAGA solver. random_state : int, RandomState instance or None, optional, default None The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when solver == sag. New in version 0.17: random_state to support Stochastic Average Gradient.", "funcAttrBody": "coef_ : array, shape (n_features,) or (n_targets, n_features) Weight vector(s). intercept_ : float | array, shape = (n_targets,) Independent term in decision function. Set to 0.0 if fit_intercept  =  False . n_iter_ : array or None, shape (n_targets,) Actual number of iterations for each target. Available only for sag and lsqr solvers. Other solvers will return None. New in version 0.17."},
{"funcName": "RANSACRegressor", "notes": "", "libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(base_estimator=None, min_samples=None, residual_threshold=None, is_data_valid=None, is_model_valid=None, max_trials=100, max_skips=inf, stop_n_inliers=inf, stop_score=inf, stop_probability=0.99, residual_metric=None, loss=absolute_loss, random_state=None)", "methodDesc": ""}, {"methodParams": ["X", "y", "sample_weight"], "methodName": "fit(X, y, sample_weight=None)", "methodParamsBody": "X : array-like or sparse matrix, shape [n_samples, n_features]   Training data.   y : array-like, shape = [n_samples] or [n_samples, n_targets]   Target values.   sample_weight : array-like, shape = [n_samples]   Individual weights for each sample raises error if sample_weight is passed and base_estimator fit method does not support it.  ", "methodDesc": "Fit estimator using RANSAC algorithm."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array, shape = [n_samples] or [n_samples, n_targets]   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict using the estimated model. This is a wrapper for estimator_.predict(X) .", "methodParamsBody": "X : numpy array of shape [n_samples, n_features] "}, {"methodName": "score(X, y)", "methodReturnsBody": "z : float   Score of the prediction.  ", "methodParams": ["X", "y"], "methodReturns": ["z"], "methodDesc": "Returns the score of the prediction. This is a wrapper for estimator_.score(X, y) .", "methodParamsBody": "X : numpy array or sparse matrix of shape [n_samples, n_features]   Training data.   y : array, shape = [n_samples] or [n_samples, n_targets]   Target values.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "RANSAC (RANdom SAmple Consensus) algorithm."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(penalty=None, alpha=0.0001, fit_intercept=True, max_iter=None, tol=None, shuffle=True, verbose=0, eta0=1.0, n_jobs=1, random_state=0, class_weight=None, warm_start=False, n_iter=None)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :   Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted.  ", "methodParams": ["X"], "methodReturns": ["array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"], "methodDesc": "Predict confidence scores for samples. The confidence score for a sample is the signed distance of that sample to the hyperplane.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "densify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to dense array format. Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.", "methodReturnsBody": "self : estimator"}, {"methodName": "fit(X, y, coef_init=None, intercept_init=None, sample_weight=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y", "coef_init", "intercept_init", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit linear model with Stochastic Gradient Descent.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Training data   y : numpy array, shape (n_samples,)   Target values   coef_init : array, shape (n_classes, n_features)   The initial coefficients to warm-start the optimization.   intercept_init : array, shape (n_classes,)   The initial intercept to warm-start the optimization.   sample_weight : array-like, shape (n_samples,), optional   Weights applied to individual samples. If not provided, uniform weights are assumed. These weights will be multiplied with class_weight (passed through the constructor) if class_weight is specified  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "partial_fit(X, y, classes=None, sample_weight=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y", "classes", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit linear model with Stochastic Gradient Descent.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Subset of the training data   y : numpy array, shape (n_samples,)   Subset of the target values   classes : array, shape (n_classes,)   Classes across all calls to partial_fit. Can be obtained by via np.unique(y_all) , where y_all is the target vector of the entire dataset. This argument is required for the first call to partial_fit and can be omitted in the subsequent calls. Note that y doesnt need to contain all labels in classes .   sample_weight : array-like, shape (n_samples,), optional   Weights applied to individual samples. If not provided, uniform weights are assumed.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples]   Predicted class label per sample.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict class labels for samples in X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "sparsify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to sparse format. Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation. The intercept_ member is not converted. Notes For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "methodReturnsBody": "self : estimator"}], "allFuncParams": ["penalty", "alpha", "fit_intercept", "max_iter", "tol", "shuffle", "verbose", "eta0", "n_jobs", "random_state", "class_weight", "warm_start", "n_iter"], "notes": "Perceptron and SGDClassifier share the same underlying implementation. In fact, Perceptron() is equivalent to SGDClassifier(loss=perceptron, eta0=1, learning_rate=constant, penalty=None) . https://en.wikipedia.org/wiki/Perceptron and references therein. For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify. Perceptron and SGDClassifier share the same underlying implementation. In fact, Perceptron() is equivalent to SGDClassifier(loss=perceptron, eta0=1, learning_rate=constant, penalty=None) . https://en.wikipedia.org/wiki/Perceptron and references therein. For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "funcName": "Perceptron", "allFuncAttributes": ["coef_", "intercept_", "n_iter_"], "funcDesc": "Read more in the ", "funcParamBody": "penalty : None, l2 or l1 or elasticnet The penalty (aka regularization term) to be used. Defaults to None. alpha : float Constant that multiplies the regularization term if regularization is used. Defaults to 0.0001 fit_intercept : bool Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True. max_iter : int, optional The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit . Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None. New in version 0.19. tol : float or None, optional The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21. New in version 0.19. shuffle : bool, optional, default True Whether or not the training data should be shuffled after each epoch. verbose : integer, optional The verbosity level eta0 : double Constant by which the updates are multiplied. Defaults to 1. n_jobs : integer, optional The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. -1 means all CPUs. Defaults to 1. random_state : int, RandomState instance or None, optional, default None The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . class_weight : dict, {class_label: weight} or balanced or None, optional Preset for the class_weight fit parameter. Weights associated with classes. If not given, all classes are supposed to have weight one. The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples  /  (n_classes  *  np.bincount(y)) warm_start : bool, optional When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. n_iter : int, optional The number of passes over the training data (aka epochs). Defaults to None. Deprecated, will be removed in 0.21. Changed in version 0.19: Deprecated", "funcAttrBody": "coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features] Weights assigned to the features. intercept_ : array, shape = [1] if n_classes == 2 else [n_classes] Constants in decision function. n_iter_ : int The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(C=1.0, fit_intercept=True, max_iter=None, tol=None, shuffle=True, verbose=0, loss=epsilon_insensitive, epsilon=0.1, random_state=None, warm_start=False, average=False, n_iter=None)", "methodDesc": ""}, {"methodName": "densify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to dense array format. Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.", "methodReturnsBody": "self : estimator"}, {"methodName": "fit(X, y, coef_init=None, intercept_init=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y", "coef_init", "intercept_init"], "methodReturns": ["self"], "methodDesc": "Fit linear model with Passive Aggressive algorithm.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training data   y : numpy array of shape [n_samples]   Target values   coef_init : array, shape = [n_features]   The initial coefficients to warm-start the optimization.   intercept_init : array, shape = [1]   The initial intercept to warm-start the optimization.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "partial_fit(X, y)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit linear model with Passive Aggressive algorithm.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Subset of training data   y : numpy array of shape [n_samples]   Subset of target values  "}, {"methodName": "predict(X)", "methodReturnsBody": "array, shape (n_samples,) :   Predicted target values per element in X.  ", "methodParams": ["X"], "methodReturns": ["array, shape (n_samples,)"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features) "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "sparsify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to sparse format. Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation. The intercept_ member is not converted. Notes For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "methodReturnsBody": "self : estimator"}], "allFuncParams": ["C", "fit_intercept", "max_iter", "tol", "shuffle", "verbose", "loss", "epsilon", "random_state", "warm_start", "average", "n_iter"], "notes": "For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "funcName": "PassiveAggressiveRegressor", "allFuncAttributes": ["coef_", "intercept_", "n_iter_"], "funcDesc": "Passive Aggressive Regressor", "funcParamBody": "C : float Maximum step size (regularization). Defaults to 1.0. fit_intercept : bool Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. Defaults to True. max_iter : int, optional The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit . Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None. New in version 0.19. tol : float or None, optional The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21. New in version 0.19. shuffle : bool, default=True Whether or not the training data should be shuffled after each epoch. verbose : integer, optional The verbosity level loss : string, optional The loss function to be used: epsilon_insensitive: equivalent to PA-I in the reference paper. squared_epsilon_insensitive: equivalent to PA-II in the reference paper. epsilon : float If the difference between the current prediction and the correct label is below this threshold, the model is not updated. random_state : int, RandomState instance or None, optional, default=None The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . warm_start : bool, optional When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. average : bool or int, optional When set to True, computes the averaged SGD weights and stores the result in the coef_ attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples. New in version 0.19: parameter average to use weights averaging in SGD n_iter : int, optional The number of passes over the training data (aka epochs). Defaults to None. Deprecated, will be removed in 0.21. Changed in version 0.19: Deprecated", "funcAttrBody": "coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features] Weights assigned to the features. intercept_ : array, shape = [1] if n_classes == 2 else [n_classes] Constants in decision function. n_iter_ : int The actual number of iterations to reach the stopping criterion."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(C=1.0, fit_intercept=True, max_iter=None, tol=None, shuffle=True, verbose=0, loss=hinge, n_jobs=1, random_state=None, warm_start=False, class_weight=None, average=False, n_iter=None)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :   Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted.  ", "methodParams": ["X"], "methodReturns": ["array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"], "methodDesc": "Predict confidence scores for samples. The confidence score for a sample is the signed distance of that sample to the hyperplane.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "densify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to dense array format. Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.", "methodReturnsBody": "self : estimator"}, {"methodName": "fit(X, y, coef_init=None, intercept_init=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y", "coef_init", "intercept_init"], "methodReturns": ["self"], "methodDesc": "Fit linear model with Passive Aggressive algorithm.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training data   y : numpy array of shape [n_samples]   Target values   coef_init : array, shape = [n_classes,n_features]   The initial coefficients to warm-start the optimization.   intercept_init : array, shape = [n_classes]   The initial intercept to warm-start the optimization.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "partial_fit(X, y, classes=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y", "classes"], "methodReturns": ["self"], "methodDesc": "Fit linear model with Passive Aggressive algorithm.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Subset of the training data   y : numpy array of shape [n_samples]   Subset of the target values   classes : array, shape = [n_classes]   Classes across all calls to partial_fit. Can be obtained by via np.unique(y_all) , where y_all is the target vector of the entire dataset. This argument is required for the first call to partial_fit and can be omitted in the subsequent calls. Note that y doesnt need to contain all labels in classes .  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples]   Predicted class label per sample.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict class labels for samples in X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "sparsify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to sparse format. Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation. The intercept_ member is not converted. Notes For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "methodReturnsBody": "self : estimator"}], "allFuncParams": ["C", "fit_intercept", "max_iter", "tol", "shuffle", "verbose", "loss", "n_jobs", "random_state", "warm_start", "class_weight", "average", "n_iter"], "notes": "For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify. For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "funcName": "PassiveAggressiveClassifier", "allFuncAttributes": ["coef_", "intercept_", "n_iter_"], "funcDesc": "Passive Aggressive Classifier", "funcParamBody": "C : float Maximum step size (regularization). Defaults to 1.0. fit_intercept : bool, default=False Whether the intercept should be estimated or not. If False, the data is assumed to be already centered. max_iter : int, optional The maximum number of passes over the training data (aka epochs). It only impacts the behavior in the fit method, and not the partial_fit . Defaults to 5. Defaults to 1000 from 0.21, or if tol is not None. New in version 0.19. tol : float or None, optional The stopping criterion. If it is not None, the iterations will stop when (loss > previous_loss - tol). Defaults to None. Defaults to 1e-3 from 0.21. New in version 0.19. shuffle : bool, default=True Whether or not the training data should be shuffled after each epoch. verbose : integer, optional The verbosity level loss : string, optional The loss function to be used: hinge: equivalent to PA-I in the reference paper. squared_hinge: equivalent to PA-II in the reference paper. n_jobs : integer, optional The number of CPUs to use to do the OVA (One Versus All, for multi-class problems) computation. -1 means all CPUs. Defaults to 1. random_state : int, RandomState instance or None, optional, default=None The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . warm_start : bool, optional When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. class_weight : dict, {class_label: weight} or balanced or None, optional Preset for the class_weight fit parameter. Weights associated with classes. If not given, all classes are supposed to have weight one. The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples  /  (n_classes  *  np.bincount(y)) New in version 0.17: parameter class_weight to automatically weight samples. average : bool or int, optional When set to True, computes the averaged SGD weights and stores the result in the coef_ attribute. If set to an int greater than 1, averaging will begin once the total number of samples seen reaches average. So average=10 will begin averaging after seeing 10 samples. New in version 0.19: parameter average to use weights averaging in SGD n_iter : int, optional The number of passes over the training data (aka epochs). Defaults to None. Deprecated, will be removed in 0.21. Changed in version 0.19: Deprecated", "funcAttrBody": "coef_ : array, shape = [1, n_features] if n_classes == 2 else [n_classes,            n_features] Weights assigned to the features. intercept_ : array, shape = [1] if n_classes == 2 else [n_classes] Constants in decision function. n_iter_ : int The actual number of iterations to reach the stopping criterion. For multiclass fits, it is the maximum over every binary fit."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(copy=True, fit_intercept=True, normalize=True, max_iter=None, cv=None, n_jobs=1, verbose=False)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : object   returns an instance of self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model using X, y as training data.", "methodParamsBody": "X : array-like, shape [n_samples, n_features]   Training data.   y : array-like, shape [n_samples]   Target values. Will be cast to Xs dtype if necessary  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["copy", "fit_intercept", "normalize", "max_iter", "cv", "n_jobs", "verbose"], "notes": "", "funcName": "OrthogonalMatchingPursuitCV", "allFuncAttributes": ["intercept_", "coef_", "n_nonzero_coefs_", "n_iter_"], "funcDesc": "Cross-validated Orthogonal Matching Pursuit model (OMP)", "funcParamBody": "copy : bool, optional Whether the design matrix X must be copied by the algorithm. A false value is only helpful if X is already Fortran-ordered, otherwise a copy is made anyway. fit_intercept : boolean, optional whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). normalize : boolean, optional, default True This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . max_iter : integer, optional Maximum numbers of iterations to perform, therefore maximum features to include. 10% of n_features but at least 5 if available. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. n_jobs : integer, optional Number of CPUs to use during the cross validation. If -1 , use all the CPUs verbose : boolean or integer, optional Sets the verbosity amount", "funcAttrBody": "intercept_ : float or array, shape (n_targets,) Independent term in decision function. coef_ : array, shape (n_features,) or (n_targets, n_features) Parameter vector (w in the problem formulation). n_nonzero_coefs_ : int Estimated number of non-zero coefficients giving the best mean squared error over the cross-validation folds. n_iter_ : int or array-like Number of active features across every target for the model refit with the best hyperparameters got by cross-validating across all folds."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(n_nonzero_coefs=None, tol=None, fit_intercept=True, normalize=True, precompute=auto)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : object   returns an instance of self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model using X, y as training data.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data.   y : array-like, shape (n_samples,) or (n_samples, n_targets)   Target values. Will be cast to Xs dtype if necessary  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["n_nonzero_coefs", "tol", "fit_intercept", "normalize", "precompute"], "notes": "Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang, Matching pursuits with time-frequency dictionaries, IEEE Transactions on Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415. ( http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf ) This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008. http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang, Matching pursuits with time-frequency dictionaries, IEEE Transactions on Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415. ( http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf ) This implementation is based on Rubinstein, R., Zibulevsky, M. and Elad, M., Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit Technical Report - CS Technion, April 2008. http://www.cs.technion.ac.il/~ronrubin/Publications/KSVD-OMP-v2.pdf", "funcName": "OrthogonalMatchingPursuit", "allFuncAttributes": ["coef_", "intercept_", "n_iter_"], "funcDesc": "Orthogonal Matching Pursuit model (OMP)", "funcParamBody": "n_nonzero_coefs : int, optional Desired number of non-zero entries in the solution. If None (by default) this value is set to 10% of n_features. tol : float, optional Maximum norm of the residual. If not None, overrides n_nonzero_coefs. fit_intercept : boolean, optional whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). normalize : boolean, optional, default True This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . precompute : {True, False, auto}, default auto Whether to use a precomputed Gram and Xy matrix to speed up calculations. Improves performance when n_targets or n_samples is very large. Note that if you already have such matrices, you can pass them directly to the fit method.", "funcAttrBody": "coef_ : array, shape (n_features,) or (n_targets, n_features) parameter vector (w in the formula) intercept_ : float or array, shape (n_targets,) independent term in decision function. n_iter_ : int or array-like Number of active features across every target."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=1, random_state=None, selection=cyclic)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : {array-like}, shape (n_samples, n_features)   Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output, X can be sparse.   y : array-like, shape (n_samples,) or (n_samples, n_targets)   Target values  ", "methodDesc": "Fit linear model with coordinate descent Fit is on grid of alphas and best alpha estimated by cross-validation."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["l1_ratio", "eps", "n_alphas", "alphas", "fit_intercept", "normalize", "max_iter", "tol", "cv", "copy_X", "verbose", "n_jobs", "random_state", "selection"], "notes": "The algorithm used to fit the model is coordinate descent. To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py .", "funcName": "MultiTaskElasticNetCV", "allFuncAttributes": ["intercept_", "coef_", "alpha_", "mse_path_", "alphas_", "l1_ratio_", "n_iter_"], "funcDesc": "Multi-task L1/L2 ElasticNet with built-in cross-validation.", "funcParamBody": "l1_ratio : float or array of floats The ElasticNet mixing parameter, with 0 < l1_ratio <= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For 0  <  l1_ratio  <  1 , the penalty is a combination of L1/L2 and L2. This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in [.1,  .5,  .7, .9,  .95,  .99,  1] eps : float, optional Length of the path. eps=1e-3 means that alpha_min  /  alpha_max  =  1e-3 . n_alphas : int, optional Number of alphas along the regularization path alphas : array-like, optional List of alphas where to compute the models. If not provided, set automatically. fit_intercept : boolean whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . max_iter : int, optional The maximum number of iterations tol : float, optional The tolerance for the optimization: if the updates are smaller than tol , the optimization code checks the dual gap for optimality and continues until it is smaller than tol . cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. copy_X : boolean, optional, default True If True , X will be copied; else, it may be overwritten. verbose : bool or integer Amount of verbosity. n_jobs : integer, optional Number of CPUs to use during the cross validation. If -1 , use all the CPUs. Note that this is used only if multiple values for l1_ratio are given. random_state : int, RandomState instance or None, optional, default None The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when selection == random. selection : str, default cyclic If set to random, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to random) often leads to significantly faster convergence especially when tol is higher than 1e-4.", "funcAttrBody": "intercept_ : array, shape (n_tasks,) Independent term in decision function. coef_ : array, shape (n_tasks, n_features) Parameter vector (W in the cost function formula). Note that coef_ stores the transpose of W , W.T . alpha_ : float The amount of penalization chosen by cross validation mse_path_ : array, shape (n_alphas, n_folds) or                 (n_l1_ratio, n_alphas, n_folds) mean square error for the test set on each fold, varying alpha alphas_ : numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas) The grid of alphas used for fitting, for each l1_ratio l1_ratio_ : float best l1_ratio obtained by cross-validation. n_iter_ : int number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=1, random_state=None, selection=cyclic)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : {array-like}, shape (n_samples, n_features)   Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output, X can be sparse.   y : array-like, shape (n_samples,) or (n_samples, n_targets)   Target values  ", "methodDesc": "Fit linear model with coordinate descent Fit is on grid of alphas and best alpha estimated by cross-validation."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["eps", "n_alphas", "alphas", "fit_intercept", "normalize", "max_iter", "tol", "copy_X", "cv", "verbose", "n_jobs", "random_state", "selection"], "notes": "The algorithm used to fit the model is coordinate descent. To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py . To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. Note that in certain cases, the Lars solver may be significantly faster to implement this functionality. In particular, linear interpolation can be used to retrieve model coefficients between the values output by lars_path Comparing lasso_path and lars_path with interpolation:", "funcName": "MultiTaskLassoCV", "allFuncAttributes": ["intercept_", "coef_", "alpha_", "mse_path_", "alphas_", "n_iter_"], "funcDesc": "Multi-task L1/L2 Lasso with built-in cross-validation.", "funcParamBody": "eps : float, optional Length of the path. eps=1e-3 means that alpha_min  /  alpha_max  =  1e-3 . n_alphas : int, optional Number of alphas along the regularization path alphas : array-like, optional List of alphas where to compute the models. If not provided, set automatically. fit_intercept : boolean whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . max_iter : int, optional The maximum number of iterations. tol : float, optional The tolerance for the optimization: if the updates are smaller than tol , the optimization code checks the dual gap for optimality and continues until it is smaller than tol . copy_X : boolean, optional, default True If True , X will be copied; else, it may be overwritten. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. verbose : bool or integer Amount of verbosity. n_jobs : integer, optional Number of CPUs to use during the cross validation. If -1 , use all the CPUs. Note that this is used only if multiple values for l1_ratio are given. random_state : int, RandomState instance or None, optional, default None The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when selection == random selection : str, default cyclic If set to random, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to random) often leads to significantly faster convergence especially when tol is higher than 1e-4.", "funcAttrBody": "intercept_ : array, shape (n_tasks,) Independent term in decision function. coef_ : array, shape (n_tasks, n_features) Parameter vector (W in the cost function formula). Note that coef_ stores the transpose of W , W.T . alpha_ : float The amount of penalization chosen by cross validation mse_path_ : array, shape (n_alphas, n_folds) mean square error for the test set on each fold, varying alpha alphas_ : numpy array, shape (n_alphas,) The grid of alphas used for fitting. n_iter_ : int number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection=cyclic)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Data   y : ndarray, shape (n_samples, n_tasks)   Target. Will be cast to Xs dtype if necessary  ", "methodDesc": "Fit MultiTaskElasticNet model with coordinate descent Notes Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary. To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute=auto, Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)", "methodReturnsBody": "alphas : array, shape (n_alphas,)   The alphas along the path where models are computed.   coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)   Coefficients along the path.   dual_gaps : array, shape (n_alphas,)   The dual gaps at the end of the optimization for each alpha.   n_iters : array-like, shape (n_alphas,)   The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha. (Is returned when return_n_iter is set to True).  ", "methodParams": ["X", "y", "l1_ratio", "eps", "n_alphas", "alphas", "precompute", "Xy", "copy_X", "coef_init", "verbose", "return_n_iter", "positive", "check_input", "**params"], "methodReturns": ["alphas", "coefs", "dual_gaps", "n_iters"], "methodDesc": "Compute elastic net path with coordinate descent The elastic net optimization function varies for mono and multi-outputs. For mono-output tasks it is: For multi-output tasks it is: Where: i.e. the sum of norm of each row. Read more in the User Guide . Notes For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py .", "methodParamsBody": "X : {array-like}, shape (n_samples, n_features)   Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output then X can be sparse.   y : ndarray, shape (n_samples,) or (n_samples, n_outputs)   Target values   l1_ratio : float, optional   float between 0 and 1 passed to elastic net (scaling between l1 and l2 penalties). l1_ratio=1 corresponds to the Lasso   eps : float   Length of the path. eps=1e-3 means that alpha_min  /  alpha_max  =  1e-3   n_alphas : int, optional   Number of alphas along the regularization path   alphas : ndarray, optional   List of alphas where to compute the models. If None alphas are set automatically   precompute : True | False | auto | array-like   Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument.   Xy : array-like, optional   Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.   copy_X : boolean, optional, default True   If True , X will be copied; else, it may be overwritten.   coef_init : array, shape (n_features, ) | None   The initial values of the coefficients.   verbose : bool or integer   Amount of verbosity.   return_n_iter : bool   whether to return the number of iterations or not.   positive : bool, default False   If set to True, forces coefficients to be positive. (Only allowed when y.ndim  ==  1 ).   check_input : bool, default True   Skip input validation checks, including the Gram matrix when provided assuming there are handled by the caller when check_input=False.   **params : kwargs   keyword arguments passed to the coordinate descent solver.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["alpha", "l1_ratio", "fit_intercept", "normalize", "copy_X", "max_iter", "tol", "warm_start", "random_state", "selection"], "notes": "The algorithm used to fit the model is coordinate descent. To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary. To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format. For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py .", "funcName": "MultiTaskElasticNet", "allFuncAttributes": ["intercept_", "coef_", "n_iter_"], "funcDesc": "Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer", "funcParamBody": "alpha : float, optional Constant that multiplies the L1/L2 term. Defaults to 1.0 l1_ratio : float The ElasticNet mixing parameter, with 0 < l1_ratio <= 1. For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 it is an L2 penalty. For 0  <  l1_ratio  <  1 , the penalty is a combination of L1/L2 and L2. fit_intercept : boolean whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . copy_X : boolean, optional, default True If True , X will be copied; else, it may be overwritten. max_iter : int, optional The maximum number of iterations tol : float, optional The tolerance for the optimization: if the updates are smaller than tol , the optimization code checks the dual gap for optimality and continues until it is smaller than tol . warm_start : bool, optional When set to True , reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. random_state : int, RandomState instance or None, optional, default None The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when selection == random. selection : str, default cyclic If set to random, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to random) often leads to significantly faster convergence especially when tol is higher than 1e-4.", "funcAttrBody": "intercept_ : array, shape (n_tasks,) Independent term in decision function. coef_ : array, shape (n_tasks, n_features) Parameter vector (W in the cost function formula). If a 1D y is         passed in at fit (non multi-task usage), coef_ is then a 1D array. Note that coef_ stores the transpose of W , W.T . n_iter_ : int number of iterations run by the coordinate descent solver to reach the specified tolerance."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(alpha=1.0, fit_intercept=True, normalize=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, random_state=None, selection=cyclic)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Data   y : ndarray, shape (n_samples, n_tasks)   Target. Will be cast to Xs dtype if necessary  ", "methodDesc": "Fit MultiTaskElasticNet model with coordinate descent Notes Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary. To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "path(X, y, l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, precompute=auto, Xy=None, copy_X=True, coef_init=None, verbose=False, return_n_iter=False, positive=False, check_input=True, **params)", "methodReturnsBody": "alphas : array, shape (n_alphas,)   The alphas along the path where models are computed.   coefs : array, shape (n_features, n_alphas) or             (n_outputs, n_features, n_alphas)   Coefficients along the path.   dual_gaps : array, shape (n_alphas,)   The dual gaps at the end of the optimization for each alpha.   n_iters : array-like, shape (n_alphas,)   The number of iterations taken by the coordinate descent optimizer to reach the specified tolerance for each alpha. (Is returned when return_n_iter is set to True).  ", "methodParams": ["X", "y", "l1_ratio", "eps", "n_alphas", "alphas", "precompute", "Xy", "copy_X", "coef_init", "verbose", "return_n_iter", "positive", "check_input", "**params"], "methodReturns": ["alphas", "coefs", "dual_gaps", "n_iters"], "methodDesc": "Compute elastic net path with coordinate descent The elastic net optimization function varies for mono and multi-outputs. For mono-output tasks it is: For multi-output tasks it is: Where: i.e. the sum of norm of each row. Read more in the User Guide . Notes For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py .", "methodParamsBody": "X : {array-like}, shape (n_samples, n_features)   Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output then X can be sparse.   y : ndarray, shape (n_samples,) or (n_samples, n_outputs)   Target values   l1_ratio : float, optional   float between 0 and 1 passed to elastic net (scaling between l1 and l2 penalties). l1_ratio=1 corresponds to the Lasso   eps : float   Length of the path. eps=1e-3 means that alpha_min  /  alpha_max  =  1e-3   n_alphas : int, optional   Number of alphas along the regularization path   alphas : ndarray, optional   List of alphas where to compute the models. If None alphas are set automatically   precompute : True | False | auto | array-like   Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument.   Xy : array-like, optional   Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.   copy_X : boolean, optional, default True   If True , X will be copied; else, it may be overwritten.   coef_init : array, shape (n_features, ) | None   The initial values of the coefficients.   verbose : bool or integer   Amount of verbosity.   return_n_iter : bool   whether to return the number of iterations or not.   positive : bool, default False   If set to True, forces coefficients to be positive. (Only allowed when y.ndim  ==  1 ).   check_input : bool, default True   Skip input validation checks, including the Gram matrix when provided assuming there are handled by the caller when check_input=False.   **params : kwargs   keyword arguments passed to the coordinate descent solver.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["alpha", "fit_intercept", "normalize", "copy_X", "max_iter", "tol", "warm_start", "random_state", "selection"], "notes": "The algorithm used to fit the model is coordinate descent. To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary. To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format. For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py . The algorithm used to fit the model is coordinate descent. To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary. To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format. For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py .", "funcName": "MultiTaskLasso", "allFuncAttributes": ["coef_", "intercept_", "n_iter_"], "funcDesc": "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer", "funcParamBody": "alpha : float, optional Constant that multiplies the L1/L2 term. Defaults to 1.0 fit_intercept : boolean whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . copy_X : boolean, optional, default True If True , X will be copied; else, it may be overwritten. max_iter : int, optional The maximum number of iterations tol : float, optional The tolerance for the optimization: if the updates are smaller than tol , the optimization code checks the dual gap for optimality and continues until it is smaller than tol . warm_start : bool, optional When set to True , reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. random_state : int, RandomState instance or None, optional, default None The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when selection == random. selection : str, default cyclic If set to random, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to random) often leads to significantly faster convergence especially when tol is higher than 1e-4", "funcAttrBody": "coef_ : array, shape (n_tasks, n_features) Parameter vector (W in the cost function formula). Note that coef_ stores the transpose of W , W.T . intercept_ : array, shape (n_tasks,) independent term in decision function. n_iter_ : int number of iterations run by the coordinate descent solver to reach the specified tolerance."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(Cs=10, fit_intercept=True, cv=None, dual=False, penalty=l2, scoring=None, solver=lbfgs, tol=0.0001, max_iter=100, class_weight=None, n_jobs=1, verbose=0, refit=True, intercept_scaling=1.0, multi_class=ovr, random_state=None)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :   Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted.  ", "methodParams": ["X"], "methodReturns": ["array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"], "methodDesc": "Predict confidence scores for samples. The confidence score for a sample is the signed distance of that sample to the hyperplane.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "densify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to dense array format. Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.", "methodReturnsBody": "self : estimator"}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the model according to the given training data.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Training vector, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape (n_samples,)   Target vector relative to X.   sample_weight : array-like, shape (n_samples,) optional   Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples]   Predicted class label per sample.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict class labels for samples in X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Samples.  "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "T : array-like, shape = [n_samples, n_classes]   Returns the log-probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ .  ", "methodParams": ["X"], "methodReturns": ["T"], "methodDesc": "Log of probability estimates. The returned estimates for all classes are ordered by the label of classes.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "T : array-like, shape = [n_samples, n_classes]   Returns the probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ .  ", "methodParams": ["X"], "methodReturns": ["T"], "methodDesc": "Probability estimates. The returned estimates for all classes are ordered by the label of classes. For a multi_class problem, if multi_class is set to be multinomial the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "sparsify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to sparse format. Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation. The intercept_ member is not converted. Notes For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "methodReturnsBody": "self : estimator"}], "allFuncParams": ["Cs", "fit_intercept", "cv", "dual", "penalty", "scoring", "solver", "tol", "max_iter", "class_weight", "n_jobs", "verbose", "refit", "intercept_scaling", "multi_class", "random_state"], "notes": "For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "funcName": "LogisticRegressionCV", "allFuncAttributes": ["coef_", "intercept_", "Cs_", "coefs_paths_", "scores_", "C_", "n_iter_"], "funcDesc": "Logistic Regression CV (aka logit, MaxEnt) classifier.", "funcParamBody": "Cs : list of floats | int Each of the values in Cs describes the inverse of regularization strength. If Cs is as an int, then a grid of Cs values are chosen in a logarithmic scale between 1e-4 and 1e4. Like in support vector machines, smaller values specify stronger regularization. fit_intercept : bool, default: True Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function. cv : integer or cross-validation generator The default cross-validation generator used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. See the module sklearn.model_selection module for the list of possible cross-validation objects. dual : bool Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features. penalty : str, l1 or l2 Used to specify the norm used in the penalization. The newton-cg, sag and lbfgs solvers support only l2 penalties. scoring : string, callable, or None A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator,  X,  y) . For a list of scoring functions that can be used, look at sklearn.metrics . The default scoring option used is accuracy. solver : {newton-cg, lbfgs, liblinear, sag, saga}, default: lbfgs Algorithm to use in the optimization problem. For small datasets, liblinear is a good choice, whereas sag and saga are faster for large ones. For multiclass problems, only newton-cg, sag, saga and lbfgs handle multinomial loss; liblinear is limited to one-versus-rest schemes. newton-cg, lbfgs and sag only handle L2 penalty, whereas liblinear and saga handle L1 penalty. liblinear might be slower in LogisticRegressionCV because it does not handle warm-starting. Note that sag and saga fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing. New in version 0.17: Stochastic Average Gradient descent solver. New in version 0.19: SAGA solver. tol : float, optional Tolerance for stopping criteria. max_iter : int, optional Maximum number of iterations of the optimization algorithm. class_weight : dict or balanced, optional Weights associated with classes in the form {class_label:  weight} . If not given, all classes are supposed to have weight one. The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples  /  (n_classes  *  np.bincount(y)) . Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. New in version 0.17: class_weight == balanced n_jobs : int, optional Number of CPU cores used during the cross-validation loop. If given a value of -1, all cores are used. verbose : int For the liblinear, sag and lbfgs solvers set verbose to any positive number for verbosity. refit : bool If set to True, the scores are averaged across all folds, and the coefs and the C that corresponds to the best score is taken, and a final refit is done using these parameters. Otherwise the coefs, intercepts and C that correspond to the best scores across folds are averaged. intercept_scaling : float, default 1. Useful only when the solver liblinear is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a synthetic feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling  *  synthetic_feature_weight . Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. multi_class : str, {ovr, multinomial} Multiclass option can be either ovr or multinomial. If the option chosen is ovr, then a binary problem is fit for each label. Else the loss minimised is the multinomial loss fit across the entire probability distribution. Works only for the newton-cg, sag, saga and lbfgs solver. New in version 0.18: Stochastic Average Gradient descent solver for multinomial case. random_state : int, RandomState instance or None, optional, default None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcAttrBody": "coef_ : array, shape (1, n_features) or (n_classes, n_features) Coefficient of the features in the decision function. coef_ is of shape (1, n_features) when the given problem is binary. intercept_ : array, shape (1,) or (n_classes,) Intercept (a.k.a. bias) added to the decision function. If fit_intercept is set to False, the intercept is set to zero. intercept_ is of shape(1,) when the problem is binary. Cs_ : array Array of C i.e. inverse of regularization parameter values used for cross-validation. coefs_paths_ : array, shape (n_folds,  len(Cs_),  n_features) or (n_folds,  len(Cs_),  n_features  +  1) dict with classes as the keys, and the path of coefficients obtained during cross-validating across each fold and then across each Cs after doing an OvR for the corresponding class as values. If the multi_class option is set to multinomial, then the coefs_paths are the coefficients corresponding to each class. Each dict value has shape (n_folds,  len(Cs_),  n_features) or (n_folds,  len(Cs_),  n_features  +  1) depending on whether the intercept is fit or not. scores_ : dict dict with classes as the keys, and the values as the grid of scores obtained during cross-validating each fold, after doing an OvR for the corresponding class. If the multi_class option given is multinomial then the same scores are repeated across all classes, since this is the multinomial class. Each dict value has shape (n_folds, len(Cs)) C_ : array, shape (n_classes,) or (n_classes - 1,) Array of C that maps to the best scores across every class. If refit is set to False, then for each class, the best C is the average of the Cs that correspond to the best scores for each fold. C_ is of shape(n_classes,) when the problem is binary. n_iter_ : array, shape (n_classes, n_folds, n_cs) or (1, n_folds, n_cs) Actual number of iterations for all classes, folds and Cs. In the binary or multinomial cases, the first dimension is equal to 1."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(penalty=l2, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=liblinear, max_iter=100, multi_class=ovr, verbose=0, warm_start=False, n_jobs=1)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :   Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted.  ", "methodParams": ["X"], "methodReturns": ["array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"], "methodDesc": "Predict confidence scores for samples. The confidence score for a sample is the signed distance of that sample to the hyperplane.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "densify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to dense array format. Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.", "methodReturnsBody": "self : estimator"}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the model according to the given training data.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Training vector, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape (n_samples,)   Target vector relative to X.   sample_weight : array-like, shape (n_samples,) optional   Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight.   New in version 0.17: sample_weight support to LogisticRegression.   "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples]   Predicted class label per sample.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict class labels for samples in X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Samples.  "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "T : array-like, shape = [n_samples, n_classes]   Returns the log-probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ .  ", "methodParams": ["X"], "methodReturns": ["T"], "methodDesc": "Log of probability estimates. The returned estimates for all classes are ordered by the label of classes.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "T : array-like, shape = [n_samples, n_classes]   Returns the probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ .  ", "methodParams": ["X"], "methodReturns": ["T"], "methodDesc": "Probability estimates. The returned estimates for all classes are ordered by the label of classes. For a multi_class problem, if multi_class is set to be multinomial the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "sparsify()", "methodReturns": [], "methodDesc": "Convert coefficient matrix to sparse format. Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation. The intercept_ member is not converted. Notes For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "methodReturnsBody": "self : estimator"}], "allFuncParams": ["penalty", "dual", "tol", "C", "fit_intercept", "intercept_scaling", "class_weight", "random_state", "solver", "max_iter", "multi_class", "verbose", "warm_start", "n_jobs"], "notes": "The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. Predict output may not match that of standalone liblinear in certain cases. See differences from liblinear in the narrative documentation. For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify. The underlying C implementation uses a random number generator to select features when fitting the model. It is thus not uncommon, to have slightly different results for the same input data. If that happens, try with a smaller tol parameter. Predict output may not match that of standalone liblinear in certain cases. See differences from liblinear in the narrative documentation. For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_  ==  0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.", "funcName": "LogisticRegression", "allFuncAttributes": ["coef_", "intercept_", "n_iter_"], "funcDesc": "Logistic Regression (aka logit, MaxEnt) classifier.", "funcParamBody": "penalty : str, l1 or l2, default: l2 Used to specify the norm used in the penalization. The newton-cg, sag and lbfgs solvers support only l2 penalties. New in version 0.19: l1 penalty with SAGA solver (allowing multinomial + L1) dual : bool, default: False Dual or primal formulation. Dual formulation is only implemented for l2 penalty with liblinear solver. Prefer dual=False when n_samples > n_features. tol : float, default: 1e-4 Tolerance for stopping criteria. C : float, default: 1.0 Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization. fit_intercept : bool, default: True Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function. intercept_scaling : float, default 1. Useful only when the solver liblinear is used and self.fit_intercept is set to True. In this case, x becomes [x, self.intercept_scaling], i.e. a synthetic feature with constant value equal to intercept_scaling is appended to the instance vector. The intercept becomes intercept_scaling  *  synthetic_feature_weight . Note! the synthetic feature weight is subject to l1/l2 regularization as all other features. To lessen the effect of regularization on synthetic feature weight (and therefore on the intercept) intercept_scaling has to be increased. class_weight : dict or balanced, default: None Weights associated with classes in the form {class_label:  weight} . If not given, all classes are supposed to have weight one. The balanced mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples  /  (n_classes  *  np.bincount(y)) . Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. New in version 0.17: class_weight=balanced random_state : int, RandomState instance or None, optional, default: None The seed of the pseudo random number generator to use when shuffling the data.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when solver == sag or liblinear. solver : {newton-cg, lbfgs, liblinear, sag, saga}, default: liblinear Algorithm to use in the optimization problem. For small datasets, liblinear is a good choice, whereas sag and saga are faster for large ones. For multiclass problems, only newton-cg, sag, saga and lbfgs handle multinomial loss; liblinear is limited to one-versus-rest schemes. newton-cg, lbfgs and sag only handle L2 penalty, whereas liblinear and saga handle L1 penalty. Note that sag and saga fast convergence is only guaranteed on features with approximately the same scale. You can preprocess the data with a scaler from sklearn.preprocessing. New in version 0.17: Stochastic Average Gradient descent solver. New in version 0.19: SAGA solver. max_iter : int, default: 100 Useful only for the newton-cg, sag and lbfgs solvers. Maximum number of iterations taken for the solvers to converge. multi_class : str, {ovr, multinomial}, default: ovr Multiclass option can be either ovr or multinomial. If the option chosen is ovr, then a binary problem is fit for each label. Else the loss minimised is the multinomial loss fit across the entire probability distribution. Does not work for liblinear solver. New in version 0.18: Stochastic Average Gradient descent solver for multinomial case. verbose : int, default: 0 For the liblinear and lbfgs solvers set verbose to any positive number for verbosity. warm_start : bool, default: False When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. Useless for liblinear solver. New in version 0.17: warm_start to support lbfgs , newton-cg , sag , saga solvers. n_jobs : int, default: 1 Number of CPU cores used when parallelizing over classes if multi_class=ovr. This parameter is ignored when the `` solver``is set to liblinear regardless of whether multi_class is specified or not. If given a value of -1, all cores are used.", "funcAttrBody": "coef_ : array, shape (1, n_features) or (n_classes, n_features) Coefficient of the features in the decision function. coef_ is of shape (1, n_features) when the given problem is binary. intercept_ : array, shape (1,) or (n_classes,) Intercept (a.k.a. bias) added to the decision function. If fit_intercept is set to False, the intercept is set to zero. intercept_ is of shape(1,) when the problem is binary. n_iter_ : array, shape (n_classes,) or (1, ) Actual number of iterations for all classes. If binary or multinomial, it returns only 1 element. For liblinear solver, only the maximum number of iteration across all classes is given."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit linear model.", "methodParamsBody": "X : numpy array or sparse matrix of shape [n_samples,n_features]   Training data   y : numpy array of shape [n_samples, n_targets]   Target values. Will be cast to Xs dtype if necessary   sample_weight : numpy array of shape [n_samples]   Individual weights for each sample   New in version 0.17: parameter sample_weight support to LinearRegression.   "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["fit_intercept", "normalize", "copy_X", "n_jobs"], "notes": "From the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) wrapped as a predictor object. From the implementation point of view, this is just plain Ordinary Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.", "funcName": "LinearRegression", "allFuncAttributes": ["coef_", "intercept_"], "funcDesc": "Ordinary least squares Linear Regression.", "funcParamBody": "fit_intercept : boolean, optional, default True whether to calculate the intercept for this model. If set to False, no intercept will be used in calculations (e.g. data is expected to be already centered). normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit on an estimator with normalize=False . copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. n_jobs : int, optional, default 1 The number of jobs to use for the computation. If -1 all CPUs are used. This will only provide speedup for n_targets > 1 and sufficient large problems.", "funcAttrBody": "coef_ : array, shape (n_features, ) or (n_targets, n_features) Estimated coefficients for the linear regression problem. If multiple targets are passed during the fit (y 2D), this is a 2D array of shape (n_targets, n_features), while if only one target is passed, this is a 1D array of length n_features. intercept_ : array Independent term in the linear model."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(criterion=aic, fit_intercept=True, verbose=False, normalize=True, precompute=auto, max_iter=500, eps=2.2204460492503131e-16, copy_X=True, positive=False)", "methodDesc": ""}, {"methodName": "fit(X, y, copy_X=True)", "methodReturnsBody": "self : object   returns an instance of self.  ", "methodParams": ["X", "y", "copy_X"], "methodReturns": ["self"], "methodDesc": "Fit the model using X, y as training data.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   training data.   y : array-like, shape (n_samples,)   target values. Will be cast to Xs dtype if necessary   copy_X : boolean, optional, default True   If True , X will be copied; else, it may be overwritten.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["criterion", "fit_intercept", "verbose", "normalize", "precompute", "max_iter", "eps", "copy_X", "positive"], "notes": "The estimation of the number of degrees of freedom is given by: On the degrees of freedom of the lasso Hui Zou, Trevor Hastie, and Robert Tibshirani Ann. Statist. Volume 35, Number 5 (2007), 2173-2192. https://en.wikipedia.org/wiki/Akaike_information_criterion  https://en.wikipedia.org/wiki/Bayesian_information_criterion The estimation of the number of degrees of freedom is given by: On the degrees of freedom of the lasso Hui Zou, Trevor Hastie, and Robert Tibshirani Ann. Statist. Volume 35, Number 5 (2007), 2173-2192. https://en.wikipedia.org/wiki/Akaike_information_criterion  https://en.wikipedia.org/wiki/Bayesian_information_criterion", "funcName": "LassoLarsIC", "allFuncAttributes": ["coef_", "intercept_", "alpha_", "n_iter_", "criterion_"], "funcDesc": "Lasso model fit with Lars using BIC or AIC for model selection", "funcParamBody": "criterion : bic | aic The type of criterion to use. fit_intercept : boolean whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). verbose : boolean or integer, optional Sets the verbosity amount normalize : boolean, optional, default True This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . precompute : True | False | auto | array-like Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. max_iter : integer, optional Maximum number of iterations to perform. Can be used for early stopping. eps : float, optional The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the tol parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. positive : boolean (default=False) Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients do not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value ( alphas_[alphas_  > 0.].min() when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator. As a consequence using LassoLarsIC only makes sense for problems where a sparse solution is expected and/or reached.", "funcAttrBody": "coef_ : array, shape (n_features,) parameter vector (w in the formulation formula) intercept_ : float independent term in decision function. alpha_ : float the alpha parameter chosen by the information criterion n_iter_ : int number of iterations run by lars_path to find the grid of alphas. criterion_ : array, shape (n_alphas,) The value of the information criteria (aic, bic) across all alphas. The alpha which has the smallest information criterion is chosen. This value is larger by a factor of n_samples compared to Eqns. 2.15 and 2.16 in (Zou et al, 2007)."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(fit_intercept=True, verbose=False, max_iter=500, normalize=True, precompute=auto, cv=None, max_n_alphas=1000, n_jobs=1, eps=2.2204460492503131e-16, copy_X=True, positive=False)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : object   returns an instance of self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model using X, y as training data.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data.   y : array-like, shape (n_samples,)   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["fit_intercept", "verbose", "max_iter", "normalize", "precompute", "cv", "max_n_alphas", "n_jobs", "eps", "copy_X", "positive"], "notes": "The object solves the same problem as the LassoCV object. However, unlike the LassoCV, it find the relevant alphas values by itself. In general, because of this property, it will be more stable. However, it is more fragile to heavily multicollinear datasets. It is more efficient than the LassoCV if only a small number of features are selected compared to the total number, for instance if there are very few samples compared to the number of features. The object solves the same problem as the LassoCV object. However, unlike the LassoCV, it find the relevant alphas values by itself. In general, because of this property, it will be more stable. However, it is more fragile to heavily multicollinear datasets. It is more efficient than the LassoCV if only a small number of features are selected compared to the total number, for instance if there are very few samples compared to the number of features.", "funcName": "LassoLarsCV", "allFuncAttributes": ["coef_", "intercept_", "coef_path_", "alpha_", "alphas_", "cv_alphas_", "mse_path_", "n_iter_"], "funcDesc": "Cross-validated Lasso, using the LARS algorithm", "funcParamBody": "fit_intercept : boolean whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). verbose : boolean or integer, optional Sets the verbosity amount max_iter : integer, optional Maximum number of iterations to perform. normalize : boolean, optional, default True This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . precompute : True | False | auto Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix cannot be passed as argument since we will use only subsets of X. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. max_n_alphas : integer, optional The maximum number of points on the path used to compute the residuals in the cross-validation n_jobs : integer, optional Number of CPUs to use during the cross validation. If -1 , use all the CPUs eps : float, optional The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. positive : boolean (default=False) Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients do not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value ( alphas_[alphas_  > 0.].min() when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator. As a consequence using LassoLarsCV only makes sense for problems where a sparse solution is expected and/or reached.", "funcAttrBody": "coef_ : array, shape (n_features,) parameter vector (w in the formulation formula) intercept_ : float independent term in decision function. coef_path_ : array, shape (n_features, n_alphas) the varying values of the coefficients along the path alpha_ : float the estimated regularization parameter alpha alphas_ : array, shape (n_alphas,) the different values of alpha along the path cv_alphas_ : array, shape (n_cv_alphas,) all the values of alpha along the path for the different folds mse_path_ : array, shape (n_folds, n_cv_alphas) the mean square error on left-out for each fold along the path (alpha values given by cv_alphas ) n_iter_ : array-like or int the number of iterations run by Lars with the optimal alpha."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(alpha=1.0, fit_intercept=True, verbose=False, normalize=True, precompute=auto, max_iter=500, eps=2.2204460492503131e-16, copy_X=True, fit_path=True, positive=False)", "methodDesc": ""}, {"methodName": "fit(X, y, Xy=None)", "methodReturnsBody": "self : object   returns an instance of self.  ", "methodParams": ["X", "y", "Xy"], "methodReturns": ["self"], "methodDesc": "Fit the model using X, y as training data.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data.   y : array-like, shape (n_samples,) or (n_samples, n_targets)   Target values.   Xy : array-like, shape (n_samples,) or (n_samples, n_targets),                 optional   Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["alpha", "fit_intercept", "verbose", "normalize", "precompute", "max_iter", "eps", "copy_X", "fit_path", "positive"], "notes": "", "funcName": "LassoLars", "allFuncAttributes": ["alphas_", "active_", "coef_path_", "coef_", "intercept_", "n_iter_"], "funcDesc": "Lasso model fit with Least Angle Regression a.k.a. Lars", "funcParamBody": "alpha : float Constant that multiplies the penalty term. Defaults to 1.0. alpha  =  0 is equivalent to an ordinary least square, solved by LinearRegression . For numerical reasons, using alpha  =  0 with the LassoLars object is not advised and you should prefer the LinearRegression object. fit_intercept : boolean whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). verbose : boolean or integer, optional Sets the verbosity amount normalize : boolean, optional, default True This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . precompute : True | False | auto | array-like Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. max_iter : integer, optional Maximum number of iterations to perform. eps : float, optional The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the tol parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. fit_path : boolean If True the full path is stored in the coef_path_ attribute. If you compute the solution for a large problem or many targets, setting fit_path to False will lead to a speedup, especially with a small alpha. positive : boolean (default=False) Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default. Under the positive restriction the model coefficients will not converge to the ordinary-least-squares solution for small values of alpha. Only coefficients up to the smallest alpha value ( alphas_[alphas_  > 0.].min() when fit_path=True) reached by the stepwise Lars-Lasso algorithm are typically in congruence with the solution of the coordinate descent Lasso estimator.", "funcAttrBody": "alphas_ : array, shape (n_alphas + 1,) | list of n_targets such arrays Maximum of covariances (in absolute value) at each iteration. n_alphas is either max_iter , n_features , or the number of         nodes in the path with correlation greater than alpha , whichever         is smaller. active_ : list, length = n_alphas | list of n_targets such lists Indices of active variables at the end of the path. coef_path_ : array, shape (n_features, n_alphas + 1) or list If a list is passed its expected to be one of n_targets such arrays. The varying values of the coefficients along the path. It is not present if the fit_path parameter is False . coef_ : array, shape (n_features,) or (n_targets, n_features) Parameter vector (w in the formulation formula). intercept_ : float | array, shape (n_targets,) Independent term in decision function. n_iter_ : array-like or int. The number of iterations taken by lars_path to find the grid of alphas for each target."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute=auto, max_iter=1000, tol=0.0001, copy_X=True, cv=None, verbose=False, n_jobs=1, positive=False, random_state=None, selection=cyclic)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : {array-like}, shape (n_samples, n_features)   Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output, X can be sparse.   y : array-like, shape (n_samples,) or (n_samples, n_targets)   Target values  ", "methodDesc": "Fit linear model with coordinate descent Fit is on grid of alphas and best alpha estimated by cross-validation."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["eps", "n_alphas", "alphas", "fit_intercept", "normalize", "precompute", "max_iter", "tol", "copy_X", "cv", "verbose", "n_jobs", "positive", "random_state", "selection"], "notes": "For an example, see examples/linear_model/plot_lasso_model_selection.py . To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py . To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. Note that in certain cases, the Lars solver may be significantly faster to implement this functionality. In particular, linear interpolation can be used to retrieve model coefficients between the values output by lars_path Comparing lasso_path and lars_path with interpolation: For an example, see examples/linear_model/plot_lasso_model_selection.py . To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py . To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. Note that in certain cases, the Lars solver may be significantly faster to implement this functionality. In particular, linear interpolation can be used to retrieve model coefficients between the values output by lars_path Comparing lasso_path and lars_path with interpolation:", "funcName": "LassoCV", "allFuncAttributes": ["alpha_", "coef_", "intercept_", "mse_path_", "alphas_", "dual_gap_", "n_iter_"], "funcDesc": "Lasso linear model with iterative fitting along a regularization path", "funcParamBody": "eps : float, optional Length of the path. eps=1e-3 means that alpha_min  /  alpha_max  =  1e-3 . n_alphas : int, optional Number of alphas along the regularization path alphas : numpy array, optional List of alphas where to compute the models. If None alphas are set automatically fit_intercept : boolean, default True whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . precompute : True | False | auto | array-like Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. max_iter : int, optional The maximum number of iterations tol : float, optional The tolerance for the optimization: if the updates are smaller than tol , the optimization code checks the dual gap for optimality and continues until it is smaller than tol . copy_X : boolean, optional, default True If True , X will be copied; else, it may be overwritten. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. verbose : bool or integer Amount of verbosity. n_jobs : integer, optional Number of CPUs to use during the cross validation. If -1 , use all the CPUs. positive : bool, optional If positive, restrict regression coefficients to be positive random_state : int, RandomState instance or None, optional, default None The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when selection == random. selection : str, default cyclic If set to random, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to random) often leads to significantly faster convergence especially when tol is higher than 1e-4.", "funcAttrBody": "alpha_ : float The amount of penalization chosen by cross validation coef_ : array, shape (n_features,) | (n_targets, n_features) parameter vector (w in the cost function formula) intercept_ : float | array, shape (n_targets,) independent term in decision function. mse_path_ : array, shape (n_alphas, n_folds) mean square error for the test set on each fold, varying alpha alphas_ : numpy array, shape (n_alphas,) The grid of alphas used for fitting dual_gap_ : ndarray, shape () The dual gap at the end of the optimization for the optimal alpha ( alpha_ ). n_iter_ : int number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(alpha=1.0, fit_intercept=True, normalize=False, precompute=False, copy_X=True, max_iter=1000, tol=0.0001, warm_start=False, positive=False, random_state=None, selection=cyclic)", "methodDesc": ""}, {"methodParams": ["X", "y", "check_input"], "methodName": "fit(X, y, check_input=True)", "methodParamsBody": "X : ndarray or scipy.sparse matrix, (n_samples, n_features)   Data   y : ndarray, shape (n_samples,) or (n_samples, n_targets)   Target. Will be cast to Xs dtype if necessary   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.  ", "methodDesc": "Fit model with coordinate descent. Notes Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary. To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["alpha", "fit_intercept", "normalize", "precompute", "copy_X", "max_iter", "tol", "warm_start", "positive", "random_state", "selection"], "notes": "The algorithm used to fit the model is coordinate descent. To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary. To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format. For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py . The algorithm used to fit the model is coordinate descent. To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary. To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format. For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py .", "funcName": "Lasso", "allFuncAttributes": ["coef_", "sparse_coef_", "intercept_", "n_iter_"], "funcDesc": "Linear Model trained with L1 prior as regularizer (aka the Lasso)", "funcParamBody": "alpha : float, optional Constant that multiplies the L1 term. Defaults to 1.0. alpha  =  0 is equivalent to an ordinary least square, solved by the LinearRegression object. For numerical reasons, using alpha  =  0 with the Lasso object is not advised. Given this, you should use the LinearRegression object. fit_intercept : boolean whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . precompute : True | False | array-like, default=False Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. For sparse input this option is always True to preserve sparsity. copy_X : boolean, optional, default True If True , X will be copied; else, it may be overwritten. max_iter : int, optional The maximum number of iterations tol : float, optional The tolerance for the optimization: if the updates are smaller than tol , the optimization code checks the dual gap for optimality and continues until it is smaller than tol . warm_start : bool, optional When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. positive : bool, optional When set to True , forces the coefficients to be positive. random_state : int, RandomState instance or None, optional, default None The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when selection == random. selection : str, default cyclic If set to random, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to random) often leads to significantly faster convergence especially when tol is higher than 1e-4.", "funcAttrBody": "coef_ : array, shape (n_features,) | (n_targets, n_features) parameter vector (w in the cost function formula) sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features) sparse_coef_ is a readonly property derived from coef_ intercept_ : float | array, shape (n_targets,) independent term in decision function. n_iter_ : int | array-like, shape (n_targets,) number of iterations run by the coordinate descent solver to reach the specified tolerance."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(fit_intercept=True, verbose=False, max_iter=500, normalize=True, precompute=auto, cv=None, max_n_alphas=1000, n_jobs=1, eps=2.2204460492503131e-16, copy_X=True, positive=False)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : object   returns an instance of self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model using X, y as training data.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data.   y : array-like, shape (n_samples,)   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["fit_intercept", "verbose", "max_iter", "normalize", "precompute", "cv", "max_n_alphas", "n_jobs", "eps", "copy_X", "positive"], "notes": "", "funcName": "LarsCV", "allFuncAttributes": ["coef_", "intercept_", "coef_path_", "alpha_", "alphas_", "cv_alphas_", "mse_path_", "n_iter_"], "funcDesc": "Cross-validated Least Angle Regression model", "funcParamBody": "fit_intercept : boolean whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). verbose : boolean or integer, optional Sets the verbosity amount max_iter : integer, optional Maximum number of iterations to perform. normalize : boolean, optional, default True This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . precompute : True | False | auto | array-like Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix cannot be passed as argument since we will use only subsets of X. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. max_n_alphas : integer, optional The maximum number of points on the path used to compute the residuals in the cross-validation n_jobs : integer, optional Number of CPUs to use during the cross validation. If -1 , use all the CPUs eps : float, optional The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. copy_X : boolean, optional, default True If True , X will be copied; else, it may be overwritten. positive : boolean (default=False) Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default.", "funcAttrBody": "coef_ : array, shape (n_features,) parameter vector (w in the formulation formula) intercept_ : float independent term in decision function coef_path_ : array, shape (n_features, n_alphas) the varying values of the coefficients along the path alpha_ : float the estimated regularization parameter alpha alphas_ : array, shape (n_alphas,) the different values of alpha along the path cv_alphas_ : array, shape (n_cv_alphas,) all the values of alpha along the path for the different folds mse_path_ : array, shape (n_folds, n_cv_alphas) the mean square error on left-out for each fold along the path (alpha values given by cv_alphas ) n_iter_ : array-like or int the number of iterations run by Lars with the optimal alpha."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(fit_intercept=True, verbose=False, normalize=True, precompute=auto, n_nonzero_coefs=500, eps=2.2204460492503131e-16, copy_X=True, fit_path=True, positive=False)", "methodDesc": ""}, {"methodName": "fit(X, y, Xy=None)", "methodReturnsBody": "self : object   returns an instance of self.  ", "methodParams": ["X", "y", "Xy"], "methodReturns": ["self"], "methodDesc": "Fit the model using X, y as training data.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data.   y : array-like, shape (n_samples,) or (n_samples, n_targets)   Target values.   Xy : array-like, shape (n_samples,) or (n_samples, n_targets),                 optional   Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the Gram matrix is precomputed.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["fit_intercept", "verbose", "normalize", "precompute", "n_nonzero_coefs", "eps", "copy_X", "fit_path", "positive"], "notes": "", "funcName": "Lars", "allFuncAttributes": ["alphas_", "active_", "coef_path_", "coef_", "intercept_", "n_iter_"], "funcDesc": "Least Angle Regression model a.k.a. LAR", "funcParamBody": "fit_intercept : boolean Whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). verbose : boolean or integer, optional Sets the verbosity amount normalize : boolean, optional, default True This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . precompute : True | False | auto | array-like Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. n_nonzero_coefs : int, optional Target number of non-zero coefficients. Use np.inf for no limit. eps : float, optional The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. Unlike the tol parameter in some iterative optimization-based algorithms, this parameter does not control the tolerance of the optimization. copy_X : boolean, optional, default True If True , X will be copied; else, it may be overwritten. fit_path : boolean If True the full path is stored in the coef_path_ attribute. If you compute the solution for a large problem or many targets, setting fit_path to False will lead to a speedup, especially with a small alpha. positive : boolean (default=False) Restrict coefficients to be >= 0. Be aware that you might want to remove fit_intercept which is set True by default.", "funcAttrBody": "alphas_ : array, shape (n_alphas + 1,) | list of n_targets such arrays Maximum of covariances (in absolute value) at each iteration. n_alphas is either n_nonzero_coefs or n_features ,         whichever is smaller. active_ : list, length = n_alphas | list of n_targets such lists Indices of active variables at the end of the path. coef_path_ : array, shape (n_features, n_alphas + 1)         | list of n_targets such arrays The varying values of the coefficients along the path. It is not present if the fit_path parameter is False . coef_ : array, shape (n_features,) or (n_targets, n_features) Parameter vector (w in the formulation formula). intercept_ : float | array, shape (n_targets,) Independent term in decision function. n_iter_ : array-like or int The number of iterations taken by lars_path to find the grid of alphas for each target."},
{"funcName": "HuberRegressor", "notes": "", "libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(epsilon=1.35, max_iter=100, alpha=0.0001, warm_start=False, fit_intercept=True, tol=1e-05)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the model according to the given training data.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training vector, where n_samples in the number of samples and n_features is the number of features.   y : array-like, shape (n_samples,)   Target vector relative to X.   sample_weight : array-like, shape (n_samples,)   Weight given to each sample.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "Linear regression model that is robust to outliers."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(l1_ratio=0.5, eps=0.001, n_alphas=100, alphas=None, fit_intercept=True, normalize=False, precompute=auto, max_iter=1000, tol=0.0001, cv=None, copy_X=True, verbose=0, n_jobs=1, positive=False, random_state=None, selection=cyclic)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : {array-like}, shape (n_samples, n_features)   Training data. Pass directly as Fortran-contiguous data to avoid unnecessary memory duplication. If y is mono-output, X can be sparse.   y : array-like, shape (n_samples,) or (n_samples, n_targets)   Target values  ", "methodDesc": "Fit linear model with coordinate descent Fit is on grid of alphas and best alpha estimated by cross-validation."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["l1_ratio", "eps", "n_alphas", "alphas", "fit_intercept", "normalize", "precompute", "max_iter", "tol", "cv", "copy_X", "verbose", "n_jobs", "positive", "random_state", "selection"], "notes": "For an example, see examples/linear_model/plot_lasso_model_selection.py . To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. More specifically, the optimization objective is: If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to: for: For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py .", "funcName": "ElasticNetCV", "allFuncAttributes": ["alpha_", "l1_ratio_", "coef_", "intercept_", "mse_path_", "alphas_", "n_iter_"], "funcDesc": "Elastic Net model with iterative fitting along a regularization path", "funcParamBody": "l1_ratio : float or array of floats, optional float between 0 and 1 passed to ElasticNet (scaling between l1 and l2 penalties). For l1_ratio  =  0 l1_ratio  =  1 it is an L1 penalty. For 0  <  l1_ratio  <  1 , the penalty is a combination of L1 and L2 This parameter can be a list, in which case the different values are tested by cross-validation and the one giving the best prediction score is used. Note that a good choice of list of values for l1_ratio is often to put more values close to 1 (i.e. Lasso) and less close to 0 (i.e. Ridge), as in [.1,  .5,  .7, .9,  .95,  .99,  1] eps : float, optional Length of the path. eps=1e-3 means that alpha_min  /  alpha_max  =  1e-3 . n_alphas : int, optional Number of alphas along the regularization path, used for each l1_ratio. alphas : numpy array, optional List of alphas where to compute the models. If None alphas are set automatically fit_intercept : boolean whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . precompute : True | False | auto | array-like Whether to use a precomputed Gram matrix to speed up calculations. If set to 'auto' let us decide. The Gram matrix can also be passed as argument. max_iter : int, optional The maximum number of iterations tol : float, optional The tolerance for the optimization: if the updates are smaller than tol , the optimization code checks the dual gap for optimality and continues until it is smaller than tol . cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs, KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. copy_X : boolean, optional, default True If True , X will be copied; else, it may be overwritten. verbose : bool or integer Amount of verbosity. n_jobs : integer, optional Number of CPUs to use during the cross validation. If -1 , use all the CPUs. positive : bool, optional When set to True , forces the coefficients to be positive. random_state : int, RandomState instance or None, optional, default None The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when selection == random. selection : str, default cyclic If set to random, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to random) often leads to significantly faster convergence especially when tol is higher than 1e-4.", "funcAttrBody": "alpha_ : float The amount of penalization chosen by cross validation l1_ratio_ : float The compromise between l1 and l2 penalization chosen by cross validation coef_ : array, shape (n_features,) | (n_targets, n_features) Parameter vector (w in the cost function formula), intercept_ : float | array, shape (n_targets, n_features) Independent term in the decision function. mse_path_ : array, shape (n_l1_ratio, n_alpha, n_folds) Mean square error for the test set on each fold, varying l1_ratio and alpha. alphas_ : numpy array, shape (n_alphas,) or (n_l1_ratio, n_alphas) The grid of alphas used for fitting, for each l1_ratio. n_iter_ : int number of iterations run by the coordinate descent solver to reach the specified tolerance for the optimal alpha."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(alpha=1.0, l1_ratio=0.5, fit_intercept=True, normalize=False, precompute=False, max_iter=1000, copy_X=True, tol=0.0001, warm_start=False, positive=False, random_state=None, selection=cyclic)", "methodDesc": ""}, {"methodParams": ["X", "y", "check_input"], "methodName": "fit(X, y, check_input=True)", "methodParamsBody": "X : ndarray or scipy.sparse matrix, (n_samples, n_features)   Data   y : ndarray, shape (n_samples,) or (n_samples, n_targets)   Target. Will be cast to Xs dtype if necessary   check_input : boolean, (default=True)   Allow to bypass several input checking. Dont use this parameter unless you know what you do.  ", "methodDesc": "Fit model with coordinate descent. Notes Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary. To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the linear model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["alpha", "l1_ratio", "fit_intercept", "normalize", "precompute", "max_iter", "copy_X", "tol", "warm_start", "positive", "random_state", "selection"], "notes": "To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary. To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format. For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py . To avoid unnecessary memory duplication the X argument of the fit method should be directly passed as a Fortran-contiguous numpy array. Coordinate descent is an algorithm that considers each column of data at a time hence it will automatically convert the X input as a Fortran-contiguous numpy array if necessary. To avoid memory re-allocation it is advised to allocate the initial data in memory directly using that format. For an example, see examples/linear_model/plot_lasso_coordinate_descent_path.py .", "funcName": "ElasticNet", "allFuncAttributes": ["coef_", "sparse_coef_", "intercept_", "n_iter_"], "funcDesc": "Linear regression with combined L1 and L2 priors as regularizer.", "funcParamBody": "alpha : float, optional Constant that multiplies the penalty terms. Defaults to 1.0. See the notes for the exact mathematical meaning of this parameter.``alpha = 0`` is equivalent to an ordinary least square, solved by the LinearRegression object. For numerical reasons, using alpha  =  0 with the Lasso object is not advised. Given this, you should use the LinearRegression object. l1_ratio : float The ElasticNet mixing parameter, with 0  <=  l1_ratio  <=  1 . For l1_ratio  =  0 the penalty is an L2 penalty. For  l1_ratio  =  1 it is an L1 penalty.  For 0  <  l1_ratio  <  1 , the penalty is a combination of L1 and L2. fit_intercept : bool Whether the intercept should be estimated or not. If False , the data is assumed to be already centered. normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . precompute : True | False | array-like Whether to use a precomputed Gram matrix to speed up calculations. The Gram matrix can also be passed as argument. For sparse input this option is always True to preserve sparsity. max_iter : int, optional The maximum number of iterations copy_X : boolean, optional, default True If True , X will be copied; else, it may be overwritten. tol : float, optional The tolerance for the optimization: if the updates are smaller than tol , the optimization code checks the dual gap for optimality and continues until it is smaller than tol . warm_start : bool, optional When set to True , reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution. positive : bool, optional When set to True , forces the coefficients to be positive. random_state : int, RandomState instance or None, optional, default None The seed of the pseudo random number generator that selects a random feature to update.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when selection == random. selection : str, default cyclic If set to random, a random coefficient is updated every iteration rather than looping over features sequentially by default. This (setting to random) often leads to significantly faster convergence especially when tol is higher than 1e-4.", "funcAttrBody": "coef_ : array, shape (n_features,) | (n_targets, n_features) parameter vector (w in the cost function formula) sparse_coef_ : scipy.sparse matrix, shape (n_features, 1) |             (n_targets, n_features) sparse_coef_ is a readonly property derived from coef_ intercept_ : float | array, shape (n_targets,) independent term in decision function. n_iter_ : array-like, shape (n_targets,) number of iterations run by the coordinate descent solver to reach the specified tolerance."},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, fit_intercept=True, normalize=False, copy_X=True, verbose=False)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model", "methodParamsBody": "X : numpy array of shape [n_samples,n_features]   Training data   y : numpy array of shape [n_samples]   Target values. Will be cast to Xs dtype if necessary  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X, return_std=False)", "methodReturnsBody": "y_mean : array, shape = (n_samples,)   Mean of predictive distribution of query points.   y_std : array, shape = (n_samples,)   Standard deviation of predictive distribution of query points.  ", "methodParams": ["X", "return_std"], "methodReturns": ["y_mean", "y_std"], "methodDesc": "Predict using the linear model. In addition to the mean of the predictive distribution, also its standard deviation can be returned.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.   return_std : boolean, optional   Whether to return the standard deviation of posterior prediction.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["n_iter", "tol", "alpha_1", "alpha_2", "lambda_1", "lambda_2", "compute_score", "fit_intercept", "normalize", "copy_X", "verbose"], "notes": "For an example, see examples/linear_model/plot_bayesian_ridge.py . D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems, Vol. 4, No. 3, 1992. R. Salakhutdinov, Lecture notes on Statistical Machine Learning, http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15 Their beta is our self.alpha_ Their alpha is our self.lambda_ For an example, see examples/linear_model/plot_bayesian_ridge.py . D. J. C. MacKay, Bayesian Interpolation, Computation and Neural Systems, Vol. 4, No. 3, 1992. R. Salakhutdinov, Lecture notes on Statistical Machine Learning, http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15 Their beta is our self.alpha_ Their alpha is our self.lambda_", "funcName": "BayesianRidge", "allFuncAttributes": ["coef_", "alpha_", "lambda_", "sigma_", "scores_"], "funcDesc": "Bayesian ridge regression", "funcParamBody": "n_iter : int, optional Maximum number of iterations.  Default is 300. tol : float, optional Stop the algorithm if w has converged. Default is 1.e-3. alpha_1 : float, optional Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Default is 1.e-6 alpha_2 : float, optional Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter. Default is 1.e-6. lambda_1 : float, optional Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter. Default is 1.e-6. lambda_2 : float, optional Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. Default is 1.e-6 compute_score : boolean, optional If True, compute the objective function at each step of the model. Default is False fit_intercept : boolean, optional whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). Default is True. normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . copy_X : boolean, optional, default True If True, X will be copied; else, it may be overwritten. verbose : boolean, optional, default False Verbose mode when fitting the model.", "funcAttrBody": "coef_ : array, shape = (n_features) Coefficients of the regression model (mean of distribution) alpha_ : float estimated precision of the noise. lambda_ : float estimated precision of the weights. sigma_ : array, shape = (n_features, n_features) estimated variance-covariance matrix of the weights scores_ : float if computed, value of the objective function (to be maximized)"},
{"libName": "sklearn.linear_model", "methods": [{"methodName": "__init__(n_iter=300, tol=0.001, alpha_1=1e-06, alpha_2=1e-06, lambda_1=1e-06, lambda_2=1e-06, compute_score=False, threshold_lambda=10000.0, fit_intercept=True, normalize=False, copy_X=True, verbose=False)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the ARDRegression model according to the given training data and parameters. Iterative procedure to maximize the evidence", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vector, where n_samples in the number of samples and n_features is the number of features.   y : array, shape = [n_samples]   Target values (integers). Will be cast to Xs dtype if necessary  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X, return_std=False)", "methodReturnsBody": "y_mean : array, shape = (n_samples,)   Mean of predictive distribution of query points.   y_std : array, shape = (n_samples,)   Standard deviation of predictive distribution of query points.  ", "methodParams": ["X", "return_std"], "methodReturns": ["y_mean", "y_std"], "methodDesc": "Predict using the linear model. In addition to the mean of the predictive distribution, also its standard deviation can be returned.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.   return_std : boolean, optional   Whether to return the standard deviation of posterior prediction.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["n_iter", "tol", "alpha_1", "alpha_2", "lambda_1", "lambda_2", "compute_score", "threshold_lambda", "fit_intercept", "normalize", "copy_X", "verbose"], "notes": "For an example, see examples/linear_model/plot_ard.py . D. J. C. MacKay, Bayesian nonlinear modeling for the prediction competition, ASHRAE Transactions, 1994. R. Salakhutdinov, Lecture notes on Statistical Machine Learning, http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15 Their beta is our self.alpha_ Their alpha is our self.lambda_ ARD is a little different than the slide: only dimensions/features for which self.lambda_  <  self.threshold_lambda are kept and the rest are discarded. For an example, see examples/linear_model/plot_ard.py . D. J. C. MacKay, Bayesian nonlinear modeling for the prediction competition, ASHRAE Transactions, 1994. R. Salakhutdinov, Lecture notes on Statistical Machine Learning, http://www.utstat.toronto.edu/~rsalakhu/sta4273/notes/Lecture2.pdf#page=15 Their beta is our self.alpha_ Their alpha is our self.lambda_ ARD is a little different than the slide: only dimensions/features for which self.lambda_  <  self.threshold_lambda are kept and the rest are discarded.", "funcName": "ARDRegression", "allFuncAttributes": ["coef_", "alpha_", "lambda_", "sigma_", "scores_"], "funcDesc": "Bayesian ARD regression.", "funcParamBody": "n_iter : int, optional Maximum number of iterations. Default is 300 tol : float, optional Stop the algorithm if w has converged. Default is 1.e-3. alpha_1 : float, optional Hyper-parameter : shape parameter for the Gamma distribution prior over the alpha parameter. Default is 1.e-6. alpha_2 : float, optional Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the alpha parameter. Default is 1.e-6. lambda_1 : float, optional Hyper-parameter : shape parameter for the Gamma distribution prior over the lambda parameter. Default is 1.e-6. lambda_2 : float, optional Hyper-parameter : inverse scale parameter (rate parameter) for the Gamma distribution prior over the lambda parameter. Default is 1.e-6. compute_score : boolean, optional If True, compute the objective function at each step of the model. Default is False. threshold_lambda : float, optional threshold for removing (pruning) weights with high precision from the computation. Default is 1.e+4. fit_intercept : boolean, optional whether to calculate the intercept for this model. If set to false, no intercept will be used in calculations (e.g. data is expected to be already centered). Default is True. normalize : boolean, optional, default False This parameter is ignored when fit_intercept is set to False. If True, the regressors X will be normalized before regression by subtracting the mean and dividing by the l2-norm. If you wish to standardize, please use sklearn.preprocessing.StandardScaler before calling fit normalize=False . copy_X : boolean, optional, default True. If True, X will be copied; else, it may be overwritten. verbose : boolean, optional, default False Verbose mode when fitting the model.", "funcAttrBody": "coef_ : array, shape = (n_features) Coefficients of the regression model (mean of distribution) alpha_ : float estimated precision of the noise. lambda_ : array, shape = (n_features) estimated precisions of the weights. sigma_ : array, shape = (n_features, n_features) estimated variance-covariance matrix of the weights scores_ : float if computed, value of the objective function (to be maximized)"},
{"libName": "sklearn.kernel_ridge", "methods": [{"methodName": "__init__(alpha=1, kernel=linear, gamma=None, degree=3, coef0=1, kernel_params=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None, sample_weight=None)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit Kernel Ridge regression model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training data   y : array-like, shape = [n_samples] or [n_samples, n_targets]   Target values   sample_weight : float or array-like of shape [n_samples]   Individual weights for each sample, ignored if None is passed.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples] or [n_samples, n_targets]   Returns predicted values.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict using the kernel ridge model", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["alpha", "kernel", "gamma", "degree", "coef0", "kernel_params"], "notes": "", "funcName": "KernelRidge", "allFuncAttributes": ["dual_coef_", "X_fit_"], "funcDesc": "Kernel ridge regression.", "funcParamBody": "alpha : {float, array-like}, shape = [n_targets] Small positive values of alpha improve the conditioning of the problem and reduce the variance of the estimates.  Alpha corresponds to (2*C)^-1 in other linear models such as LogisticRegression or LinearSVC. If an array is passed, penalties are assumed to be specific to the targets. Hence they must correspond in number. kernel : string or callable, default=linear Kernel mapping used internally. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number. gamma : float, default=None Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Interpretation of the default value is left to the kernel; see the documentation for sklearn.metrics.pairwise. Ignored by other kernels. degree : float, default=3 Degree of the polynomial kernel. Ignored by other kernels. coef0 : float, default=1 Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels. kernel_params : mapping of string to any, optional Additional parameters (keyword arguments) for kernel function passed as callable object.", "funcAttrBody": "dual_coef_ : array, shape = [n_samples] or [n_samples, n_targets] Representation of weight vector(s) in kernel space X_fit_ : {array-like, sparse matrix}, shape = [n_samples, n_features] Training data, which is also required for prediction"},
{"libName": "sklearn.kernel_approximation", "methods": [{"methodName": "__init__(skewedness=1.0, n_components=100, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns the transformer.  ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Fit the model with X. Samples random projection according to n_features.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples in the number of samples and n_features is the number of features.  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components) ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Apply the approximate feature map to X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   New data, where n_samples in the number of samples and n_features is the number of features. All values of X must be strictly greater than -skewedness.  "}], "notes": "", "funcName": "SkewedChi2Sampler", "allFuncParams": ["skewedness", "n_components", "random_state"], "funcDesc": "Approximates feature map of the skewed chi-squared kernel by Monte Carlo approximation of its Fourier transform.", "funcParamBody": "skewedness : float skewedness parameter of the kernel. Needs to be cross-validated. n_components : int number of Monte Carlo samples per original feature. Equals the dimensionality of the computed feature space. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random ."},
{"libName": "sklearn.kernel_approximation", "methods": [{"methodName": "__init__(gamma=1.0, n_components=100, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns the transformer.  ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Fit the model with X. Samples random projection according to n_features.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Training data, where n_samples in the number of samples and n_features is the number of features.  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components) ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Apply the approximate feature map to X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   New data, where n_samples in the number of samples and n_features is the number of features.  "}], "notes": "See Random Features for Large-Scale Kernel Machines by A. Rahimi and Benjamin Recht. [1] Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning by A. Rahimi and Benjamin Recht. ( http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf ) See Random Features for Large-Scale Kernel Machines by A. Rahimi and Benjamin Recht. [1] Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning by A. Rahimi and Benjamin Recht. ( http://people.eecs.berkeley.edu/~brecht/papers/08.rah.rec.nips.pdf )", "funcName": "RBFSampler", "allFuncParams": ["gamma", "n_components", "random_state"], "funcDesc": "Approximates feature map of an RBF kernel by Monte Carlo approximation of its Fourier transform.", "funcParamBody": "gamma : float Parameter of RBF kernel: exp(-gamma * x^2) n_components : int Number of Monte Carlo samples per original feature. Equals the dimensionality of the computed feature space. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random ."},
{"libName": "sklearn.kernel_approximation", "methods": [{"methodName": "__init__(kernel=rbf, gamma=None, coef0=None, degree=None, kernel_params=None, n_components=100, random_state=None)", "methodDesc": ""}, {"methodParams": ["X"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like, shape=(n_samples, n_feature)   Training data.  ", "methodDesc": "Fit estimator to data. Samples a subset of training points, computes kernel on these and computes normalization matrix."}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_transformed : array, shape=(n_samples, n_components)   Transformed data.  ", "methodParams": ["X"], "methodReturns": ["X_transformed"], "methodDesc": "Apply feature map to X. Computes an approximate feature map using the kernel between some training points and X.", "methodParamsBody": "X : array-like, shape=(n_samples, n_features)   Data to transform.  "}], "allFuncParams": ["kernel", "n_components", "gamma", "degree", "coef0", "kernel_params", "random_state"], "notes": "", "funcName": "Nystroem", "allFuncAttributes": ["components_", "component_indices_", "normalization_"], "funcDesc": "Approximate a kernel map using a subset of the training data.", "funcParamBody": "kernel : string or callable, default=rbf Kernel map to be approximated. A callable should accept two arguments and the keyword arguments passed to this object as kernel_params, and should return a floating point number. n_components : int Number of features to construct. How many data points will be used to construct the mapping. gamma : float, default=None Gamma parameter for the RBF, laplacian, polynomial, exponential chi2 and sigmoid kernels. Interpretation of the default value is left to the kernel; see the documentation for sklearn.metrics.pairwise. Ignored by other kernels. degree : float, default=None Degree of the polynomial kernel. Ignored by other kernels. coef0 : float, default=None Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels. kernel_params : mapping of string to any, optional Additional parameters (keyword arguments) for kernel function passed as callable object. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcAttrBody": "components_ : array, shape (n_components, n_features) Subset of training points used to construct the feature map. component_indices_ : array, shape (n_components) Indices of components_ in the training set. normalization_ : array, shape (n_components, n_components) Normalization matrix needed for embedding. Square root of the kernel matrix on components_ ."},
{"libName": "sklearn.kernel_approximation", "methods": [{"methodName": "__init__(sample_steps=2, sample_interval=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodDesc": "Set parameters."}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : {array, sparse matrix},                shape = (n_samples, n_features * (2*sample_steps + 1))   Whether the return value is an array of sparse matrix depends on the type of the input X.  ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Apply approximate feature map to X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features) "}], "notes": "This estimator approximates a slightly different version of the additive chi squared kernel then metric.additive_chi2 computes. See Efficient additive kernels via explicit feature maps A. Vedaldi and A. Zisserman, Pattern Analysis and Machine Intelligence, 2011", "funcName": "AdditiveChi2Sampler", "allFuncParams": ["sample_steps", "sample_interval"], "funcDesc": "Approximate feature map for additive chi2 kernel.", "funcParamBody": "sample_steps : int, optional Gives the number of (complex) sampling points. sample_interval : float, optional Sampling interval. Must be specified when sample_steps not in {1,2,3}."},
{"allReturnParams": ["y_"], "libName": "sklearn.isotonic", "methods": [], "notes": "", "funcName": "isotonic_regression", "allFuncParams": ["y", "sample_weight", "y_min", "y_max", "increasing"], "funcDesc": "Solve the isotonic regression model:", "funcParamBody": "y : iterable of floating-point values The data. sample_weight : iterable of floating-point values, optional, default: None Weights on each point of the regression. If None, weight is set to 1 (equal weights). y_min : optional, default: None If not None, set the lowest value of the fit to y_min. y_max : optional, default: None If not None, set the highest value of the fit to y_max. increasing : boolean, optional, default: True Whether to compute y_ is increasing (if set to True) or decreasing (if set to False)", "funcReturnBody": "y_ : list of floating-point values Isotonic fit of y."},
{"allReturnParams": ["increasing_bool"], "libName": "sklearn.isotonic", "methods": [], "notes": "The Spearman correlation coefficient is estimated from the data, and the sign of the resulting estimate is used as the result. In the event that the 95% confidence interval based on Fisher transform spans zero, a warning is raised. Fisher transformation. Wikipedia. https://en.wikipedia.org/wiki/Fisher_transformation", "funcName": "check_increasing", "allFuncParams": ["x", "y"], "funcDesc": "Determine whether y is monotonically correlated with x.", "funcParamBody": "x : array-like, shape=(n_samples,) Training data. y : array-like, shape=(n_samples,) Training target.", "funcReturnBody": "increasing_bool : boolean Whether the relationship is increasing or decreasing."},
{"libName": "sklearn.isotonic", "methods": [{"methodName": "__init__(y_min=None, y_max=None, increasing=True, out_of_bounds=nan)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns an instance of self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the model using X, y as training data. Notes X is stored for future use, as transform needs X to interpolate new input data.", "methodParamsBody": "X : array-like, shape=(n_samples,)   Training data.   y : array-like, shape=(n_samples,)   Training target.   sample_weight : array-like, shape=(n_samples,), optional, default: None   Weights. If set to None, all weights will be set to 1 (equal weights).  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(T)", "methodReturnsBody": "T_ : array, shape=(n_samples,)   Transformed data.  ", "methodParams": ["T"], "methodReturns": ["T_"], "methodDesc": "Predict new data by linear interpolation.", "methodParamsBody": "T : array-like, shape=(n_samples,)   Data to transform.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(T)", "methodReturnsBody": "T_ : array, shape=(n_samples,)   The transformed data  ", "methodParams": ["T"], "methodReturns": ["T_"], "methodDesc": "Transform new data by linear interpolation", "methodParamsBody": "T : array-like, shape=(n_samples,)   Data to transform.  "}], "allFuncParams": ["y_min", "y_max", "increasing", "out_of_bounds"], "notes": "Ties are broken using the secondary method from Leeuw, 1977. Isotonic Median Regression: A Linear Programming Approach Nilotpal Chakravarti Mathematics of Operations Research Vol. 14, No. 2 (May, 1989), pp. 303-308 Isotone Optimization in R : Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods Leeuw, Hornik, Mair Journal of Statistical Software 2009 Correctness of Kruskals algorithms for monotone regression with ties Leeuw, Psychometrica, 1977 X is stored for future use, as transform needs X to interpolate new input data. Ties are broken using the secondary method from Leeuw, 1977. Isotonic Median Regression: A Linear Programming Approach Nilotpal Chakravarti Mathematics of Operations Research Vol. 14, No. 2 (May, 1989), pp. 303-308 Isotone Optimization in R : Pool-Adjacent-Violators Algorithm (PAVA) and Active Set Methods Leeuw, Hornik, Mair Journal of Statistical Software 2009 Correctness of Kruskals algorithms for monotone regression with ties Leeuw, Psychometrica, 1977 X is stored for future use, as transform needs X to interpolate new input data.", "funcName": "IsotonicRegression", "allFuncAttributes": ["X_min_", "X_max_", "f_"], "funcDesc": "Isotonic regression model.", "funcParamBody": "y_min : optional, default: None If not None, set the lowest value of the fit to y_min. y_max : optional, default: None If not None, set the highest value of the fit to y_max. increasing : boolean or string, optional, default: True If boolean, whether or not to fit the isotonic regression with y increasing or decreasing. The string value auto determines whether y should increase or decrease based on the Spearman correlation estimates sign. out_of_bounds : string, optional, default: nan The out_of_bounds parameter handles how x-values outside of the training domain are handled.  When set to nan, predicted y-values will be NaN.  When set to clip, predicted y-values will be set to the value corresponding to the nearest train interval endpoint. When set to raise, allow interp1d to throw ValueError.", "funcAttrBody": "X_min_ : float Minimum value of input array X_ for left bound. X_max_ : float Maximum value of input array X_ for right bound. f_ : function The stepwise interpolating function that covers the domain X_ ."},
{"libName": "sklearn.gaussian_process.kernels", "methods": [{"methodName": "__init__(noise_level=1.0, noise_level_bounds=(1e-05, 100000.0))", "methodDesc": ""}, {"methodName": "__call__(X, Y=None, eval_gradient=False)", "methodReturnsBody": "K : array, shape (n_samples_X, n_samples_Y)   Kernel k(X, Y)   K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)   The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.  ", "methodParams": ["X", "Y", "eval_gradient"], "methodReturns": ["K", "K_gradient"], "methodDesc": "Return the kernel k(X, Y) and optionally its gradient.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)   Y : array, shape (n_samples_Y, n_features), (optional, default=None)   Right argument of the returned kernel k(X, Y). If None, k(X, X) if evaluated instead.   eval_gradient : bool (optional, default=False)   Determines whether the gradient with respect to the kernel hyperparameter is determined. Only supported when Y is None.  "}, {"methodName": "clone_with_theta(theta)", "methodDesc": "Returns a clone of self with given hyperparameters theta."}, {"methodName": "diag(X)", "methodReturnsBody": "K_diag : array, shape (n_samples_X,)   Diagonal of kernel k(X, X)  ", "methodParams": ["X"], "methodReturns": ["K_diag"], "methodDesc": "Returns the diagonal of the kernel k(X, X). The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters of this kernel.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "is_stationary()", "methodDesc": "Returns whether the kernel is stationary."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this kernel. The method works on simple kernels as well as on nested kernels. The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "", "funcName": "WhiteKernel", "allFuncParams": ["noise_level", "noise_level_bounds"], "funcDesc": "White kernel.", "funcParamBody": "noise_level : float, default: 1.0 Parameter controlling the noise level noise_level_bounds : pair of floats >= 0, default: (1e-5, 1e5) The lower and upper bound on noise_level"},
{"libName": "sklearn.gaussian_process.kernels", "methods": [{"methodName": "__init__(k1, k2)", "methodDesc": ""}, {"methodName": "__call__(X, Y=None, eval_gradient=False)", "methodReturnsBody": "K : array, shape (n_samples_X, n_samples_Y)   Kernel k(X, Y)   K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)   The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.  ", "methodParams": ["X", "Y", "eval_gradient"], "methodReturns": ["K", "K_gradient"], "methodDesc": "Return the kernel k(X, Y) and optionally its gradient.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)   Y : array, shape (n_samples_Y, n_features), (optional, default=None)   Right argument of the returned kernel k(X, Y). If None, k(X, X) if evaluated instead.   eval_gradient : bool (optional, default=False)   Determines whether the gradient with respect to the kernel hyperparameter is determined.  "}, {"methodName": "clone_with_theta(theta)", "methodDesc": "Returns a clone of self with given hyperparameters theta."}, {"methodName": "diag(X)", "methodReturnsBody": "K_diag : array, shape (n_samples_X,)   Diagonal of kernel k(X, X)  ", "methodParams": ["X"], "methodReturns": ["K_diag"], "methodDesc": "Returns the diagonal of the kernel k(X, X). The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters of this kernel.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "is_stationary()", "methodDesc": "Returns whether the kernel is stationary."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this kernel. The method works on simple kernels as well as on nested kernels. The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "", "funcName": "Sum", "allFuncParams": ["k1", "k2"], "funcDesc": "Sum-kernel k1 + k2 of two kernels k1 and k2.", "funcParamBody": "k1 : Kernel object The first base-kernel of the sum-kernel k2 : Kernel object The second base-kernel of the sum-kernel"},
{"libName": "sklearn.gaussian_process.kernels", "methods": [{"methodName": "__init__(length_scale=1.0, alpha=1.0, length_scale_bounds=(1e-05, 100000.0), alpha_bounds=(1e-05, 100000.0))", "methodDesc": ""}, {"methodName": "__call__(X, Y=None, eval_gradient=False)", "methodReturnsBody": "K : array, shape (n_samples_X, n_samples_Y)   Kernel k(X, Y)   K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)   The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.  ", "methodParams": ["X", "Y", "eval_gradient"], "methodReturns": ["K", "K_gradient"], "methodDesc": "Return the kernel k(X, Y) and optionally its gradient.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)   Y : array, shape (n_samples_Y, n_features), (optional, default=None)   Right argument of the returned kernel k(X, Y). If None, k(X, X) if evaluated instead.   eval_gradient : bool (optional, default=False)   Determines whether the gradient with respect to the kernel hyperparameter is determined. Only supported when Y is None.  "}, {"methodName": "clone_with_theta(theta)", "methodDesc": "Returns a clone of self with given hyperparameters theta."}, {"methodName": "diag(X)", "methodReturnsBody": "K_diag : array, shape (n_samples_X,)   Diagonal of kernel k(X, X)  ", "methodParams": ["X"], "methodReturns": ["K_diag"], "methodDesc": "Returns the diagonal of the kernel k(X, X). The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters of this kernel.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "is_stationary()", "methodDesc": "Returns whether the kernel is stationary."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this kernel. The method works on simple kernels as well as on nested kernels. The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "", "funcName": "RationalQuadratic", "allFuncParams": ["length_scale", "alpha", "length_scale_bounds", "alpha_bounds"], "funcDesc": "Rational Quadratic kernel.", "funcParamBody": "length_scale : float > 0, default: 1.0 The length scale of the kernel. alpha : float > 0, default: 1.0 Scale mixture parameter length_scale_bounds : pair of floats >= 0, default: (1e-5, 1e5) The lower and upper bound on length_scale alpha_bounds : pair of floats >= 0, default: (1e-5, 1e5) The lower and upper bound on alpha"},
{"libName": "sklearn.gaussian_process.kernels", "methods": [{"methodName": "__init__(length_scale=1.0, length_scale_bounds=(1e-05, 100000.0))", "methodDesc": ""}, {"methodName": "__call__(X, Y=None, eval_gradient=False)", "methodReturnsBody": "K : array, shape (n_samples_X, n_samples_Y)   Kernel k(X, Y)   K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)   The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.  ", "methodParams": ["X", "Y", "eval_gradient"], "methodReturns": ["K", "K_gradient"], "methodDesc": "Return the kernel k(X, Y) and optionally its gradient.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)   Y : array, shape (n_samples_Y, n_features), (optional, default=None)   Right argument of the returned kernel k(X, Y). If None, k(X, X) if evaluated instead.   eval_gradient : bool (optional, default=False)   Determines whether the gradient with respect to the kernel hyperparameter is determined. Only supported when Y is None.  "}, {"methodName": "clone_with_theta(theta)", "methodDesc": "Returns a clone of self with given hyperparameters theta."}, {"methodName": "diag(X)", "methodReturnsBody": "K_diag : array, shape (n_samples_X,)   Diagonal of kernel k(X, X)  ", "methodParams": ["X"], "methodReturns": ["K_diag"], "methodDesc": "Returns the diagonal of the kernel k(X, X). The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters of this kernel.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "is_stationary()", "methodDesc": "Returns whether the kernel is stationary."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this kernel. The method works on simple kernels as well as on nested kernels. The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "", "funcName": "RBF", "allFuncParams": ["length_scale", "length_scale_bounds"], "funcDesc": "Radial-basis function kernel (aka squared-exponential kernel).", "funcParamBody": "length_scale : float or array with shape (n_features,), default: 1.0 The length scale of the kernel. If a float, an isotropic kernel is used. If an array, an anisotropic kernel is used where each dimension of l defines the length-scale of the respective feature dimension. length_scale_bounds : pair of floats >= 0, default: (1e-5, 1e5) The lower and upper bound on length_scale"},
{"libName": "sklearn.gaussian_process.kernels", "methods": [{"methodName": "__init__(k1, k2)", "methodDesc": ""}, {"methodName": "__call__(X, Y=None, eval_gradient=False)", "methodReturnsBody": "K : array, shape (n_samples_X, n_samples_Y)   Kernel k(X, Y)   K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)   The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.  ", "methodParams": ["X", "Y", "eval_gradient"], "methodReturns": ["K", "K_gradient"], "methodDesc": "Return the kernel k(X, Y) and optionally its gradient.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)   Y : array, shape (n_samples_Y, n_features), (optional, default=None)   Right argument of the returned kernel k(X, Y). If None, k(X, X) if evaluated instead.   eval_gradient : bool (optional, default=False)   Determines whether the gradient with respect to the kernel hyperparameter is determined.  "}, {"methodName": "clone_with_theta(theta)", "methodDesc": "Returns a clone of self with given hyperparameters theta."}, {"methodName": "diag(X)", "methodReturnsBody": "K_diag : array, shape (n_samples_X,)   Diagonal of kernel k(X, X)  ", "methodParams": ["X"], "methodReturns": ["K_diag"], "methodDesc": "Returns the diagonal of the kernel k(X, X). The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters of this kernel.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "is_stationary()", "methodDesc": "Returns whether the kernel is stationary."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this kernel. The method works on simple kernels as well as on nested kernels. The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "", "funcName": "Product", "allFuncParams": ["k1", "k2"], "funcDesc": "Product-kernel k1 * k2 of two kernels k1 and k2.", "funcParamBody": "k1 : Kernel object The first base-kernel of the product-kernel k2 : Kernel object The second base-kernel of the product-kernel"},
{"libName": "sklearn.gaussian_process.kernels", "methods": [{"methodName": "__init__(gamma=1.0, gamma_bounds=(1e-05, 100000.0), metric=linear, pairwise_kernels_kwargs=None)", "methodDesc": ""}, {"methodName": "__call__(X, Y=None, eval_gradient=False)", "methodReturnsBody": "K : array, shape (n_samples_X, n_samples_Y)   Kernel k(X, Y)   K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)   The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.  ", "methodParams": ["X", "Y", "eval_gradient"], "methodReturns": ["K", "K_gradient"], "methodDesc": "Return the kernel k(X, Y) and optionally its gradient.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)   Y : array, shape (n_samples_Y, n_features), (optional, default=None)   Right argument of the returned kernel k(X, Y). If None, k(X, X) if evaluated instead.   eval_gradient : bool (optional, default=False)   Determines whether the gradient with respect to the kernel hyperparameter is determined. Only supported when Y is None.  "}, {"methodName": "clone_with_theta(theta)", "methodDesc": "Returns a clone of self with given hyperparameters theta."}, {"methodName": "diag(X)", "methodReturnsBody": "K_diag : array, shape (n_samples_X,)   Diagonal of kernel k(X, X)  ", "methodParams": ["X"], "methodReturns": ["K_diag"], "methodDesc": "Returns the diagonal of the kernel k(X, X). The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters of this kernel.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "is_stationary()", "methodDesc": "Returns whether the kernel is stationary."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this kernel. The method works on simple kernels as well as on nested kernels. The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "", "funcName": "PairwiseKernel", "allFuncParams": ["gamma: float >= 0, default: 1.0", "gamma_bounds", "metric", "pairwise_kernels_kwargs"], "funcDesc": "Wrapper for kernels in sklearn.metrics.pairwise.", "funcParamBody": "gamma: float >= 0, default: 1.0 : Parameter gamma of the pairwise kernel specified by metric gamma_bounds : pair of floats >= 0, default: (1e-5, 1e5) The lower and upper bound on gamma metric : string, or callable, default: linear The metric to use when calculating kernel between instances in a feature array. If metric is a string, it must be one of the metrics in pairwise.PAIRWISE_KERNEL_FUNCTIONS. If metric is precomputed, X is assumed to be a kernel matrix. Alternatively, if metric is a callable function, it is called on each pair of instances (rows) and the resulting value recorded. The callable should take two arrays from X as input and return a value indicating the distance between them. pairwise_kernels_kwargs : dict, default: None All entries of this dict (if any) are passed as keyword arguments to the pairwise kernel function."},
{"libName": "sklearn.gaussian_process.kernels", "methods": [{"methodName": "__init__(length_scale=1.0, length_scale_bounds=(1e-05, 100000.0), nu=1.5)", "methodDesc": ""}, {"methodName": "__call__(X, Y=None, eval_gradient=False)", "methodReturnsBody": "K : array, shape (n_samples_X, n_samples_Y)   Kernel k(X, Y)   K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)   The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.  ", "methodParams": ["X", "Y", "eval_gradient"], "methodReturns": ["K", "K_gradient"], "methodDesc": "Return the kernel k(X, Y) and optionally its gradient.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)   Y : array, shape (n_samples_Y, n_features), (optional, default=None)   Right argument of the returned kernel k(X, Y). If None, k(X, X) if evaluated instead.   eval_gradient : bool (optional, default=False)   Determines whether the gradient with respect to the kernel hyperparameter is determined. Only supported when Y is None.  "}, {"methodName": "clone_with_theta(theta)", "methodDesc": "Returns a clone of self with given hyperparameters theta."}, {"methodName": "diag(X)", "methodReturnsBody": "K_diag : array, shape (n_samples_X,)   Diagonal of kernel k(X, X)  ", "methodParams": ["X"], "methodReturns": ["K_diag"], "methodDesc": "Returns the diagonal of the kernel k(X, X). The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters of this kernel.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "is_stationary()", "methodDesc": "Returns whether the kernel is stationary."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this kernel. The method works on simple kernels as well as on nested kernels. The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "", "funcName": "Matern", "allFuncParams": ["length_scale", "length_scale_bounds", "nu: float, default: 1.5"], "funcDesc": "Matern kernel.", "funcParamBody": "length_scale : float or array with shape (n_features,), default: 1.0 The length scale of the kernel. If a float, an isotropic kernel is used. If an array, an anisotropic kernel is used where each dimension of l defines the length-scale of the respective feature dimension. length_scale_bounds : pair of floats >= 0, default: (1e-5, 1e5) The lower and upper bound on length_scale nu: float, default: 1.5 : The parameter nu controlling the smoothness of the learned function. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions). Note that values of nu not in [0.5, 1.5, 2.5, inf] incur a considerably higher computational cost (appr. 10 times higher) since they require to evaluate the modified Bessel function. Furthermore, in contrast to l, nu is kept fixed to its initial value and not optimized."},
{"funcName": "Kernel", "notes": "", "libName": "sklearn.gaussian_process.kernels", "methods": [{"methodName": "__init__()", "methodDesc": "Initialize self.  See help(type(self)) for accurate signature."}, {"methodName": "__call__(X, Y=None, eval_gradient=False)", "methodDesc": "Evaluate the kernel."}, {"methodName": "clone_with_theta(theta)", "methodDesc": "Returns a clone of self with given hyperparameters theta."}, {"methodName": "diag(X)", "methodReturnsBody": "K_diag : array, shape (n_samples_X,)   Diagonal of kernel k(X, X)  ", "methodParams": ["X"], "methodReturns": ["K_diag"], "methodDesc": "Returns the diagonal of the kernel k(X, X). The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters of this kernel.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "is_stationary()", "methodDesc": "Returns whether the kernel is stationary."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this kernel. The method works on simple kernels as well as on nested kernels. The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "Base class for all kernels."},
{"libName": "sklearn.gaussian_process.kernels", "methods": [{"methodName": "__init__()", "methodDesc": "Initialize self.  See help(type(self)) for accurate signature."}, {"methodName": "__call__(*args, **kwargs)", "methodDesc": "Call self as a function."}, {"methodName": "count(value)  integer  return number of occurrences of value", "methodDesc": ""}, {"methodName": "index(value[, start[, stop]])  integer  return first index of value.", "methodDesc": "Raises ValueError if the value is not present."}], "notes": "", "funcName": "Hyperparameter", "allFuncAttributes": ["name", "value_type", "bounds", "n_elements", "fixed"], "funcDesc": "A kernel hyperparameters specification in form of a namedtuple.", "funcAttrBody": "name : string The name of the hyperparameter. Note that a kernel using a hyperparameter with name x must have the attributes self.x and self.x_bounds value_type : string The type of the hyperparameter. Currently, only numeric hyperparameters are supported. bounds : pair of floats >= 0 or fixed The lower and upper bound on the parameter. If n_elements>1, a pair of 1d array with n_elements each may be given alternatively. If the string fixed is passed as bounds, the hyperparameters value cannot be changed. n_elements : int, default=1 The number of elements of the hyperparameter value. Defaults to 1, which corresponds to a scalar hyperparameter. n_elements > 1 corresponds to a hyperparameter which is vector-valued, such as, e.g., anisotropic length-scales. fixed : bool, default: None Whether the value of this hyperparameter is fixed, i.e., cannot be changed during hyperparameter tuning. If None is passed, the fixed is derived based on the given bounds."},
{"libName": "sklearn.gaussian_process.kernels", "methods": [{"methodName": "__init__(kernel, exponent)", "methodDesc": ""}, {"methodName": "__call__(X, Y=None, eval_gradient=False)", "methodReturnsBody": "K : array, shape (n_samples_X, n_samples_Y)   Kernel k(X, Y)   K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)   The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.  ", "methodParams": ["X", "Y", "eval_gradient"], "methodReturns": ["K", "K_gradient"], "methodDesc": "Return the kernel k(X, Y) and optionally its gradient.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)   Y : array, shape (n_samples_Y, n_features), (optional, default=None)   Right argument of the returned kernel k(X, Y). If None, k(X, X) if evaluated instead.   eval_gradient : bool (optional, default=False)   Determines whether the gradient with respect to the kernel hyperparameter is determined.  "}, {"methodName": "clone_with_theta(theta)", "methodDesc": "Returns a clone of self with given hyperparameters theta."}, {"methodName": "diag(X)", "methodReturnsBody": "K_diag : array, shape (n_samples_X,)   Diagonal of kernel k(X, X)  ", "methodParams": ["X"], "methodReturns": ["K_diag"], "methodDesc": "Returns the diagonal of the kernel k(X, X). The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters of this kernel.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "is_stationary()", "methodDesc": "Returns whether the kernel is stationary."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this kernel. The method works on simple kernels as well as on nested kernels. The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "", "funcName": "Exponentiation", "allFuncParams": ["kernel", "exponent"], "funcDesc": "Exponentiate kernel by given exponent.", "funcParamBody": "kernel : Kernel object The base kernel exponent : float The exponent for the base kernel"},
{"libName": "sklearn.gaussian_process.kernels", "methods": [{"methodName": "__init__(length_scale=1.0, periodicity=1.0, length_scale_bounds=(1e-05, 100000.0), periodicity_bounds=(1e-05, 100000.0))", "methodDesc": ""}, {"methodName": "__call__(X, Y=None, eval_gradient=False)", "methodReturnsBody": "K : array, shape (n_samples_X, n_samples_Y)   Kernel k(X, Y)   K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)   The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.  ", "methodParams": ["X", "Y", "eval_gradient"], "methodReturns": ["K", "K_gradient"], "methodDesc": "Return the kernel k(X, Y) and optionally its gradient.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)   Y : array, shape (n_samples_Y, n_features), (optional, default=None)   Right argument of the returned kernel k(X, Y). If None, k(X, X) if evaluated instead.   eval_gradient : bool (optional, default=False)   Determines whether the gradient with respect to the kernel hyperparameter is determined. Only supported when Y is None.  "}, {"methodName": "clone_with_theta(theta)", "methodDesc": "Returns a clone of self with given hyperparameters theta."}, {"methodName": "diag(X)", "methodReturnsBody": "K_diag : array, shape (n_samples_X,)   Diagonal of kernel k(X, X)  ", "methodParams": ["X"], "methodReturns": ["K_diag"], "methodDesc": "Returns the diagonal of the kernel k(X, X). The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters of this kernel.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "is_stationary()", "methodDesc": "Returns whether the kernel is stationary."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this kernel. The method works on simple kernels as well as on nested kernels. The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "", "funcName": "ExpSineSquared", "allFuncParams": ["length_scale", "periodicity", "length_scale_bounds", "periodicity_bounds"], "funcDesc": "Exp-Sine-Squared kernel.", "funcParamBody": "length_scale : float > 0, default: 1.0 The length scale of the kernel. periodicity : float > 0, default: 1.0 The periodicity of the kernel. length_scale_bounds : pair of floats >= 0, default: (1e-5, 1e5) The lower and upper bound on length_scale periodicity_bounds : pair of floats >= 0, default: (1e-5, 1e5) The lower and upper bound on periodicity"},
{"libName": "sklearn.gaussian_process.kernels", "methods": [{"methodName": "__init__(sigma_0=1.0, sigma_0_bounds=(1e-05, 100000.0))", "methodDesc": ""}, {"methodName": "__call__(X, Y=None, eval_gradient=False)", "methodReturnsBody": "K : array, shape (n_samples_X, n_samples_Y)   Kernel k(X, Y)   K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)   The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.  ", "methodParams": ["X", "Y", "eval_gradient"], "methodReturns": ["K", "K_gradient"], "methodDesc": "Return the kernel k(X, Y) and optionally its gradient.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)   Y : array, shape (n_samples_Y, n_features), (optional, default=None)   Right argument of the returned kernel k(X, Y). If None, k(X, X) if evaluated instead.   eval_gradient : bool (optional, default=False)   Determines whether the gradient with respect to the kernel hyperparameter is determined. Only supported when Y is None.  "}, {"methodName": "clone_with_theta(theta)", "methodDesc": "Returns a clone of self with given hyperparameters theta."}, {"methodName": "diag(X)", "methodReturnsBody": "K_diag : array, shape (n_samples_X,)   Diagonal of kernel k(X, X)  ", "methodParams": ["X"], "methodReturns": ["K_diag"], "methodDesc": "Returns the diagonal of the kernel k(X, X). The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters of this kernel.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "is_stationary()", "methodDesc": "Returns whether the kernel is stationary."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this kernel. The method works on simple kernels as well as on nested kernels. The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "", "funcName": "DotProduct", "allFuncParams": ["sigma_0", "sigma_0_bounds"], "funcDesc": "Dot-Product kernel.", "funcParamBody": "sigma_0 : float >= 0, default: 1.0 Parameter controlling the inhomogenity of the kernel. If sigma_0=0, the kernel is homogenous. sigma_0_bounds : pair of floats >= 0, default: (1e-5, 1e5) The lower and upper bound on l"},
{"libName": "sklearn.gaussian_process.kernels", "methods": [{"methodName": "__init__(constant_value=1.0, constant_value_bounds=(1e-05, 100000.0))", "methodDesc": ""}, {"methodName": "__call__(X, Y=None, eval_gradient=False)", "methodReturnsBody": "K : array, shape (n_samples_X, n_samples_Y)   Kernel k(X, Y)   K_gradient : array (opt.), shape (n_samples_X, n_samples_X, n_dims)   The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.  ", "methodParams": ["X", "Y", "eval_gradient"], "methodReturns": ["K", "K_gradient"], "methodDesc": "Return the kernel k(X, Y) and optionally its gradient.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)   Y : array, shape (n_samples_Y, n_features), (optional, default=None)   Right argument of the returned kernel k(X, Y). If None, k(X, X) if evaluated instead.   eval_gradient : bool (optional, default=False)   Determines whether the gradient with respect to the kernel hyperparameter is determined. Only supported when Y is None.  "}, {"methodName": "clone_with_theta(theta)", "methodDesc": "Returns a clone of self with given hyperparameters theta."}, {"methodName": "diag(X)", "methodReturnsBody": "K_diag : array, shape (n_samples_X,)   Diagonal of kernel k(X, X)  ", "methodParams": ["X"], "methodReturns": ["K_diag"], "methodDesc": "Returns the diagonal of the kernel k(X, X). The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters of this kernel.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "is_stationary()", "methodDesc": "Returns whether the kernel is stationary."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this kernel. The method works on simple kernels as well as on nested kernels. The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "notes": "", "funcName": "ConstantKernel", "allFuncParams": ["constant_value", "constant_value_bounds"], "funcDesc": "Constant kernel.", "funcParamBody": "constant_value : float, default: 1.0 The constant value which defines the covariance: k(x_1, x_2) = constant_value constant_value_bounds : pair of floats >= 0, default: (1e-5, 1e5) The lower and upper bound on constant_value"},
{"funcName": "CompoundKernel", "notes": "", "libName": "sklearn.gaussian_process.kernels", "methods": [{"methodName": "__init__(kernels)", "methodDesc": ""}, {"methodName": "__call__(X, Y=None, eval_gradient=False)", "methodReturnsBody": "K : array, shape (n_samples_X, n_samples_Y, n_kernels)   Kernel k(X, Y)   K_gradient : array, shape (n_samples_X, n_samples_X, n_dims, n_kernels)   The gradient of the kernel k(X, X) with respect to the hyperparameter of the kernel. Only returned when eval_gradient is True.  ", "methodParams": ["X", "Y", "eval_gradient"], "methodReturns": ["K", "K_gradient"], "methodDesc": "Return the kernel k(X, Y) and optionally its gradient. Note that this compound kernel returns the results of all simple kernel stacked along an additional axis.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)   Y : array, shape (n_samples_Y, n_features), (optional, default=None)   Right argument of the returned kernel k(X, Y). If None, k(X, X) if evaluated instead.   eval_gradient : bool (optional, default=False)   Determines whether the gradient with respect to the kernel hyperparameter is determined.  "}, {"methodName": "clone_with_theta(theta)", "methodDesc": "Returns a clone of self with given hyperparameters theta."}, {"methodName": "diag(X)", "methodReturnsBody": "K_diag : array, shape (n_samples_X, n_kernels)   Diagonal of kernel k(X, X)  ", "methodParams": ["X"], "methodReturns": ["K_diag"], "methodDesc": "Returns the diagonal of the kernel k(X, X). The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated.", "methodParamsBody": "X : array, shape (n_samples_X, n_features)   Left argument of the returned kernel k(X, Y)  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters of this kernel.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "is_stationary()", "methodDesc": "Returns whether the kernel is stationary."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this kernel. The method works on simple kernels as well as on nested kernels. The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "Kernel which is composed of a set of other kernels."},
{"libName": "sklearn.gaussian_process", "methods": [{"methodName": "__init__(kernel=None, alpha=1e-10, optimizer=fmin_l_bfgs_b, n_restarts_optimizer=0, normalize_y=False, copy_X_train=True, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit Gaussian process regression model.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Training data   y : array-like, shape = (n_samples, [n_output_dims])   Target values  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "log_marginal_likelihood(theta=None, eval_gradient=False)", "methodReturnsBody": "log_likelihood : float   Log-marginal likelihood of theta for training data.   log_likelihood_gradient : array, shape = (n_kernel_params,), optional   Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta. Only returned when eval_gradient is True.  ", "methodParams": ["theta", "eval_gradient"], "methodReturns": ["log_likelihood", "log_likelihood_gradient"], "methodDesc": "Returns log-marginal likelihood of theta for training data.", "methodParamsBody": "theta : array-like, shape = (n_kernel_params,) or None   Kernel hyperparameters for which the log-marginal likelihood is evaluated. If None, the precomputed log_marginal_likelihood of self.kernel_.theta is returned.   eval_gradient : bool, default: False   If True, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. If True, theta must not be None.  "}, {"methodName": "predict(X, return_std=False, return_cov=False)", "methodReturnsBody": "y_mean : array, shape = (n_samples, [n_output_dims])   Mean of predictive distribution a query points   y_std : array, shape = (n_samples,), optional   Standard deviation of predictive distribution at query points. Only returned when return_std is True.   y_cov : array, shape = (n_samples, n_samples), optional   Covariance of joint predictive distribution a query points. Only returned when return_cov is True.  ", "methodParams": ["X", "return_std", "return_cov"], "methodReturns": ["y_mean", "y_std", "y_cov"], "methodDesc": "Predict using the Gaussian process regression model We can also predict based on an unfitted model by using the GP prior. In addition to the mean of the predictive distribution, also its standard deviation (return_std=True) or covariance (return_cov=True). Note that at most one of the two can be requested.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Query points where the GP is evaluated   return_std : bool, default: False   If True, the standard-deviation of the predictive distribution at the query points is returned along with the mean.   return_cov : bool, default: False   If True, the covariance of the joint predictive distribution at the query points is returned along with the mean  "}, {"methodName": "sample_y(X, n_samples=1, random_state=0)", "methodReturnsBody": "y_samples : array, shape = (n_samples_X, [n_output_dims], n_samples)   Values of n_samples samples drawn from Gaussian process and evaluated at query points.  ", "methodParams": ["X", "n_samples", "random_state"], "methodReturns": ["y_samples"], "methodDesc": "Draw samples from Gaussian process and evaluate at X.", "methodParamsBody": "X : array-like, shape = (n_samples_X, n_features)   Query points where the GP samples are evaluated   n_samples : int, default: 1   The number of samples drawn from the Gaussian process   random_state : int, RandomState instance or None, optional (default=0)   If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["kernel", "alpha", "optimizer", "n_restarts_optimizer", "normalize_y", "copy_X_train", "random_state"], "notes": "", "funcName": "GaussianProcessRegressor", "allFuncAttributes": ["X_train_", "y_train_", "kernel_", "L_", "alpha_", "log_marginal_likelihood_value_"], "funcDesc": "Gaussian process regression (GPR).", "funcParamBody": "kernel : kernel object The kernel specifying the covariance function of the GP. If None is passed, the kernel 1.0 * RBF(1.0) is used as default. Note that the kernels hyperparameters are optimized during fitting. alpha : float or array-like, optional (default: 1e-10) Value added to the diagonal of the kernel matrix during fitting. Larger values correspond to increased noise level in the observations. This can also prevent a potential numerical issue during fitting, by ensuring that the calculated values form a positive definite matrix. If an array is passed, it must have the same number of entries as the data used for fitting and is used as datapoint-dependent noise level. Note that this is equivalent to adding a WhiteKernel with c=alpha. Allowing to specify the noise level directly as a parameter is mainly for convenience and for consistency with Ridge. optimizer : string or callable, optional (default: fmin_l_bfgs_b) Can either be one of the internally supported optimizers for optimizing the kernels parameters, specified by a string, or an externally defined optimizer passed as a callable. If a callable is passed, it must have the signature: def  optimizer ( obj_func ,  initial_theta ,  bounds ): # * 'obj_func' is the objective function to be maximized, which #   takes the hyperparameters theta as parameter and an #   optional flag eval_gradient, which determines if the #   gradient is returned additionally to the function value # * 'initial_theta': the initial value for theta, which can be #   used by local optimizers # * 'bounds': the bounds on the values of theta .... # Returned are the best found hyperparameters theta and # the corresponding value of the target function. return  theta_opt ,  func_min Per default, the fmin_l_bfgs_b algorithm from scipy.optimize is used. If None is passed, the kernels parameters are kept fixed. Available internal optimizers are: 'fmin_l_bfgs_b' n_restarts_optimizer : int, optional (default: 0) The number of restarts of the optimizer for finding the kernels parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernels initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer == 0 implies that one run is performed. normalize_y : boolean, optional (default: False) Whether the target values y are normalized, i.e., the mean of the observed target values become zero. This parameter should be set to True if the target values mean is expected to differ considerable from zero. When enabled, the normalization effectively modifies the GPs prior based on the data, which contradicts the likelihood principle; normalization is thus disabled per default. copy_X_train : bool, optional (default: True) If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally. random_state : int, RandomState instance or None, optional (default: None) The generator used to initialize the centers. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcAttrBody": "X_train_ : array-like, shape = (n_samples, n_features) Feature values in training data (also required for prediction) y_train_ : array-like, shape = (n_samples, [n_output_dims]) Target values in training data (also required for prediction) kernel_ : kernel object The kernel used for prediction. The structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters L_ : array-like, shape = (n_samples, n_samples) Lower-triangular Cholesky decomposition of the kernel in X_train_ alpha_ : array-like, shape = (n_samples,) Dual coefficients of training data points in kernel space log_marginal_likelihood_value_ : float The log-marginal-likelihood of self.kernel_.theta"},
{"libName": "sklearn.gaussian_process", "methods": [{"methodName": "__init__(kernel=None, optimizer=fmin_l_bfgs_b, n_restarts_optimizer=0, max_iter_predict=100, warm_start=False, copy_X_train=True, random_state=None, multi_class=one_vs_rest, n_jobs=1)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : returns an instance of self. ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit Gaussian process classification model", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Training data   y : array-like, shape = (n_samples,)   Target values, must be binary  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "log_marginal_likelihood(theta=None, eval_gradient=False)", "methodReturnsBody": "log_likelihood : float   Log-marginal likelihood of theta for training data.   log_likelihood_gradient : array, shape = (n_kernel_params,), optional   Gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta. Only returned when eval_gradient is True.  ", "methodParams": ["theta", "eval_gradient"], "methodReturns": ["log_likelihood", "log_likelihood_gradient"], "methodDesc": "Returns log-marginal likelihood of theta for training data. In the case of multi-class classification, the mean log-marginal likelihood of the one-versus-rest classifiers are returned.", "methodParamsBody": "theta : array-like, shape = (n_kernel_params,) or none   Kernel hyperparameters for which the log-marginal likelihood is evaluated. In the case of multi-class classification, theta may be the  hyperparameters of the compound kernel or of an individual kernel. In the latter case, all individual kernel get assigned the same theta values. If None, the precomputed log_marginal_likelihood of self.kernel_.theta is returned.   eval_gradient : bool, default: False   If True, the gradient of the log-marginal likelihood with respect to the kernel hyperparameters at position theta is returned additionally. Note that gradient computation is not supported for non-binary classification. If True, theta must not be None.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = (n_samples,)   Predicted target values for X, values are from classes_  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Perform classification on an array of test vectors X.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features) "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "C : array-like, shape = (n_samples, n_classes)   Returns the probability of the samples for each class in the model. The columns correspond to the classes in sorted order, as they appear in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Return probability estimates for the test vector X.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features) "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["kernel", "optimizer", "n_restarts_optimizer", "max_iter_predict", "warm_start", "copy_X_train", "random_state", "multi_class", "n_jobs"], "notes": "", "funcName": "GaussianProcessClassifier", "allFuncAttributes": ["kernel_", "log_marginal_likelihood_value_", "classes_", "n_classes_", ".. versionadded:: 0.18"], "funcDesc": "Gaussian process classification (GPC) based on Laplace approximation.", "funcParamBody": "kernel : kernel object The kernel specifying the covariance function of the GP. If None is passed, the kernel 1.0 * RBF(1.0) is used as default. Note that the kernels hyperparameters are optimized during fitting. optimizer : string or callable, optional (default: fmin_l_bfgs_b) Can either be one of the internally supported optimizers for optimizing the kernels parameters, specified by a string, or an externally defined optimizer passed as a callable. If a callable is passed, it must have the  signature: def  optimizer ( obj_func ,  initial_theta ,  bounds ): # * 'obj_func' is the objective function to be maximized, which #   takes the hyperparameters theta as parameter and an #   optional flag eval_gradient, which determines if the #   gradient is returned additionally to the function value # * 'initial_theta': the initial value for theta, which can be #   used by local optimizers # * 'bounds': the bounds on the values of theta .... # Returned are the best found hyperparameters theta and # the corresponding value of the target function. return  theta_opt ,  func_min Per default, the fmin_l_bfgs_b algorithm from scipy.optimize is used. If None is passed, the kernels parameters are kept fixed. Available internal optimizers are: 'fmin_l_bfgs_b' n_restarts_optimizer : int, optional (default: 0) The number of restarts of the optimizer for finding the kernels parameters which maximize the log-marginal likelihood. The first run of the optimizer is performed from the kernels initial parameters, the remaining ones (if any) from thetas sampled log-uniform randomly from the space of allowed theta-values. If greater than 0, all bounds must be finite. Note that n_restarts_optimizer=0 implies that one run is performed. max_iter_predict : int, optional (default: 100) The maximum number of iterations in Newtons method for approximating the posterior during predict. Smaller values will reduce computation time at the cost of worse results. warm_start : bool, optional (default: False) If warm-starts are enabled, the solution of the last Newton iteration on the Laplace approximation of the posterior mode is used as initialization for the next call of _posterior_mode(). This can speed up convergence when _posterior_mode is called several times on similar problems as in hyperparameter optimization. copy_X_train : bool, optional (default: True) If True, a persistent copy of the training data is stored in the object. Otherwise, just a reference to the training data is stored, which might cause predictions to change if the data is modified externally. random_state : int, RandomState instance or None, optional (default: None) The generator used to initialize the centers. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . multi_class : string, default Specifies how multi-class classification problems are handled. Supported are one_vs_rest and one_vs_one. In one_vs_rest, one binary Gaussian process classifier is fitted for each class, which is trained to separate this class from the rest. In one_vs_one, one binary Gaussian process classifier is fitted for each pair of classes, which is trained to separate these two classes. The predictions of these binary predictors are combined into multi-class predictions. Note that one_vs_one does not support predicting probability estimates. n_jobs : int, optional, default: 1 The number of jobs to use for the computation. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.", "funcAttrBody": "kernel_ : kernel object The kernel used for prediction. In case of binary classification, the structure of the kernel is the same as the one passed as parameter but with optimized hyperparameters. In case of multi-class classification, a CompoundKernel is returned which consists of the different kernels used in the one-versus-rest classifiers. log_marginal_likelihood_value_ : float The log-marginal-likelihood of self.kernel_.theta classes_ : array-like, shape = (n_classes,) Unique class labels. n_classes_ : int The number of classes in the training data .. versionadded:: 0.18 :"},
{"allReturnParams": ["mi"], "libName": "sklearn.feature_selection", "methods": [], "notes": "", "funcName": "mutual_info_regression", "allFuncParams": ["X", "y", "discrete_features", "n_neighbors", "copy", "random_state"], "funcDesc": "Estimate mutual information for a continuous target variable.", "funcParamBody": "X : array_like or sparse matrix, shape (n_samples, n_features) Feature matrix. y : array_like, shape (n_samples,) Target vector. discrete_features : {auto, bool, array_like}, default auto If bool, then determines whether to consider all features discrete or continuous. If array, then it should be either a boolean mask with shape (n_features,) or array with indices of discrete features. If auto, it is assigned to False for dense X and to True for sparse X . n_neighbors : int, default 3 Number of neighbors to use for MI estimation for continuous variables, see [R177] and [R178] . Higher values reduce variance of the estimation, but could introduce a bias. copy : bool, default True Whether to make a copy of the given data. If set to False, the initial data will be overwritten. random_state : int, RandomState instance or None, optional, default None The seed of the pseudo random number generator for adding small noise to continuous variables in order to remove repeated values. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "mi : ndarray, shape (n_features,) Estimated mutual information between each feature and the target."},
{"allReturnParams": ["mi"], "libName": "sklearn.feature_selection", "methods": [], "notes": "", "funcName": "mutual_info_classif", "allFuncParams": ["X", "y", "discrete_features", "n_neighbors", "copy", "random_state"], "funcDesc": "Estimate mutual information for a discrete target variable.", "funcParamBody": "X : array_like or sparse matrix, shape (n_samples, n_features) Feature matrix. y : array_like, shape (n_samples,) Target vector. discrete_features : {auto, bool, array_like}, default auto If bool, then determines whether to consider all features discrete or continuous. If array, then it should be either a boolean mask with shape (n_features,) or array with indices of discrete features. If auto, it is assigned to False for dense X and to True for sparse X . n_neighbors : int, default 3 Number of neighbors to use for MI estimation for continuous variables, see [R173] and [R174] . Higher values reduce variance of the estimation, but could introduce a bias. copy : bool, default True Whether to make a copy of the given data. If set to False, the initial data will be overwritten. random_state : int, RandomState instance or None, optional, default None The seed of the pseudo random number generator for adding small noise to continuous variables in order to remove repeated values.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "mi : ndarray, shape (n_features,) Estimated mutual information between each feature and the target."},
{"allReturnParams": ["F", "pval"], "libName": "sklearn.feature_selection", "methods": [], "notes": "", "funcName": "f_regression", "allFuncParams": ["X", "y", "center"], "funcDesc": "Univariate linear regression tests.", "funcParamBody": "X : {array-like, sparse matrix}  shape = (n_samples, n_features) The set of regressors that will be tested sequentially. y : array of shape(n_samples). The data matrix center : True, bool, If true, X and y will be centered.", "funcReturnBody": "F : array, shape=(n_features,) F values of features. pval : array, shape=(n_features,) p-values of F-scores."},
{"allReturnParams": ["F", "pval"], "libName": "sklearn.feature_selection", "methods": [], "notes": "", "funcName": "f_classif", "allFuncParams": ["X", "y"], "funcDesc": "Compute the ANOVA F-value for the provided sample.", "funcParamBody": "X : {array-like, sparse matrix} shape = [n_samples, n_features] The set of regressors that will be tested sequentially. y : array of shape(n_samples) The data matrix.", "funcReturnBody": "F : array, shape = [n_features,] The set of F values. pval : array, shape = [n_features,] The set of p-values."},
{"allReturnParams": ["chi2", "pval"], "libName": "sklearn.feature_selection", "methods": [], "notes": "Complexity of this algorithm is O(n_classes * n_features). Complexity of this algorithm is O(n_classes * n_features).", "funcName": "chi2", "allFuncParams": ["X", "y"], "funcDesc": "Compute chi-squared stats between each non-negative feature and class.", "funcParamBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features_in) Sample vectors. y : array-like, shape = (n_samples,) Target vector (class labels).", "funcReturnBody": "chi2 : array, shape = (n_features,) chi2 statistics of each feature. pval : array, shape = (n_features,) p-values of each feature."},
{"libName": "sklearn.feature_selection", "methods": [{"methodName": "__init__(threshold=0.0)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Learn empirical variances from X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Sample vectors from which to compute variances.   y : any   Ignored. This parameter exists only for compatibility with sklearn.pipeline.Pipeline.  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_support(indices=False)", "methodReturnsBody": "support : array   An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.  ", "methodParams": ["indices"], "methodReturns": ["support"], "methodDesc": "Get a mask, or integer index, of the features selected", "methodParamsBody": "indices : boolean (default False)   If True, the return value will be an array of integers, rather than a boolean mask.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_original_features]   X with columns of zeros inserted where features would have been removed by transform .  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reverse the transformation operation", "methodParamsBody": "X : array of shape [n_samples, n_selected_features]   The input samples.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_selected_features]   The input samples with only the selected features.  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reduce X to the selected features.", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.  "}], "allFuncParams": ["threshold"], "notes": "", "funcName": "VarianceThreshold", "allFuncAttributes": ["variances_"], "funcDesc": "Feature selector that removes all low-variance features.", "funcParamBody": "threshold : float, optional Features with a training-set variance lower than this threshold will be removed. The default is to keep all features with non-zero variance, i.e. remove the features that have the same value in all samples.", "funcAttrBody": "variances_ : array, shape (n_features,) Variances of individual features."},
{"funcName": "RFECV", "notes": "The size of grid_scores_ is equal to ceil((n_features - 1) / step) + 1, where step is the number of features removed at each iteration. The following example shows how to retrieve the a-priori not known 5 informative features in the Friedman #1 dataset. The size of grid_scores_ is equal to ceil((n_features - 1) / step) + 1, where step is the number of features removed at each iteration. The following example shows how to retrieve the a-priori not known 5 informative features in the Friedman #1 dataset.", "libName": "sklearn.feature_selection", "methods": [{"methodName": "__init__(estimator, step=1, cv=None, scoring=None, verbose=0, n_jobs=1)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training vector, where n_samples is the number of samples and n_features is the total number of features.   y : array-like, shape = [n_samples]   Target values (integers for classification, real numbers for regression).  ", "methodDesc": ""}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_support(indices=False)", "methodReturnsBody": "support : array   An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.  ", "methodParams": ["indices"], "methodReturns": ["support"], "methodDesc": "Get a mask, or integer index, of the features selected", "methodParamsBody": "indices : boolean (default False)   If True, the return value will be an array of integers, rather than a boolean mask.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_original_features]   X with columns of zeros inserted where features would have been removed by transform .  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reverse the transformation operation", "methodParamsBody": "X : array of shape [n_samples, n_selected_features]   The input samples.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of shape [n_samples]   The predicted target values.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.  "}, {"methodParams": ["X", "y"], "methodName": "score(X, y)", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.   y : array of shape [n_samples]   The target values.  ", "methodDesc": ""}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_selected_features]   The input samples with only the selected features.  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reduce X to the selected features.", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.  "}], "funcDesc": "Feature ranking with recursive feature elimination and cross-validated selection of the best number of features."},
{"funcName": "RFE", "notes": "", "libName": "sklearn.feature_selection", "methods": [{"methodName": "__init__(estimator, n_features_to_select=None, step=1, verbose=0)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   The training input samples.   y : array-like, shape = [n_samples]   The target values.  ", "methodDesc": ""}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_support(indices=False)", "methodReturnsBody": "support : array   An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.  ", "methodParams": ["indices"], "methodReturns": ["support"], "methodDesc": "Get a mask, or integer index, of the features selected", "methodParamsBody": "indices : boolean (default False)   If True, the return value will be an array of integers, rather than a boolean mask.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_original_features]   X with columns of zeros inserted where features would have been removed by transform .  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reverse the transformation operation", "methodParamsBody": "X : array of shape [n_samples, n_selected_features]   The input samples.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of shape [n_samples]   The predicted target values.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.  "}, {"methodParams": ["X", "y"], "methodName": "score(X, y)", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.   y : array of shape [n_samples]   The target values.  ", "methodDesc": ""}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_selected_features]   The input samples with only the selected features.  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reduce X to the selected features.", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.  "}], "funcDesc": "Feature ranking with recursive feature elimination."},
{"libName": "sklearn.feature_selection", "methods": [{"methodName": "__init__(score_func=<function f_classif>, alpha=0.05)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Run score function on (X, y) and get the appropriate features.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   The training input samples.   y : array-like, shape = [n_samples]   The target values (class labels in classification, real numbers in regression).  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_support(indices=False)", "methodReturnsBody": "support : array   An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.  ", "methodParams": ["indices"], "methodReturns": ["support"], "methodDesc": "Get a mask, or integer index, of the features selected", "methodParamsBody": "indices : boolean (default False)   If True, the return value will be an array of integers, rather than a boolean mask.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_original_features]   X with columns of zeros inserted where features would have been removed by transform .  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reverse the transformation operation", "methodParamsBody": "X : array of shape [n_samples, n_selected_features]   The input samples.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_selected_features]   The input samples with only the selected features.  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reduce X to the selected features.", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.  "}], "allFuncParams": ["score_func", "alpha"], "notes": "", "funcName": "SelectFwe", "allFuncAttributes": ["scores_", "pvalues_"], "funcDesc": "Filter: Select the p-values corresponding to Family-wise error rate", "funcParamBody": "score_func : callable Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below See also). The default function only works with classification tasks. alpha : float, optional The highest uncorrected p-value for features to keep.", "funcAttrBody": "scores_ : array-like, shape=(n_features,) Scores of features. pvalues_ : array-like, shape=(n_features,) p-values of feature scores."},
{"libName": "sklearn.feature_selection", "methods": [{"methodName": "__init__(estimator, threshold=None, prefit=False, norm_order=1)", "methodDesc": ""}, {"methodName": "fit(X, y=None, **fit_params)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "**fit_params"], "methodReturns": ["self"], "methodDesc": "Fit the SelectFromModel meta-transformer.", "methodParamsBody": "X : array-like of shape (n_samples, n_features)   The training input samples.   y : array-like, shape (n_samples,)   The target values (integers that correspond to classes in classification, real numbers in regression).   **fit_params : Other estimator specific parameters "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_support(indices=False)", "methodReturnsBody": "support : array   An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.  ", "methodParams": ["indices"], "methodReturns": ["support"], "methodDesc": "Get a mask, or integer index, of the features selected", "methodParamsBody": "indices : boolean (default False)   If True, the return value will be an array of integers, rather than a boolean mask.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_original_features]   X with columns of zeros inserted where features would have been removed by transform .  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reverse the transformation operation", "methodParamsBody": "X : array of shape [n_samples, n_selected_features]   The input samples.  "}, {"methodName": "partial_fit(X, y=None, **fit_params)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "**fit_params"], "methodReturns": ["self"], "methodDesc": "Fit the SelectFromModel meta-transformer only once.", "methodParamsBody": "X : array-like of shape (n_samples, n_features)   The training input samples.   y : array-like, shape (n_samples,)   The target values (integers that correspond to classes in classification, real numbers in regression).   **fit_params : Other estimator specific parameters "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_selected_features]   The input samples with only the selected features.  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reduce X to the selected features.", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.  "}], "allFuncParams": ["estimator", "threshold", "prefit", "norm_order"], "notes": "", "funcName": "SelectFromModel", "allFuncAttributes": ["estimator_", "threshold_"], "funcDesc": "Meta-transformer for selecting features based on importance weights.", "funcParamBody": "estimator : object The base estimator from which the transformer is built. This can be both a fitted (if prefit is set to True) or a non-fitted estimator. The estimator must have either a feature_importances_ or coef_ attribute after fitting. threshold : string, float, optional default None The threshold value to use for feature selection. Features whose importance is greater or equal are kept while the others are discarded. If median (resp. mean), then the threshold value is the median (resp. the mean) of the feature importances. A scaling factor (e.g., 1.25*mean) may also be used. If None and if the estimator has a parameter penalty set to l1, either explicitly or implicitly (e.g, Lasso), the threshold used is 1e-5. Otherwise, mean is used by default. prefit : bool, default False Whether a prefit model is expected to be passed into the constructor directly or not. If True, transform must be called directly and SelectFromModel cannot be used with cross_val_score , GridSearchCV and similar utilities that clone the estimator. Otherwise train the model using fit and then transform to do feature selection. norm_order : non-zero int, inf, -inf, default 1 Order of the norm used to filter the vectors of coefficients below threshold in the case where the coef_ attribute of the estimator is of dimension 2.", "funcAttrBody": "estimator_ : an estimator The base estimator from which the transformer is built. This is stored only when a non-fitted estimator is passed to the SelectFromModel , i.e when prefit is False. threshold_ : float The threshold value used for feature selection."},
{"libName": "sklearn.feature_selection", "methods": [{"methodName": "__init__(score_func=<function f_classif>, alpha=0.05)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Run score function on (X, y) and get the appropriate features.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   The training input samples.   y : array-like, shape = [n_samples]   The target values (class labels in classification, real numbers in regression).  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_support(indices=False)", "methodReturnsBody": "support : array   An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.  ", "methodParams": ["indices"], "methodReturns": ["support"], "methodDesc": "Get a mask, or integer index, of the features selected", "methodParamsBody": "indices : boolean (default False)   If True, the return value will be an array of integers, rather than a boolean mask.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_original_features]   X with columns of zeros inserted where features would have been removed by transform .  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reverse the transformation operation", "methodParamsBody": "X : array of shape [n_samples, n_selected_features]   The input samples.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_selected_features]   The input samples with only the selected features.  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reduce X to the selected features.", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.  "}], "allFuncParams": ["score_func", "alpha"], "notes": "", "funcName": "SelectFdr", "allFuncAttributes": ["scores_", "pvalues_"], "funcDesc": "Filter: Select the p-values for an estimated false discovery rate", "funcParamBody": "score_func : callable Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below See also). The default function only works with classification tasks. alpha : float, optional The highest uncorrected p-value for features to keep.", "funcAttrBody": "scores_ : array-like, shape=(n_features,) Scores of features. pvalues_ : array-like, shape=(n_features,) p-values of feature scores."},
{"libName": "sklearn.feature_selection", "methods": [{"methodName": "__init__(score_func=<function f_classif>, alpha=0.05)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Run score function on (X, y) and get the appropriate features.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   The training input samples.   y : array-like, shape = [n_samples]   The target values (class labels in classification, real numbers in regression).  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_support(indices=False)", "methodReturnsBody": "support : array   An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.  ", "methodParams": ["indices"], "methodReturns": ["support"], "methodDesc": "Get a mask, or integer index, of the features selected", "methodParamsBody": "indices : boolean (default False)   If True, the return value will be an array of integers, rather than a boolean mask.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_original_features]   X with columns of zeros inserted where features would have been removed by transform .  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reverse the transformation operation", "methodParamsBody": "X : array of shape [n_samples, n_selected_features]   The input samples.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_selected_features]   The input samples with only the selected features.  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reduce X to the selected features.", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.  "}], "allFuncParams": ["score_func", "alpha"], "notes": "", "funcName": "SelectFpr", "allFuncAttributes": ["scores_", "pvalues_"], "funcDesc": "Filter: Select the pvalues below alpha based on a FPR test.", "funcParamBody": "score_func : callable Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). Default is f_classif (see below See also). The default function only works with classification tasks. alpha : float, optional The highest p-value for features to be kept.", "funcAttrBody": "scores_ : array-like, shape=(n_features,) Scores of features. pvalues_ : array-like, shape=(n_features,) p-values of feature scores."},
{"libName": "sklearn.feature_selection", "methods": [{"methodName": "__init__(score_func=<function f_classif>, k=10)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Run score function on (X, y) and get the appropriate features.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   The training input samples.   y : array-like, shape = [n_samples]   The target values (class labels in classification, real numbers in regression).  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_support(indices=False)", "methodReturnsBody": "support : array   An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.  ", "methodParams": ["indices"], "methodReturns": ["support"], "methodDesc": "Get a mask, or integer index, of the features selected", "methodParamsBody": "indices : boolean (default False)   If True, the return value will be an array of integers, rather than a boolean mask.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_original_features]   X with columns of zeros inserted where features would have been removed by transform .  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reverse the transformation operation", "methodParamsBody": "X : array of shape [n_samples, n_selected_features]   The input samples.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_selected_features]   The input samples with only the selected features.  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reduce X to the selected features.", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.  "}], "allFuncParams": ["score_func", "k"], "notes": "Ties between features with equal scores will be broken in an unspecified way. Ties between features with equal scores will be broken in an unspecified way.", "funcName": "SelectKBest", "allFuncAttributes": ["scores_", "pvalues_"], "funcDesc": "Select features according to the k highest scores.", "funcParamBody": "score_func : callable Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below See also). The default function only works with classification tasks. k : int or all, optional, default=10 Number of top features to select. The all option bypasses selection, for use in a parameter search.", "funcAttrBody": "scores_ : array-like, shape=(n_features,) Scores of features. pvalues_ : array-like, shape=(n_features,) p-values of feature scores, None if score_func returned only scores."},
{"libName": "sklearn.feature_selection", "methods": [{"methodName": "__init__(score_func=<function f_classif>, percentile=10)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Run score function on (X, y) and get the appropriate features.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   The training input samples.   y : array-like, shape = [n_samples]   The target values (class labels in classification, real numbers in regression).  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_support(indices=False)", "methodReturnsBody": "support : array   An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.  ", "methodParams": ["indices"], "methodReturns": ["support"], "methodDesc": "Get a mask, or integer index, of the features selected", "methodParamsBody": "indices : boolean (default False)   If True, the return value will be an array of integers, rather than a boolean mask.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_original_features]   X with columns of zeros inserted where features would have been removed by transform .  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reverse the transformation operation", "methodParamsBody": "X : array of shape [n_samples, n_selected_features]   The input samples.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_selected_features]   The input samples with only the selected features.  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reduce X to the selected features.", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.  "}], "allFuncParams": ["score_func", "percentile"], "notes": "Ties between features with equal scores will be broken in an unspecified way. Ties between features with equal scores will be broken in an unspecified way.", "funcName": "SelectPercentile", "allFuncAttributes": ["scores_", "pvalues_"], "funcDesc": "Select features according to a percentile of the highest scores.", "funcParamBody": "score_func : callable Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) or a single array with scores. Default is f_classif (see below See also). The default function only works with classification tasks. percentile : int, optional, default=10 Percent of features to keep.", "funcAttrBody": "scores_ : array-like, shape=(n_features,) Scores of features. pvalues_ : array-like, shape=(n_features,) p-values of feature scores, None if score_func returned only scores."},
{"libName": "sklearn.feature_selection", "methods": [{"methodName": "__init__(score_func=<function f_classif>, mode=percentile, param=1e-05)", "methodDesc": ""}, {"methodName": "fit(X, y)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Run score function on (X, y) and get the appropriate features.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   The training input samples.   y : array-like, shape = [n_samples]   The target values (class labels in classification, real numbers in regression).  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_support(indices=False)", "methodReturnsBody": "support : array   An index that selects the retained features from a feature vector. If indices is False, this is a boolean array of shape [# input features], in which an element is True iff its corresponding feature is selected for retention. If indices is True, this is an integer array of shape [# output features] whose values are indices into the input feature vector.  ", "methodParams": ["indices"], "methodReturns": ["support"], "methodDesc": "Get a mask, or integer index, of the features selected", "methodParamsBody": "indices : boolean (default False)   If True, the return value will be an array of integers, rather than a boolean mask.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_original_features]   X with columns of zeros inserted where features would have been removed by transform .  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reverse the transformation operation", "methodParamsBody": "X : array of shape [n_samples, n_selected_features]   The input samples.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_r : array of shape [n_samples, n_selected_features]   The input samples with only the selected features.  ", "methodParams": ["X"], "methodReturns": ["X_r"], "methodDesc": "Reduce X to the selected features.", "methodParamsBody": "X : array of shape [n_samples, n_features]   The input samples.  "}], "allFuncParams": ["score_func", "mode", "param"], "notes": "", "funcName": "GenericUnivariateSelect", "allFuncAttributes": ["scores_", "pvalues_"], "funcDesc": "Univariate feature selector with configurable strategy.", "funcParamBody": "score_func : callable Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues). For modes percentile or kbest it can return a single array scores. mode : {percentile, k_best, fpr, fdr, fwe} Feature selection mode. param : float or int depending on the feature selection mode Parameter of the corresponding mode.", "funcAttrBody": "scores_ : array-like, shape=(n_features,) Scores of features. pvalues_ : array-like, shape=(n_features,) p-values of feature scores, None if score_func returned scores only."},
{"libName": "sklearn.feature_extraction.text", "methods": [{"methodName": "__init__(input=content, encoding=utf-8, decode_error=strict, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, analyzer=word, stop_words=None, token_pattern=(?u)\\b\\w\\w+\\b, ngram_range=(1, 1), max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class numpy.int64>, norm=l2, use_idf=True, smooth_idf=True, sublinear_tf=False)", "methodDesc": ""}, {"methodName": "build_analyzer()", "methodDesc": "Return a callable that handles preprocessing and tokenization"}, {"methodName": "build_preprocessor()", "methodDesc": "Return a function to preprocess the text before tokenization"}, {"methodName": "build_tokenizer()", "methodDesc": "Return a function that splits a string into a sequence of tokens"}, {"methodName": "decode(doc)", "methodDesc": "Decode the input into a string of unicode symbols The decoding strategy depends on the vectorizer parameters."}, {"methodName": "fit(raw_documents, y=None)", "methodReturnsBody": "self : TfidfVectorizer ", "methodParams": ["raw_documents"], "methodReturns": ["self"], "methodDesc": "Learn vocabulary and idf from training set.", "methodParamsBody": "raw_documents : iterable   an iterable which yields either str, unicode or file objects  "}, {"methodName": "fit_transform(raw_documents, y=None)", "methodReturnsBody": "X : sparse matrix, [n_samples, n_features]   Tf-idf-weighted document-term matrix.  ", "methodParams": ["raw_documents"], "methodReturns": ["X"], "methodDesc": "Learn vocabulary and idf, return term-document matrix. This is equivalent to fit followed by transform, but more efficiently implemented.", "methodParamsBody": "raw_documents : iterable   an iterable which yields either str, unicode or file objects  "}, {"methodName": "get_feature_names()", "methodDesc": "Array mapping from feature integer indices to feature name"}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_stop_words()", "methodDesc": "Build or fetch the effective stop words list"}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_inv : list of arrays, len = n_samples   List of arrays of terms.  ", "methodParams": ["X"], "methodReturns": ["X_inv"], "methodDesc": "Return terms per document with nonzero entries in X.", "methodParamsBody": "X : {array, sparse matrix}, shape = [n_samples, n_features] "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(raw_documents, copy=True)", "methodReturnsBody": "X : sparse matrix, [n_samples, n_features]   Tf-idf-weighted document-term matrix.  ", "methodParams": ["raw_documents", "copy"], "methodReturns": ["X"], "methodDesc": "Transform documents to document-term matrix. Uses the vocabulary and document frequencies (df) learned by fit (or fit_transform).", "methodParamsBody": "raw_documents : iterable   an iterable which yields either str, unicode or file objects   copy : boolean, default True   Whether to copy X and operate on the copy or perform in-place operations.  "}], "allFuncParams": ["input", "encoding", "decode_error", "strip_accents", "analyzer", "preprocessor", "tokenizer", "ngram_range", "stop_words", "lowercase", "token_pattern", "max_df", "min_df", "max_features", "vocabulary", "binary", "dtype", "norm", "use_idf", "smooth_idf", "sublinear_tf"], "notes": "The stop_words_ attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling. The stop_words_ attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.", "funcName": "TfidfVectorizer", "allFuncAttributes": ["vocabulary_", "idf_", "stop_words_"], "funcDesc": "Convert a collection of raw documents to a matrix of TF-IDF features.", "funcParamBody": "input : string {filename, file, content} If filename, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze. If file, the sequence items must have a read method (file-like object) that is called to fetch the bytes in memory. Otherwise the input is expected to be the sequence strings or bytes items are expected to be analyzed directly. encoding : string, utf-8 by default. If bytes or files are given to analyze, this encoding is used to decode. decode_error : {strict, ignore, replace} Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding . By default, it is strict, meaning that a UnicodeDecodeError will be raised. Other values are ignore and replace. strip_accents : {ascii, unicode, None} Remove accents during the preprocessing step. ascii is a fast method that only works on characters that have an direct ASCII mapping. unicode is a slightly slower method that works on any characters. None (default) does nothing. analyzer : string, {word, char} or callable Whether the feature should be made of word or character n-grams. If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. preprocessor : callable or None (default) Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. tokenizer : callable or None (default) Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer  ==  'word' . ngram_range : tuple (min_n, max_n) The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. stop_words : string {english}, list, or None (default) If a string, it is passed to _check_stop_list and the appropriate stop list is returned. english is currently the only supported string value. If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer  ==  'word' . If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. lowercase : boolean, default True Convert all characters to lowercase before tokenizing. token_pattern : string Regular expression denoting what constitutes a token, only used if analyzer  ==  'word' . The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). max_df : float in range [0.0, 1.0] or int, default=1.0 When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. min_df : float in range [0.0, 1.0] or int, default=1 When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. max_features : int or None, default=None If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. This parameter is ignored if vocabulary is not None. vocabulary : Mapping or iterable, optional Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. binary : boolean, default=False If True, all non-zero term counts are set to 1. This does not mean outputs will have only 0/1 values, only that the tf term in tf-idf is binary. (Set idf and normalization to False to get 0/1 outputs.) dtype : type, optional Type of the matrix returned by fit_transform() or transform(). norm : l1, l2 or None, optional Norm used to normalize term vectors. None for no normalization. use_idf : boolean, default=True Enable inverse-document-frequency reweighting. smooth_idf : boolean, default=True Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions. sublinear_tf : boolean, default=False Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).", "funcAttrBody": "vocabulary_ : dict A mapping of terms to feature indices. idf_ : array, shape = [n_features], or None The learned idf vector (global term weights) when use_idf is set to True, None otherwise. stop_words_ : set Terms that were ignored because they either: occurred in too many documents ( max_df ) occurred in too few documents ( min_df ) were cut off by feature selection ( max_features ). This is only available if no vocabulary was given."},
{"funcName": "TfidfTransformer", "notes": "", "libName": "sklearn.feature_extraction.text", "methods": [{"methodName": "__init__(norm=l2, use_idf=True, smooth_idf=True, sublinear_tf=False)", "methodDesc": ""}, {"methodParams": ["X"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : sparse matrix, [n_samples, n_features]   a matrix of term/token counts  ", "methodDesc": "Learn the idf vector (global term weights)"}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X, copy=True)", "methodReturnsBody": "vectors : sparse matrix, [n_samples, n_features] ", "methodParams": ["X", "copy"], "methodReturns": ["vectors"], "methodDesc": "Transform a count matrix to a tf or tf-idf representation", "methodParamsBody": "X : sparse matrix, [n_samples, n_features]   a matrix of term/token counts   copy : boolean, default True   Whether to copy X and operate on the copy or perform in-place operations.  "}], "funcDesc": "Transform a count matrix to a normalized tf or tf-idf representation"},
{"libName": "sklearn.feature_extraction.text", "methods": [{"methodName": "__init__(input=content, encoding=utf-8, decode_error=strict, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=(?u)\\b\\w\\w+\\b, ngram_range=(1, 1), analyzer=word, n_features=1048576, binary=False, norm=l2, alternate_sign=True, non_negative=False, dtype=<class numpy.float64>)", "methodDesc": ""}, {"methodName": "build_analyzer()", "methodDesc": "Return a callable that handles preprocessing and tokenization"}, {"methodName": "build_preprocessor()", "methodDesc": "Return a function to preprocess the text before tokenization"}, {"methodName": "build_tokenizer()", "methodDesc": "Return a function that splits a string into a sequence of tokens"}, {"methodName": "decode(doc)", "methodDesc": "Decode the input into a string of unicode symbols The decoding strategy depends on the vectorizer parameters."}, {"methodName": "fit(X, y=None)", "methodDesc": "Does nothing: this transformer is stateless."}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_stop_words()", "methodDesc": "Build or fetch the effective stop words list"}, {"methodName": "partial_fit(X, y=None)", "methodDesc": "Does nothing: this transformer is stateless. This method is just there to mark the fact that this transformer can work in a streaming setup."}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X : scipy.sparse matrix, shape = (n_samples, self.n_features)   Document-term matrix.  ", "methodParams": ["X"], "methodReturns": ["X"], "methodDesc": "Transform a sequence of documents to a document-term matrix.", "methodParamsBody": "X : iterable over raw text documents, length = n_samples   Samples. Each sample must be a text document (either bytes or unicode strings, file name or file object depending on the constructor argument) which will be tokenized and hashed.  "}], "notes": "", "funcName": "HashingVectorizer", "allFuncParams": ["input", "encoding", "decode_error", "strip_accents", "analyzer", "preprocessor", "tokenizer", "ngram_range", "stop_words", "lowercase", "token_pattern", "n_features", "norm", "binary", "dtype", "alternate_sign", "non_negative"], "funcDesc": "Convert a collection of text documents to a matrix of token occurrences", "funcParamBody": "input : string {filename, file, content} If filename, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze. If file, the sequence items must have a read method (file-like object) that is called to fetch the bytes in memory. Otherwise the input is expected to be the sequence strings or bytes items are expected to be analyzed directly. encoding : string, default=utf-8 If bytes or files are given to analyze, this encoding is used to decode. decode_error : {strict, ignore, replace} Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding . By default, it is strict, meaning that a UnicodeDecodeError will be raised. Other values are ignore and replace. strip_accents : {ascii, unicode, None} Remove accents during the preprocessing step. ascii is a fast method that only works on characters that have an direct ASCII mapping. unicode is a slightly slower method that works on any characters. None (default) does nothing. analyzer : string, {word, char, char_wb} or callable Whether the feature should be made of word or character n-grams. Option char_wb creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space. If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. preprocessor : callable or None (default) Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. tokenizer : callable or None (default) Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer  ==  'word' . ngram_range : tuple (min_n, max_n), default=(1, 1) The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. stop_words : string {english}, list, or None (default) If english, a built-in stop word list for English is used. If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer  ==  'word' . lowercase : boolean, default=True Convert all characters to lowercase before tokenizing. token_pattern : string Regular expression denoting what constitutes a token, only used if analyzer  ==  'word' . The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). n_features : integer, default=(2 ** 20) The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners. norm : l1, l2 or None, optional Norm used to normalize term vectors. None for no normalization. binary : boolean, default=False. If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. dtype : type, optional Type of the matrix returned by fit_transform() or transform(). alternate_sign : boolean, optional, default True When True, an alternating sign is added to the features as to approximately conserve the inner product in the hashed space even for small n_features. This approach is similar to sparse random projection. New in version 0.19. non_negative : boolean, optional, default False When True, an absolute value is applied to the features matrix prior to returning it. When used in conjunction with alternate_sign=True, this significantly reduces the inner product preservation property. Deprecated since version 0.19: This option will be removed in 0.21."},
{"libName": "sklearn.feature_extraction.text", "methods": [{"methodName": "__init__(input=content, encoding=utf-8, decode_error=strict, strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=(?u)\\b\\w\\w+\\b, ngram_range=(1, 1), analyzer=word, max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class numpy.int64>)", "methodDesc": ""}, {"methodName": "build_analyzer()", "methodDesc": "Return a callable that handles preprocessing and tokenization"}, {"methodName": "build_preprocessor()", "methodDesc": "Return a function to preprocess the text before tokenization"}, {"methodName": "build_tokenizer()", "methodDesc": "Return a function that splits a string into a sequence of tokens"}, {"methodName": "decode(doc)", "methodDesc": "Decode the input into a string of unicode symbols The decoding strategy depends on the vectorizer parameters."}, {"methodName": "fit(raw_documents, y=None)", "methodReturnsBody": "self : ", "methodParams": ["raw_documents"], "methodReturns": ["self"], "methodDesc": "Learn a vocabulary dictionary of all tokens in the raw documents.", "methodParamsBody": "raw_documents : iterable   An iterable which yields either str, unicode or file objects.  "}, {"methodName": "fit_transform(raw_documents, y=None)", "methodReturnsBody": "X : array, [n_samples, n_features]   Document-term matrix.  ", "methodParams": ["raw_documents"], "methodReturns": ["X"], "methodDesc": "Learn the vocabulary dictionary and return term-document matrix. This is equivalent to fit followed by transform, but more efficiently implemented.", "methodParamsBody": "raw_documents : iterable   An iterable which yields either str, unicode or file objects.  "}, {"methodName": "get_feature_names()", "methodDesc": "Array mapping from feature integer indices to feature name"}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_stop_words()", "methodDesc": "Build or fetch the effective stop words list"}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_inv : list of arrays, len = n_samples   List of arrays of terms.  ", "methodParams": ["X"], "methodReturns": ["X_inv"], "methodDesc": "Return terms per document with nonzero entries in X.", "methodParamsBody": "X : {array, sparse matrix}, shape = [n_samples, n_features] "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(raw_documents)", "methodReturnsBody": "X : sparse matrix, [n_samples, n_features]   Document-term matrix.  ", "methodParams": ["raw_documents"], "methodReturns": ["X"], "methodDesc": "Transform documents to document-term matrix. Extract token counts out of raw text documents using the vocabulary fitted with fit or the one provided to the constructor.", "methodParamsBody": "raw_documents : iterable   An iterable which yields either str, unicode or file objects.  "}], "allFuncParams": ["input", "encoding", "decode_error", "strip_accents", "analyzer", "preprocessor", "tokenizer", "ngram_range", "stop_words", "lowercase", "token_pattern", "max_df", "min_df", "max_features", "vocabulary", "binary", "dtype"], "notes": "The stop_words_ attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling. The stop_words_ attribute can get large and increase the model size when pickling. This attribute is provided only for introspection and can be safely removed using delattr or set to None before pickling.", "funcName": "CountVectorizer", "allFuncAttributes": ["vocabulary_", "stop_words_"], "funcDesc": "Convert a collection of text documents to a matrix of token counts", "funcParamBody": "input : string {filename, file, content} If filename, the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze. If file, the sequence items must have a read method (file-like object) that is called to fetch the bytes in memory. Otherwise the input is expected to be the sequence strings or bytes items are expected to be analyzed directly. encoding : string, utf-8 by default. If bytes or files are given to analyze, this encoding is used to decode. decode_error : {strict, ignore, replace} Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding . By default, it is strict, meaning that a UnicodeDecodeError will be raised. Other values are ignore and replace. strip_accents : {ascii, unicode, None} Remove accents during the preprocessing step. ascii is a fast method that only works on characters that have an direct ASCII mapping. unicode is a slightly slower method that works on any characters. None (default) does nothing. analyzer : string, {word, char, char_wb} or callable Whether the feature should be made of word or character n-grams. Option char_wb creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space. If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input. preprocessor : callable or None (default) Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. tokenizer : callable or None (default) Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if analyzer  ==  'word' . ngram_range : tuple (min_n, max_n) The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. stop_words : string {english}, list, or None (default) If english, a built-in stop word list for English is used. If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if analyzer  ==  'word' . If None, no stop words will be used. max_df can be set to a value in the range [0.7, 1.0) to automatically detect and filter stop words based on intra corpus document frequency of terms. lowercase : boolean, True by default Convert all characters to lowercase before tokenizing. token_pattern : string Regular expression denoting what constitutes a token, only used if analyzer  ==  'word' . The default regexp select tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator). max_df : float in range [0.0, 1.0] or int, default=1.0 When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. min_df : float in range [0.0, 1.0] or int, default=1 When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None. max_features : int or None, default=None If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. This parameter is ignored if vocabulary is not None. vocabulary : Mapping or iterable, optional Either a Mapping (e.g., a dict) where keys are terms and values are indices in the feature matrix, or an iterable over terms. If not given, a vocabulary is determined from the input documents. Indices in the mapping should not be repeated and should not have any gap between 0 and the largest index. binary : boolean, default=False If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts. dtype : type, optional Type of the matrix returned by fit_transform() or transform().", "funcAttrBody": "vocabulary_ : dict A mapping of terms to feature indices. stop_words_ : set Terms that were ignored because they either: occurred in too many documents ( max_df ) occurred in too few documents ( min_df ) were cut off by feature selection ( max_features ). This is only available if no vocabulary was given."},
{"libName": "sklearn.feature_extraction.image", "methods": [{"methodName": "__init__(patch_size=None, max_patches=None, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodDesc": "Do nothing and return the estimator unchanged This method is just there to implement the usual API and hence work in pipelines."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "patches : array, shape = (n_patches, patch_height, patch_width) or   (n_patches, patch_height, patch_width, n_channels) The collection of patches extracted from the images, where n_patches is either n_samples * max_patches or the total number of patches that can be extracted.  ", "methodParams": ["X"], "methodReturns": ["patches"], "methodDesc": "Transforms the image samples in X into a matrix of patch data.", "methodParamsBody": "X : array, shape = (n_samples, image_height, image_width) or   (n_samples, image_height, image_width, n_channels) Array of images from which to extract patches. For color images, the last dimension specifies the channel: a RGB image would have n_channels=3 .  "}], "notes": "", "funcName": "PatchExtractor", "allFuncParams": ["patch_size", "max_patches", "random_state"], "funcDesc": "Extracts patches from a collection of images", "funcParamBody": "patch_size : tuple of ints (patch_height, patch_width) the dimensions of one patch max_patches : integer or float, optional default is None The maximum number of patches per image to extract. If max_patches is a float in (0, 1), it is taken to mean a proportion of the total number of patches. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random ."},
{"allReturnParams": ["image"], "libName": "sklearn.feature_extraction.image", "methods": [], "notes": "", "funcName": "reconstruct_from_patches_2d", "allFuncParams": ["patches", "image_size"], "funcDesc": "Reconstruct the image from all of its patches.", "funcParamBody": "patches : array, shape = (n_patches, patch_height, patch_width) or (n_patches, patch_height, patch_width, n_channels) The complete set of patches. If the patches contain colour information, channels are indexed along the last dimension: RGB patches would have n_channels=3 . image_size : tuple of ints (image_height, image_width) or (image_height, image_width, n_channels) the size of the image that will be reconstructed", "funcReturnBody": "image : array, shape = image_size the reconstructed image"},
{"libName": "sklearn.feature_extraction.image", "methods": [], "notes": "For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was handled by returning a dense np.matrix instance.  Going forward, np.ndarray returns an np.ndarray, as expected. For compatibility, user code relying on this method should wrap its calls in np.asarray to avoid type issues.", "funcName": "img_to_graph", "allFuncParams": ["img", "mask", "return_as", "dtype"], "funcDesc": "Graph of the pixel-to-pixel gradient connections", "funcParamBody": "img : ndarray, 2D or 3D 2D or 3D image mask : ndarray of booleans, optional An optional mask of the image, to consider only part of the pixels. return_as : np.ndarray or a sparse matrix class, optional The class to use to build the returned adjacency matrix. dtype : None or dtype, optional The data of the returned sparse matrix. By default it is the dtype of img"},
{"libName": "sklearn.feature_extraction.image", "methods": [], "notes": "For scikit-learn versions 0.14.1 and prior, return_as=np.ndarray was handled by returning a dense np.matrix instance.  Going forward, np.ndarray returns an np.ndarray, as expected. For compatibility, user code relying on this method should wrap its calls in np.asarray to avoid type issues.", "funcName": "grid_to_graph", "allFuncParams": ["n_x", "n_y", "n_z", "mask", "return_as", "dtype"], "funcDesc": "Graph of the pixel-to-pixel connections", "funcParamBody": "n_x : int Dimension in x axis n_y : int Dimension in y axis n_z : int, optional, default 1 Dimension in z axis mask : ndarray of booleans, optional An optional mask of the image, to consider only part of the pixels. return_as : np.ndarray or a sparse matrix class, optional The class to use to build the returned adjacency matrix. dtype : dtype, optional, default int The data of the returned sparse matrix. By default it is int"},
{"allReturnParams": ["patches"], "libName": "sklearn.feature_extraction.image", "methods": [], "notes": "", "funcName": "extract_patches_2d", "allFuncParams": ["image", "patch_size", "max_patches", "random_state"], "funcDesc": "Reshape a 2D image into a collection of patches", "funcParamBody": "image : array, shape = (image_height, image_width) or (image_height, image_width, n_channels) The original image data. For color images, the last dimension specifies the channel: a RGB image would have n_channels=3 . patch_size : tuple of ints (patch_height, patch_width) the dimensions of one patch max_patches : integer or float, optional default is None The maximum number of patches to extract. If max_patches is a float between 0 and 1, it is taken to be a proportion of the total number of patches. random_state : int, RandomState instance or None, optional (default=None) Pseudo number generator state used for random sampling to use if max_patches is not None.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "patches : array, shape = (n_patches, patch_height, patch_width) or (n_patches, patch_height, patch_width, n_channels) The collection of patches extracted from the image, where n_patches max_patches or the total number of patches that can be extracted."},
{"libName": "sklearn.feature_extraction", "methods": [{"methodName": "__init__(n_features=1048576, input_type=dict, dtype=<class numpy.float64>, alternate_sign=True, non_negative=False)", "methodDesc": ""}, {"methodName": "fit(X=None, y=None)", "methodReturnsBody": "self : FeatureHasher", "methodParams": [], "methodReturns": [], "methodDesc": "No-op. This method doesnt do anything. It exists purely for compatibility with the scikit-learn transformer API.", "methodParamsBody": "X : array-like"}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(raw_X)", "methodReturnsBody": "X : scipy.sparse matrix, shape = (n_samples, self.n_features)   Feature matrix, for use with estimators or further transformers.  ", "methodParams": ["raw_X"], "methodReturns": ["X"], "methodDesc": "Transform a sequence of instances to a scipy.sparse matrix.", "methodParamsBody": "raw_X : iterable over iterable over raw features, length = n_samples   Samples. Each sample must be iterable an (e.g., a list or tuple) containing/generating feature names (and optionally values, see the input_type constructor argument) which will be hashed. raw_X need not support the len function, so it can be the result of a generator; n_samples is determined on the fly.  "}], "notes": "", "funcName": "FeatureHasher", "allFuncParams": ["n_features", "input_type", "dtype", "alternate_sign", "non_negative"], "funcDesc": "Implements feature hashing, aka the hashing trick.", "funcParamBody": "n_features : integer, optional The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners. input_type : string, optional, default dict Either dict (the default) to accept dictionaries over (feature_name, value); pair to accept pairs of (feature_name, value); or string to accept single strings. feature_name should be a string, while value should be a number. In the case of string, a value of 1 is implied. The feature_name is hashed to find the appropriate column for the feature. The values sign might be flipped in the output (but see non_negative, below). dtype : numpy type, optional, default np.float64 The type of feature values. Passed to scipy.sparse matrix constructors as the dtype argument. Do not set this to bool, np.boolean or any unsigned integer type. alternate_sign : boolean, optional, default True When True, an alternating sign is added to the features as to approximately conserve the inner product in the hashed space even for small n_features. This approach is similar to sparse random projection. non_negative : boolean, optional, default False When True, an absolute value is applied to the features matrix prior to returning it. When used in conjunction with alternate_sign=True, this significantly reduces the inner product preservation property. Deprecated since version 0.19: This option will be removed in 0.21."},
{"libName": "sklearn.feature_extraction", "methods": [{"methodName": "__init__(dtype=<class numpy.float64>, separator==, sparse=True, sort=True)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Learn a list of feature name -> indices mappings.", "methodParamsBody": "X : Mapping or iterable over Mappings   Dict(s) or Mapping(s) from feature names (arbitrary Python objects) to feature values (strings or convertible to dtype).   y : (ignored) "}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "Xa : {array, sparse matrix}   Feature vectors; always 2-d.  ", "methodParams": ["X", "y"], "methodReturns": ["Xa"], "methodDesc": "Learn a list of feature name -> indices mappings and transform X. Like fit(X) followed by transform(X), but does not require materializing X in memory.", "methodParamsBody": "X : Mapping or iterable over Mappings   Dict(s) or Mapping(s) from feature names (arbitrary Python objects) to feature values (strings or convertible to dtype).   y : (ignored) "}, {"methodName": "get_feature_names()", "methodDesc": "Returns a list of feature names, ordered by their indices. If one-of-K coding is applied to categorical features, this will include the constructed feature names but not the original ones."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "inverse_transform(X, dict_type=<class dict>)", "methodReturnsBody": "D : list of dict_type objects, length = n_samples   Feature mappings for the samples in X.  ", "methodParams": ["X", "dict_type"], "methodReturns": ["D"], "methodDesc": "Transform array or sparse matrix X back to feature mappings. X must have been produced by this DictVectorizers transform or fit_transform method; it may only have passed through transformers that preserve the number of features and their order. In the case of one-hot/one-of-K coding, the constructed feature names and values are returned rather than the original ones.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Sample matrix.   dict_type : callable, optional   Constructor for feature mappings. Must conform to the collections.Mapping API.  "}, {"methodName": "restrict(support, indices=False)", "methodReturnsBody": "self : ", "methodParams": ["support", "indices"], "methodReturns": ["self"], "methodDesc": "Restrict the features to those in support using feature selection. This function modifies the estimator in-place. Examples", "methodParamsBody": "support : array-like   Boolean mask or list of indices (as returned by the get_support member of feature selectors).   indices : boolean, optional   Whether support is a list of indices.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "Xa : {array, sparse matrix}   Feature vectors; always 2-d.  ", "methodParams": ["X"], "methodReturns": ["Xa"], "methodDesc": "Transform feature->value dicts to array or sparse matrix. Named features not encountered during fit or fit_transform will be silently ignored.", "methodParamsBody": "X : Mapping or iterable over Mappings, length = n_samples   Dict(s) or Mapping(s) from feature names (arbitrary Python objects) to feature values (strings or convertible to dtype).  "}], "allFuncParams": ["dtype", "separator", "sparse", "sort"], "notes": "", "funcName": "DictVectorizer", "allFuncAttributes": ["vocabulary_", "feature_names_"], "funcDesc": "Transforms lists of feature-value mappings to vectors.", "funcParamBody": "dtype : callable, optional The type of feature values. Passed to Numpy array/scipy.sparse matrix constructors as the dtype argument. separator : string, optional Separator string used when constructing new features for one-hot coding. sparse : boolean, optional. Whether transform should produce scipy.sparse matrices. True by default. sort : boolean, optional. Whether feature_names_ and vocabulary_ should be sorted when fitting. True by default.", "funcAttrBody": "vocabulary_ : dict A dictionary mapping feature names to feature indices. feature_names_ : list A list of length n_features containing the feature names (e.g., f=ham and f=spam)."},
{"funcName": "UndefinedMetricWarning", "notes": "", "libName": "sklearn.exceptions", "methods": [{"methodName": "with_traceback()", "methodDesc": "Exception.with_traceback(tb)  set self.__traceback__ to tb and return self."}], "funcDesc": "Warning used when the metric is invalid"},
{"funcName": "NonBLASDotWarning", "notes": "", "libName": "sklearn.exceptions", "methods": [{"methodName": "with_traceback()", "methodDesc": "Exception.with_traceback(tb)  set self.__traceback__ to tb and return self."}], "funcDesc": "Warning used when the dot operation does not use BLAS."},
{"funcName": "NotFittedError", "notes": "", "libName": "sklearn.exceptions", "methods": [{"methodName": "with_traceback()", "methodDesc": "Exception.with_traceback(tb)  set self.__traceback__ to tb and return self."}], "funcDesc": "Exception class to raise if estimator is used before fitting."},
{"funcName": "FitFailedWarning", "notes": "", "libName": "sklearn.exceptions", "methods": [{"methodName": "with_traceback()", "methodDesc": "Exception.with_traceback(tb)  set self.__traceback__ to tb and return self."}], "funcDesc": "Warning class used if there is an error while fitting the estimator."},
{"funcName": "EfficiencyWarning", "notes": "", "libName": "sklearn.exceptions", "methods": [{"methodName": "with_traceback()", "methodDesc": "Exception.with_traceback(tb)  set self.__traceback__ to tb and return self."}], "funcDesc": "Warning used to notify the user of inefficient computation."},
{"funcName": "DataDimensionalityWarning", "notes": "", "libName": "sklearn.exceptions", "methods": [{"methodName": "with_traceback()", "methodDesc": "Exception.with_traceback(tb)  set self.__traceback__ to tb and return self."}], "funcDesc": "Custom warning to notify potential issues with data dimensionality."},
{"funcName": "DataConversionWarning", "notes": "", "libName": "sklearn.exceptions", "methods": [{"methodName": "with_traceback()", "methodDesc": "Exception.with_traceback(tb)  set self.__traceback__ to tb and return self."}], "funcDesc": "Warning used to notify implicit data conversions happening in the code."},
{"funcName": "ConvergenceWarning", "notes": "", "libName": "sklearn.exceptions", "methods": [{"methodName": "with_traceback()", "methodDesc": "Exception.with_traceback(tb)  set self.__traceback__ to tb and return self."}], "funcDesc": "Custom warning to capture convergence problems"},
{"funcName": "ChangedBehaviorWarning", "notes": "", "libName": "sklearn.exceptions", "methods": [{"methodName": "with_traceback()", "methodDesc": "Exception.with_traceback(tb)  set self.__traceback__ to tb and return self."}], "funcDesc": "Warning class used to notify the user of any change in the behavior."},
{"allReturnParams": ["fig", "axs"], "libName": "sklearn.ensemble.partial_dependence", "methods": [], "notes": "", "funcName": "plot_partial_dependence", "allFuncParams": ["gbrt", "X", "features", "feature_names", "label", "n_cols", "percentiles", "grid_resolution", "n_jobs", "verbose", "ax", "line_kw", "contour_kw", "fig_kw"], "funcDesc": "Partial dependence plots for ", "funcParamBody": "gbrt : BaseGradientBoosting A fitted gradient boosting model. X : array-like, shape=(n_samples, n_features) The data on which gbrt was trained. features : seq of ints, strings, or tuples of ints or strings If seq[i] is an int or a tuple with one int value, a one-way PDP is created; if seq[i] is a tuple of two ints, a two-way PDP is created. If feature_names is specified and seq[i] is an int, seq[i] must be < len(feature_names). If seq[i] is a string, feature_names must be specified, and seq[i] must be in feature_names. feature_names : seq of str Name of each feature; feature_names[i] holds the name of the feature with index i. label : object The class label for which the PDPs should be computed. Only if gbrt is a multi-class model. Must be in gbrt.classes_ . n_cols : int The number of columns in the grid plot (default: 3). percentiles : (low, high), default=(0.05, 0.95) The lower and upper percentile used to create the extreme values for the PDP axes. grid_resolution : int, default=100 The number of equally spaced points on the axes. n_jobs : int The number of CPUs to use to compute the PDs. -1 means all CPUs. Defaults to 1. verbose : int Verbose output during PD computations. Defaults to 0. ax : Matplotlib axis object, default None An axis object onto which the plots will be drawn. line_kw : dict Dict with keywords passed to the matplotlib.pyplot.plot call. For one-way partial dependence plots. contour_kw : dict Dict with keywords passed to the matplotlib.pyplot.plot call. For two-way partial dependence plots. fig_kw : dict Dict with keywords passed to the figure() call. Note that all keywords not recognized above will be automatically included here.", "funcReturnBody": "fig : figure The Matplotlib Figure object. axs : seq of Axis objects A seq of Axis objects, one for each subplot."},
{"allReturnParams": ["pdp", "axes"], "libName": "sklearn.ensemble.partial_dependence", "methods": [], "notes": "", "funcName": "partial_dependence", "allFuncParams": ["gbrt", "target_variables", "grid", "X", "percentiles", "grid_resolution"], "funcDesc": "Partial dependence of ", "funcParamBody": "gbrt : BaseGradientBoosting A fitted gradient boosting model. target_variables : array-like, dtype=int The target features for which the partial dependecy should be computed (size should be smaller than 3 for visual renderings). grid : array-like, shape=(n_points, len(target_variables)) The grid of target_variables values for which the partial dependecy should be evaluated (either grid or X X : array-like, shape=(n_samples, n_features) The data on which gbrt was trained. It is used to generate a grid for the target_variables . The grid comprises grid_resolution equally spaced points between the two percentiles . percentiles : (low, high), default=(0.05, 0.95) The lower and upper percentile used create the extreme values for the grid . Only if X is not None. grid_resolution : int, default=100 The number of equally spaced points on the grid .", "funcReturnBody": "pdp : array, shape=(n_classes, n_points) The partial dependence function evaluated on the grid . For regression and binary classification n_classes==1 . axes : seq of ndarray or None The axes with which the grid has been created or None if the grid has been given."},
{"libName": "sklearn.ensemble", "methods": [{"methodName": "__init__(estimators, voting=hard, weights=None, n_jobs=1, flatten_transform=None)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the estimators.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples]   Target values.   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Note that this is supported only if all underlying estimators support sample weights.  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodParams": ["deep: bool"], "methodName": "get_params(deep=True)", "methodParamsBody": "deep: bool :   Setting it to True gets the various classifiers and the parameters of the classifiers as well  ", "methodDesc": "Get the parameters of the VotingClassifier"}, {"methodName": "predict(X)", "methodReturnsBody": "maj : array-like, shape = [n_samples]   Predicted class labels.  ", "methodParams": ["X"], "methodReturns": ["maj"], "methodDesc": "Predict class labels for X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of features.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodParams": ["params: keyword arguments"], "methodName": "set_params(**params)", "methodParamsBody": "params: keyword arguments :   Specific parameters using e.g. set_params(parameter_name=new_value) In addition, to setting the parameters of the VotingClassifier , the individual classifiers of the VotingClassifier can also be set or replaced by setting them to None.  ", "methodDesc": "Setting the parameters for the voting classifier Valid parameter keys can be listed with get_params(). Examples # In this example, the RandomForestClassifier is removed clf1 = LogisticRegression() clf2 = RandomForestClassifier() eclf = VotingClassifier(estimators=[(lr, clf1), (rf, clf2)] eclf.set_params(rf=None)"}, {"methodName": "transform(X)", "methodReturnsBody": "If `voting=soft` and `flatten_transform=True`: :   array-like = (n_classifiers, n_samples * n_classes) otherwise array-like = (n_classifiers, n_samples, n_classes)   Class probabilities calculated by each classifier.    If `voting=hard`: :    array-like = [n_samples, n_classifiers]  Class labels predicted by each classifier.    ", "methodParams": ["X"], "methodReturns": ["If `voting=\u2019soft\u2019` and `flatten_transform=True`:", "If `voting=\u2019hard\u2019`:"], "methodDesc": "Return class labels or probabilities for X for each estimator.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of features.  "}], "allFuncParams": ["estimators", "voting", "weights", "n_jobs", "flatten_transform"], "notes": "", "funcName": "VotingClassifier", "allFuncAttributes": ["estimators_", "classes_"], "funcDesc": "Soft Voting/Majority Rule classifier for unfitted estimators.", "funcParamBody": "estimators : list of (string, estimator) tuples Invoking the fit method on the VotingClassifier will fit clones of those original estimators that will be stored in the class attribute self.estimators_ . An estimator can be set to None using set_params . voting : str, {hard, soft} (default=hard) If hard, uses predicted class labels for majority rule voting. Else if soft, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers. weights : array-like, shape = [n_classifiers], optional (default=`None`) Sequence of weights ( float or int ) to weight the occurrences of predicted class labels ( hard voting) or class probabilities before averaging ( soft voting). Uses uniform weights if None . n_jobs : int, optional (default=1) The number of jobs to run in parallel for fit . If -1, then the number of jobs is set to the number of cores. flatten_transform : bool, optional (default=None) Affects shape of transform output only when voting=soft If voting=soft and flatten_transform=True, transform method returns matrix with shape (n_samples, n_classifiers * n_classes). If flatten_transform=False, it returns (n_classifiers, n_samples, n_classes).", "funcAttrBody": "estimators_ : list of classifiers The collection of fitted sub-estimators as defined in estimators None . classes_ : array-like, shape = [n_predictions] The classes labels."},
{"funcName": "RandomTreesEmbedding", "notes": "", "libName": "sklearn.ensemble", "methods": [{"methodName": "__init__(n_estimators=10, max_depth=5, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, sparse_output=True, n_jobs=1, random_state=None, verbose=0, warm_start=False)", "methodDesc": ""}, {"methodName": "apply(X)", "methodReturnsBody": "X_leaves : array_like, shape = [n_samples, n_estimators]   For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in.  ", "methodParams": ["X"], "methodReturns": ["X_leaves"], "methodDesc": "Apply trees in the forest to X, return leaf indices.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "decision_path(X)", "methodReturnsBody": "indicator : sparse csr array, shape = [n_samples, n_nodes]   Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.   n_nodes_ptr : array of size (n_estimators + 1, )   The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.  ", "methodParams": ["X"], "methodReturns": ["indicator", "n_nodes_ptr"], "methodDesc": "Return the decision path in the forest", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "fit(X, y=None, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit estimator.", "methodParamsBody": "X : array-like or sparse matrix, shape=(n_samples, n_features)   The input samples. Use dtype=np.float32 for maximum efficiency. Sparse matrices are also supported, use sparse csc_matrix for maximum efficiency.   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.  "}, {"methodName": "fit_transform(X, y=None, sample_weight=None)", "methodReturnsBody": "X_transformed : sparse matrix, shape=(n_samples, n_out)   Transformed dataset.  ", "methodParams": ["X", "sample_weight"], "methodReturns": ["X_transformed"], "methodDesc": "Fit estimator and transform dataset.", "methodParamsBody": "X : array-like or sparse matrix, shape=(n_samples, n_features)   Input data used to build forests. Use dtype=np.float32 for maximum efficiency.   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_transformed : sparse matrix, shape=(n_samples, n_out)   Transformed dataset.  ", "methodParams": ["X"], "methodReturns": ["X_transformed"], "methodDesc": "Transform dataset.", "methodParamsBody": "X : array-like or sparse matrix, shape=(n_samples, n_features)   Input data to be transformed. Use dtype=np.float32 for maximum efficiency. Sparse matrices are also supported, use sparse csr_matrix for maximum efficiency.  "}], "funcDesc": "An ensemble of totally random trees."},
{"funcName": "RandomForestRegressor", "notes": "The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, max_features=n_features and bootstrap=False , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, max_features=n_features and bootstrap=False , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed.", "libName": "sklearn.ensemble", "methods": [{"methodName": "__init__(n_estimators=10, criterion=mse, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=auto, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)", "methodDesc": ""}, {"methodName": "apply(X)", "methodReturnsBody": "X_leaves : array_like, shape = [n_samples, n_estimators]   For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in.  ", "methodParams": ["X"], "methodReturns": ["X_leaves"], "methodDesc": "Apply trees in the forest to X, return leaf indices.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "decision_path(X)", "methodReturnsBody": "indicator : sparse csr array, shape = [n_samples, n_nodes]   Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.   n_nodes_ptr : array of size (n_estimators + 1, )   The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.  ", "methodParams": ["X"], "methodReturns": ["indicator", "n_nodes_ptr"], "methodDesc": "Return the decision path in the forest", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Build a forest of trees from the training set (X, y).", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The training input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csc_matrix .   y : array-like, shape = [n_samples] or [n_samples, n_outputs]   The target values (class labels in classification, real numbers in regression).   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of shape = [n_samples] or [n_samples, n_outputs]   The predicted values.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict regression target for X. The predicted regression target of an input sample is computed as the mean predicted regression targets of the trees in the forest.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "A random forest regressor."},
{"funcName": "RandomForestClassifier", "notes": "The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, max_features=n_features and bootstrap=False , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data, max_features=n_features and bootstrap=False , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed.", "libName": "sklearn.ensemble", "methods": [{"methodName": "__init__(n_estimators=10, criterion=gini, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=auto, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)", "methodDesc": ""}, {"methodName": "apply(X)", "methodReturnsBody": "X_leaves : array_like, shape = [n_samples, n_estimators]   For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in.  ", "methodParams": ["X"], "methodReturns": ["X_leaves"], "methodDesc": "Apply trees in the forest to X, return leaf indices.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "decision_path(X)", "methodReturnsBody": "indicator : sparse csr array, shape = [n_samples, n_nodes]   Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.   n_nodes_ptr : array of size (n_estimators + 1, )   The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.  ", "methodParams": ["X"], "methodReturns": ["indicator", "n_nodes_ptr"], "methodDesc": "Return the decision path in the forest", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Build a forest of trees from the training set (X, y).", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The training input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csc_matrix .   y : array-like, shape = [n_samples] or [n_samples, n_outputs]   The target values (class labels in classification, real numbers in regression).   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of shape = [n_samples] or [n_samples, n_outputs]   The predicted classes.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict class for X. The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "p : array of shape = [n_samples, n_classes], or a list of n_outputs   such arrays if n_outputs > 1. The class probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Predict class log-probabilities for X. The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the trees in the forest.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "p : array of shape = [n_samples, n_classes], or a list of n_outputs   such arrays if n_outputs > 1. The class probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Predict class probabilities for X. The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class in a leaf.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "A random forest classifier."},
{"funcName": "IsolationForest", "notes": "", "libName": "sklearn.ensemble", "methods": [{"methodName": "__init__(n_estimators=100, max_samples=auto, contamination=0.1, max_features=1.0, bootstrap=False, n_jobs=1, random_state=None, verbose=0)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "scores : array of shape (n_samples,)   The anomaly score of the input samples. The lower, the more abnormal.  ", "methodParams": ["X"], "methodReturns": ["scores"], "methodDesc": "Average anomaly score of X of the base classifiers. The anomaly score of an input sample is computed as the mean anomaly score of the trees in the forest. The measure of normality of an observation given a tree is the depth of the leaf containing this observation, which is equivalent to the number of splittings required to isolate this point. In case of several observations n_left in the leaf, the average path length of a n_left samples isolation tree is added.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.  "}, {"methodName": "fit(X, y=None, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit estimator.", "methodParamsBody": "X : array-like or sparse matrix, shape (n_samples, n_features)   The input samples. Use dtype=np.float32 for maximum efficiency. Sparse matrices are also supported, use sparse csc_matrix for maximum efficiency.   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "is_inlier : array, shape (n_samples,)   For each observations, tells whether or not (+1 or -1) it should be considered as an inlier according to the fitted model.  ", "methodParams": ["X"], "methodReturns": ["is_inlier"], "methodDesc": "Predict if a particular sample is an outlier or not.", "methodParamsBody": "X : array-like or sparse matrix, shape (n_samples, n_features)   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "Isolation Forest Algorithm"},
{"libName": "sklearn.ensemble", "methods": [{"methodName": "__init__(loss=ls, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=friedman_mse, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None, warm_start=False, presort=auto)", "methodDesc": ""}, {"methodName": "apply(X)", "methodReturnsBody": "X_leaves : array_like, shape = [n_samples, n_estimators]   For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator.  ", "methodParams": ["X"], "methodReturns": ["X_leaves"], "methodDesc": "Apply trees in the ensemble to X, return leaf indices.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted to a sparse csr_matrix .  "}, {"methodName": "fit(X, y, sample_weight=None, monitor=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight", "monitor"], "methodReturns": ["self"], "methodDesc": "Fit the gradient boosting model.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples]   Target values (integers in classification, real numbers in regression) For classification, labels must correspond to classes.   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.   monitor : callable, optional   The monitor is called after each iteration with the current iteration, a reference to the estimator and the local variables of _fit_stages as keyword arguments callable(i,  self,  locals()) . If the callable returns True the fitting procedure is stopped. The monitor can be used for various things such as computing held-out estimates, early stopping, model introspect, and snapshoting.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of shape = [n_samples]   The predicted values.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict regression target for X.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "staged_predict(X)", "methodReturnsBody": "y : generator of array of shape = [n_samples]   The predicted value of the input samples.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict regression target at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .  "}], "allFuncParams": ["loss", "learning_rate", "n_estimators", "max_depth", "criterion", "min_samples_split", "min_samples_leaf", "min_weight_fraction_leaf", "subsample", "max_features", "max_leaf_nodes", "min_impurity_split", "min_impurity_decrease", "alpha", "init", "verbose", "warm_start", "random_state", "presort"], "notes": "The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001. T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning Ed. 2, Springer, 2009. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001. T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning Ed. 2, Springer, 2009.", "funcName": "GradientBoostingRegressor", "allFuncAttributes": ["feature_importances_", "oob_improvement_", "train_score_", "loss_", "init", "estimators_"], "funcDesc": "Gradient Boosting for regression.", "funcParamBody": "loss : {ls, lad, huber, quantile}, optional (default=ls) loss function to be optimized. ls refers to least squares regression. lad (least absolute deviation) is a highly robust loss function solely based on order information of the input variables. huber is a combination of the two. quantile allows quantile regression (use alpha to specify the quantile). learning_rate : float, optional (default=0.1) learning rate shrinks the contribution of each tree by learning_rate . There is a trade-off between learning_rate and n_estimators. n_estimators : int (default=100) The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. max_depth : integer, optional (default=3) maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. criterion : string, optional (default=friedman_mse) The function to measure the quality of a split. Supported criteria are friedman_mse for the mean squared error with improvement score by Friedman, mse for mean squared error, and mae for the mean absolute error. The default value of friedman_mse is generally the best as it can provide a better approximation in some cases. New in version 0.18. min_samples_split : int, float, optional (default=2) The minimum number of samples required to split an internal node: If int, then consider min_samples_split as the minimum number. If float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split. Changed in version 0.18: Added float values for percentages. min_samples_leaf : int, float, optional (default=1) The minimum number of samples required to be at a leaf node: If int, then consider min_samples_leaf as the minimum number. If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node. Changed in version 0.18: Added float values for percentages. min_weight_fraction_leaf : float, optional (default=0.) The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. subsample : float, optional (default=1.0) The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators . Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias. max_features : int, float, string or None, optional (default=None) The number of features to consider when looking for the best split: If int, then consider max_features features at each split. If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split. If auto, then max_features=n_features . If sqrt, then max_features=sqrt(n_features) . If log2, then max_features=log2(n_features) . If None, then max_features=n_features . Choosing max_features < n_features leads to a reduction of variance and an increase in bias. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features. max_leaf_nodes : int or None, optional (default=None) Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. min_impurity_split : float, Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf. Deprecated since version 0.19: min_impurity_split has been deprecated in favor of min_impurity_decrease in 0.19 and will be removed in 0.21. Use min_impurity_decrease instead. min_impurity_decrease : float, optional (default=0.) A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following: N_t  /  N  *  ( impurity  -  N_t_R  /  N_t  *  right_impurity -  N_t_L  /  N_t  *  left_impurity ) where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child. N , N_t , N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed. New in version 0.19. alpha : float (default=0.9) The alpha-quantile of the huber loss function and the quantile loss function. Only if loss='huber' or loss='quantile' . init : BaseEstimator, None, optional (default=None) An estimator object that is used to compute the initial predictions. init has to provide fit and predict . If None it uses loss.init_estimator . verbose : int, default: 0 Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree. warm_start : bool, default: False When set to True , reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . presort : bool or auto, optional (default=auto) Whether to presort the data to speed up the finding of best splits in fitting. Auto mode by default will use presorting on dense data and default to normal sorting on sparse data. Setting presort to true on sparse data will raise an error. New in version 0.17: optional parameter presort .", "funcAttrBody": "feature_importances_ : array, shape = [n_features] The feature importances (the higher, the more important the feature). oob_improvement_ : array, shape = [n_estimators] The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. oob_improvement_[0] is the improvement in loss of the first stage over the init estimator. train_score_ : array, shape = [n_estimators] The i-th score train_score_[i] is the deviance (= loss) of the model at iteration i on the in-bag sample. If subsample  ==  1 this is the deviance on the training data. loss_ : LossFunction The concrete LossFunction object. init : BaseEstimator The estimator that provides the initial predictions. Set via the init argument or loss.init_estimator . estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, 1] The collection of fitted sub-estimators."},
{"libName": "sklearn.ensemble", "methods": [{"methodName": "__init__(loss=deviance, learning_rate=0.1, n_estimators=100, subsample=1.0, criterion=friedman_mse, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, presort=auto)", "methodDesc": ""}, {"methodName": "apply(X)", "methodReturnsBody": "X_leaves : array_like, shape = [n_samples, n_estimators, n_classes]   For each datapoint x in X and for each tree in the ensemble, return the index of the leaf x ends up in each estimator. In the case of binary classification n_classes is 1.  ", "methodParams": ["X"], "methodReturns": ["X_leaves"], "methodDesc": "Apply trees in the ensemble to X, return leaf indices.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted to a sparse csr_matrix .  "}, {"methodName": "decision_function(X)", "methodReturnsBody": "score : array, shape = [n_samples, n_classes] or [n_samples]   The decision function of the input samples. The order of the classes corresponds to that in the attribute classes_ . Regression and binary classification produce an array of shape [n_samples].  ", "methodParams": ["X"], "methodReturns": ["score"], "methodDesc": "Compute the decision function of X .", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .  "}, {"methodName": "fit(X, y, sample_weight=None, monitor=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight", "monitor"], "methodReturns": ["self"], "methodDesc": "Fit the gradient boosting model.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples]   Target values (integers in classification, real numbers in regression) For classification, labels must correspond to classes.   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.   monitor : callable, optional   The monitor is called after each iteration with the current iteration, a reference to the estimator and the local variables of _fit_stages as keyword arguments callable(i,  self,  locals()) . If the callable returns True the fitting procedure is stopped. The monitor can be used for various things such as computing held-out estimates, early stopping, model introspect, and snapshoting.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of shape = [n_samples]   The predicted values.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict class for X.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .  "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "p : array of shape = [n_samples]   The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Predict class log-probabilities for X.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "p : array of shape = [n_samples]   The class probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Predict class probabilities for X.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "staged_decision_function(X)", "methodReturnsBody": "score : generator of array, shape = [n_samples, k]   The decision function of the input samples. The order of the classes corresponds to that in the attribute classes_ . Regression and binary classification are special cases with k  ==  1 , otherwise k==n_classes .  ", "methodParams": ["X"], "methodReturns": ["score"], "methodDesc": "Compute decision function of X for each iteration. This method allows monitoring (i.e. determine error on testing set) after each stage.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .  "}, {"methodName": "staged_predict(X)", "methodReturnsBody": "y : generator of array of shape = [n_samples]   The predicted value of the input samples.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict class at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .  "}, {"methodName": "staged_predict_proba(X)", "methodReturnsBody": "y : generator of array of shape = [n_samples]   The predicted value of the input samples.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict class probabilities at each stage for X. This method allows monitoring (i.e. determine error on testing set) after each stage.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, it will be converted to dtype=np.float32 and if a sparse matrix is provided to a sparse csr_matrix .  "}], "allFuncParams": ["loss", "learning_rate", "n_estimators", "max_depth", "criterion", "min_samples_split", "min_samples_leaf", "min_weight_fraction_leaf", "subsample", "max_features", "max_leaf_nodes", "min_impurity_split", "min_impurity_decrease", "init", "verbose", "warm_start", "random_state", "presort"], "notes": "The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001. T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning Ed. 2, Springer, 2009. The features are always randomly permuted at each split. Therefore, the best found split may vary, even with the same training data and max_features=n_features , if the improvement of the criterion is identical for several splits enumerated during the search of the best split. To obtain a deterministic behaviour during fitting, random_state has to be fixed. J. Friedman, Greedy Function Approximation: A Gradient Boosting Machine, The Annals of Statistics, Vol. 29, No. 5, 2001. T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning Ed. 2, Springer, 2009.", "funcName": "GradientBoostingClassifier", "allFuncAttributes": ["feature_importances_", "oob_improvement_", "train_score_", "loss_", "init", "estimators_"], "funcDesc": "Gradient Boosting for classification.", "funcParamBody": "loss : {deviance, exponential}, optional (default=deviance) loss function to be optimized. deviance refers to deviance (= logistic regression) for classification with probabilistic outputs. For loss exponential gradient boosting recovers the AdaBoost algorithm. learning_rate : float, optional (default=0.1) learning rate shrinks the contribution of each tree by learning_rate . There is a trade-off between learning_rate and n_estimators. n_estimators : int (default=100) The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance. max_depth : integer, optional (default=3) maximum depth of the individual regression estimators. The maximum depth limits the number of nodes in the tree. Tune this parameter for best performance; the best value depends on the interaction of the input variables. criterion : string, optional (default=friedman_mse) The function to measure the quality of a split. Supported criteria are friedman_mse for the mean squared error with improvement score by Friedman, mse for mean squared error, and mae for the mean absolute error. The default value of friedman_mse is generally the best as it can provide a better approximation in some cases. New in version 0.18. min_samples_split : int, float, optional (default=2) The minimum number of samples required to split an internal node: If int, then consider min_samples_split as the minimum number. If float, then min_samples_split is a percentage and ceil(min_samples_split * n_samples) are the minimum number of samples for each split. Changed in version 0.18: Added float values for percentages. min_samples_leaf : int, float, optional (default=1) The minimum number of samples required to be at a leaf node: If int, then consider min_samples_leaf as the minimum number. If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node. Changed in version 0.18: Added float values for percentages. min_weight_fraction_leaf : float, optional (default=0.) The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided. subsample : float, optional (default=1.0) The fraction of samples to be used for fitting the individual base learners. If smaller than 1.0 this results in Stochastic Gradient Boosting. subsample interacts with the parameter n_estimators . Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias. max_features : int, float, string or None, optional (default=None) The number of features to consider when looking for the best split: If int, then consider max_features features at each split. If float, then max_features is a percentage and int(max_features * n_features) features are considered at each split. If auto, then max_features=sqrt(n_features) . If sqrt, then max_features=sqrt(n_features) . If log2, then max_features=log2(n_features) . If None, then max_features=n_features . Choosing max_features < n_features leads to a reduction of variance and an increase in bias. Note: the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features. max_leaf_nodes : int or None, optional (default=None) Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. min_impurity_split : float, Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf. Deprecated since version 0.19: min_impurity_split has been deprecated in favor of min_impurity_decrease in 0.19 and will be removed in 0.21. Use min_impurity_decrease instead. min_impurity_decrease : float, optional (default=0.) A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The weighted impurity decrease equation is the following: N_t  /  N  *  ( impurity  -  N_t_R  /  N_t  *  right_impurity -  N_t_L  /  N_t  *  left_impurity ) where N is the total number of samples, N_t is the number of samples at the current node, N_t_L is the number of samples in the left child, and N_t_R is the number of samples in the right child. N , N_t , N_t_R and N_t_L all refer to the weighted sum, if sample_weight is passed. New in version 0.19. init : BaseEstimator, None, optional (default=None) An estimator object that is used to compute the initial predictions. init has to provide fit and predict . If None it uses loss.init_estimator . verbose : int, default: 0 Enable verbose output. If 1 then it prints progress and performance once in a while (the more trees the lower the frequency). If greater than 1 then it prints progress and performance for every tree. warm_start : bool, default: False When set to True , reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just erase the previous solution. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . presort : bool or auto, optional (default=auto) Whether to presort the data to speed up the finding of best splits in fitting. Auto mode by default will use presorting on dense data and default to normal sorting on sparse data. Setting presort to true on sparse data will raise an error. New in version 0.17: presort parameter.", "funcAttrBody": "feature_importances_ : array, shape = [n_features] The feature importances (the higher, the more important the feature). oob_improvement_ : array, shape = [n_estimators] The improvement in loss (= deviance) on the out-of-bag samples relative to the previous iteration. oob_improvement_[0] is the improvement in loss of the first stage over the init estimator. train_score_ : array, shape = [n_estimators] The i-th score train_score_[i] is the deviance (= loss) of the model at iteration i on the in-bag sample. If subsample  ==  1 this is the deviance on the training data. loss_ : LossFunction The concrete LossFunction object. init : BaseEstimator The estimator that provides the initial predictions. Set via the init argument or loss.init_estimator . estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, loss_.K ] The collection of fitted sub-estimators. loss_.K is 1 for binary classification, otherwise n_classes."},
{"funcName": "ExtraTreesRegressor", "notes": "The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.", "libName": "sklearn.ensemble", "methods": [{"methodName": "__init__(n_estimators=10, criterion=mse, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=auto, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)", "methodDesc": ""}, {"methodName": "apply(X)", "methodReturnsBody": "X_leaves : array_like, shape = [n_samples, n_estimators]   For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in.  ", "methodParams": ["X"], "methodReturns": ["X_leaves"], "methodDesc": "Apply trees in the forest to X, return leaf indices.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "decision_path(X)", "methodReturnsBody": "indicator : sparse csr array, shape = [n_samples, n_nodes]   Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.   n_nodes_ptr : array of size (n_estimators + 1, )   The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.  ", "methodParams": ["X"], "methodReturns": ["indicator", "n_nodes_ptr"], "methodDesc": "Return the decision path in the forest", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Build a forest of trees from the training set (X, y).", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The training input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csc_matrix .   y : array-like, shape = [n_samples] or [n_samples, n_outputs]   The target values (class labels in classification, real numbers in regression).   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of shape = [n_samples] or [n_samples, n_outputs]   The predicted values.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict regression target for X. The predicted regression target of an input sample is computed as the mean predicted regression targets of the trees in the forest.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "An extra-trees regressor."},
{"funcName": "ExtraTreesClassifier", "notes": "The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values. The default values for the parameters controlling the size of the trees (e.g. max_depth , min_samples_leaf , etc.) lead to fully grown and unpruned trees which can potentially be very large on some data sets. To reduce memory consumption, the complexity and size of the trees should be controlled by setting those parameter values.", "libName": "sklearn.ensemble", "methods": [{"methodName": "__init__(n_estimators=10, criterion=gini, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=auto, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=False, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False, class_weight=None)", "methodDesc": ""}, {"methodName": "apply(X)", "methodReturnsBody": "X_leaves : array_like, shape = [n_samples, n_estimators]   For each datapoint x in X and for each tree in the forest, return the index of the leaf x ends up in.  ", "methodParams": ["X"], "methodReturns": ["X_leaves"], "methodDesc": "Apply trees in the forest to X, return leaf indices.", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "decision_path(X)", "methodReturnsBody": "indicator : sparse csr array, shape = [n_samples, n_nodes]   Return a node indicator matrix where non zero elements indicates that the samples goes through the nodes.   n_nodes_ptr : array of size (n_estimators + 1, )   The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]] gives the indicator value for the i-th estimator.  ", "methodParams": ["X"], "methodReturns": ["indicator", "n_nodes_ptr"], "methodDesc": "Return the decision path in the forest", "methodParamsBody": "X : array-like or sparse matrix, shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Build a forest of trees from the training set (X, y).", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The training input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csc_matrix .   y : array-like, shape = [n_samples] or [n_samples, n_outputs]   The target values (class labels in classification, real numbers in regression).   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Splits that would create child nodes with net zero or negative weight are ignored while searching for a split in each node. In the case of classification, splits are also ignored if they would result in any single class carrying a negative weight in either child node.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of shape = [n_samples] or [n_samples, n_outputs]   The predicted classes.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict class for X. The predicted class of an input sample is a vote by the trees in the forest, weighted by their probability estimates. That is, the predicted class is the one with highest mean probability estimate across the trees.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "p : array of shape = [n_samples, n_classes], or a list of n_outputs   such arrays if n_outputs > 1. The class probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Predict class log-probabilities for X. The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the trees in the forest.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "p : array of shape = [n_samples, n_classes], or a list of n_outputs   such arrays if n_outputs > 1. The class probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Predict class probabilities for X. The predicted class probabilities of an input sample are computed as the mean predicted class probabilities of the trees in the forest. The class probability of a single tree is the fraction of samples of the same class in a leaf.", "methodParamsBody": "X : array-like or sparse matrix of shape = [n_samples, n_features]   The input samples. Internally, its dtype will be converted to dtype=np.float32 . If a sparse matrix is provided, it will be converted into a sparse csr_matrix .  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "An extra-trees classifier."},
{"funcName": "BaggingRegressor", "notes": "", "libName": "sklearn.ensemble", "methods": [{"methodName": "__init__(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=1, random_state=None, verbose=0)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.   y : array-like, shape = [n_samples]   The target values (class labels in classification, real numbers in regression).   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Note that this is supported only if the base estimator supports sample weighting.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of shape = [n_samples]   The predicted values.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict regression target for X. The predicted regression target of an input sample is computed as the mean predicted regression targets of the estimators in the ensemble.", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "A Bagging regressor."},
{"funcName": "BaggingClassifier", "notes": "", "libName": "sklearn.ensemble", "methods": [{"methodName": "__init__(base_estimator=None, n_estimators=10, max_samples=1.0, max_features=1.0, bootstrap=True, bootstrap_features=False, oob_score=False, warm_start=False, n_jobs=1, random_state=None, verbose=0)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "score : array, shape = [n_samples, k]   The decision function of the input samples. The columns correspond to the classes in sorted order, as they appear in the attribute classes_ . Regression and binary classification are special cases with k  ==  1 , otherwise k==n_classes .  ", "methodParams": ["X"], "methodReturns": ["score"], "methodDesc": "Average of the decision functions of the base classifiers.", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.  "}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.   y : array-like, shape = [n_samples]   The target values (class labels in classification, real numbers in regression).   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted. Note that this is supported only if the base estimator supports sample weighting.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of shape = [n_samples]   The predicted classes.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict class for X. The predicted class of an input sample is computed as the class with the highest mean predicted probability. If base estimators do not implement a predict_proba method, then it resorts to voting.", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.  "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "p : array of shape = [n_samples, n_classes]   The class log-probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Predict class log-probabilities for X. The predicted class log-probabilities of an input sample is computed as the log of the mean predicted class probabilities of the base estimators in the ensemble.", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "p : array of shape = [n_samples, n_classes]   The class probabilities of the input samples. The order of the classes corresponds to that in the attribute classes_ .  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Predict class probabilities for X. The predicted class probabilities of an input sample is computed as the mean predicted class probabilities of the base estimators in the ensemble. If base estimators do not implement a predict_proba method, then it resorts to voting and the predicted class probabilities of an input sample represents the proportion of estimators predicting each class.", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrices are accepted only if they are supported by the base estimator.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "A Bagging classifier."},
{"funcName": "AdaBoostRegressor", "notes": "", "libName": "sklearn.ensemble", "methods": [{"methodName": "__init__(base_estimator=None, n_estimators=50, learning_rate=1.0, loss=linear, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Build a boosted regressor from the training set (X, y).", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.   y : array-like of shape = [n_samples]   The target values (real numbers).   sample_weight : array-like of shape = [n_samples], optional   Sample weights. If None, the sample weights are initialized to 1 / n_samples.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of shape = [n_samples]   The predicted regression values.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict regression value for X. The predicted regression value of an input sample is computed as the weighted median prediction of the classifiers in the ensemble.", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "staged_predict(X)", "methodReturnsBody": "y : generator of array, shape = [n_samples]   The predicted regression values.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Return staged predictions for X. The predicted regression value of an input sample is computed as the weighted median prediction of the classifiers in the ensemble. This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost.", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.  "}, {"methodName": "staged_score(X, y, sample_weight=None)", "methodReturnsBody": "z : float ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["z"], "methodDesc": "Return staged scores for X, y. This generator method yields the ensemble score after each iteration of boosting and therefore allows monitoring, such as to determine the score on a test set after each boost.", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.   y : array-like, shape = [n_samples]   Labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}], "funcDesc": "An AdaBoost regressor."},
{"funcName": "AdaBoostClassifier", "notes": "", "libName": "sklearn.ensemble", "methods": [{"methodName": "__init__(base_estimator=None, n_estimators=50, learning_rate=1.0, algorithm=SAMME.R, random_state=None)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "score : array, shape = [n_samples, k]   The decision function of the input samples. The order of outputs is the same of that of the classes_ attribute. Binary classification is a special cases with k  ==  1 , otherwise k==n_classes . For binary classification, values closer to -1 or 1 mean more like the first or second class in classes_ , respectively.  ", "methodParams": ["X"], "methodReturns": ["score"], "methodDesc": "Compute the decision function of X .", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.  "}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Build a boosted classifier from the training set (X, y).", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.   y : array-like of shape = [n_samples]   The target values (class labels).   sample_weight : array-like of shape = [n_samples], optional   Sample weights. If None, the sample weights are initialized to 1  /  n_samples .  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array of shape = [n_samples]   The predicted classes.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Predict classes for X. The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble.", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.  "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "p : array of shape = [n_samples]   The class probabilities of the input samples. The order of outputs is the same of that of the classes_ attribute.  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Predict class log-probabilities for X. The predicted class log-probabilities of an input sample is computed as the weighted mean predicted class log-probabilities of the classifiers in the ensemble.", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "p : array of shape = [n_samples]   The class probabilities of the input samples. The order of outputs is the same of that of the classes_ attribute.  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Predict class probabilities for X. The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble.", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "staged_decision_function(X)", "methodReturnsBody": "score : generator of array, shape = [n_samples, k]   The decision function of the input samples. The order of outputs is the same of that of the classes_ attribute. Binary classification is a special cases with k  ==  1 , otherwise k==n_classes . For binary classification, values closer to -1 or 1 mean more like the first or second class in classes_ , respectively.  ", "methodParams": ["X"], "methodReturns": ["score"], "methodDesc": "Compute decision function of X for each boosting iteration. This method allows monitoring (i.e. determine error on testing set) after each boosting iteration.", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.  "}, {"methodName": "staged_predict(X)", "methodReturnsBody": "y : generator of array, shape = [n_samples]   The predicted classes.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Return staged predictions for X. The predicted class of an input sample is computed as the weighted mean prediction of the classifiers in the ensemble. This generator method yields the ensemble prediction after each iteration of boosting and therefore allows monitoring, such as to determine the prediction on a test set after each boost.", "methodParamsBody": "X : array-like of shape = [n_samples, n_features]   The input samples.  "}, {"methodName": "staged_predict_proba(X)", "methodReturnsBody": "p : generator of array, shape = [n_samples]   The class probabilities of the input samples. The order of outputs is the same of that of the classes_ attribute.  ", "methodParams": ["X"], "methodReturns": ["p"], "methodDesc": "Predict class probabilities for X. The predicted class probabilities of an input sample is computed as the weighted mean predicted class probabilities of the classifiers in the ensemble. This generator method yields the ensemble predicted class probabilities after each iteration of boosting and therefore allows monitoring, such as to determine the predicted class probabilities on a test set after each boost.", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.  "}, {"methodName": "staged_score(X, y, sample_weight=None)", "methodReturnsBody": "z : float ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["z"], "methodDesc": "Return staged scores for X, y. This generator method yields the ensemble score after each iteration of boosting and therefore allows monitoring, such as to determine the score on a test set after each boost.", "methodParamsBody": "X : {array-like, sparse matrix} of shape = [n_samples, n_features]   The training input samples. Sparse matrix can be CSC, CSR, COO, DOK, or LIL. DOK and LIL are converted to CSR.   y : array-like, shape = [n_samples]   Labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}], "funcDesc": "An AdaBoost classifier."},
{"libName": "sklearn.dummy", "methods": [{"methodName": "__init__(strategy=mean, constant=None, quantile=None)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the random regressor.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples] or [n_samples, n_outputs]   Target values.   sample_weight : array-like of shape = [n_samples], optional   Sample weights.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array, shape = [n_samples]  or [n_samples, n_outputs]   Predicted target values for X.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Perform classification on test vectors X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Input vectors, where n_samples is the number of samples and n_features is the number of features.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["strategy", "constant", "quantile"], "notes": "", "funcName": "DummyRegressor", "allFuncAttributes": ["constant_", "n_outputs_", "outputs_2d_"], "funcDesc": "DummyRegressor is a regressor that makes predictions using simple rules.", "funcParamBody": "strategy : str Strategy to use to generate predictions. mean: always predicts the mean of the training set median: always predicts the median of the training set quantile: always predicts a specified quantile of the training set, provided with the quantile parameter. constant: always predicts a constant value that is provided by the user. constant : int or float or array of shape = [n_outputs] The explicit constant as predicted by the constant strategy. This parameter is useful only for the constant strategy. quantile : float in [0.0, 1.0] The quantile to predict using the quantile strategy. A quantile of 0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to the maximum.", "funcAttrBody": "constant_ : float or array of shape [n_outputs] Mean or median or quantile of the training targets or constant value given by the user. n_outputs_ : int, Number of outputs. outputs_2d_ : bool, True if the output at fit is 2d, else false."},
{"libName": "sklearn.dummy", "methods": [{"methodName": "__init__(strategy=stratified, random_state=None, constant=None)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the random classifier.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of features.   y : array-like, shape = [n_samples] or [n_samples, n_outputs]   Target values.   sample_weight : array-like of shape = [n_samples], optional   Sample weights.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "y : array, shape = [n_samples] or [n_samples, n_outputs]   Predicted target values for X.  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Perform classification on test vectors X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Input vectors, where n_samples is the number of samples and n_features is the number of features.  "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "P : array-like or list of array-like of shape = [n_samples, n_classes]   Returns the log probability of the sample for each class in the model, where classes are ordered arithmetically for each output.  ", "methodParams": ["X"], "methodReturns": ["P"], "methodDesc": "Return log probability estimates for the test vectors X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Input vectors, where n_samples is the number of samples and n_features is the number of features.  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "P : array-like or list of array-lke of shape = [n_samples, n_classes]   Returns the probability of the sample for each class in the model, where classes are ordered arithmetically, for each output.  ", "methodParams": ["X"], "methodReturns": ["P"], "methodDesc": "Return probability estimates for the test vectors X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Input vectors, where n_samples is the number of samples and n_features is the number of features.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["strategy", "random_state", "constant"], "notes": "", "funcName": "DummyClassifier", "allFuncAttributes": ["classes_", "n_classes_", "class_prior_", "n_outputs_", "outputs_2d_", "sparse_output_"], "funcDesc": "DummyClassifier is a classifier that makes predictions using simple rules.", "funcParamBody": "strategy : str, default=stratified Strategy to use to generate predictions. stratified: generates predictions by respecting the training sets class distribution. most_frequent: always predicts the most frequent label in the training set. prior: always predicts the class that maximizes the class prior (like most_frequent) and predict_proba returns the class prior. uniform: generates predictions uniformly at random. constant: always predicts a constant label that is provided by the user. This is useful for metrics that evaluate a non-majority class New in version 0.17: Dummy Classifier now supports prior fitting strategy using parameter prior . random_state : int, RandomState instance or None, optional, default=None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . constant : int or str or array of shape = [n_outputs] The explicit constant as predicted by the constant strategy. This parameter is useful only for the constant strategy.", "funcAttrBody": "classes_ : array or list of array of shape = [n_classes] Class labels for each output. n_classes_ : array or list of array of shape = [n_classes] Number of label for each output. class_prior_ : array or list of array of shape = [n_classes] Probability of each class for each output. n_outputs_ : int, Number of outputs. outputs_2d_ : bool, True if the output at fit is 2d, else false. sparse_output_ : bool, True if the array returned from predict is to be in sparse CSC format. Is automatically set to True if the input y is passed in sparse format."},
{"libName": "sklearn.discriminant_analysis", "methods": [{"methodName": "__init__(priors=None, reg_param=0.0, store_covariance=False, tol=0.0001, store_covariances=None)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "C : array, shape = [n_samples, n_classes] or [n_samples,]   Decision function values related to each class, per sample. In the two-class case, the shape is [n_samples,], giving the log likelihood ratio of the positive class.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Apply decision function to an array of samples.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Array of samples (test vectors).  "}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vector, where n_samples is the number of samples and n_features is the number of features.   y : array, shape = [n_samples]   Target values (integers)  ", "methodDesc": "Fit the model according to the given training data and parameters."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples]", "methodParams": [], "methodReturns": [], "methodDesc": "Perform classification on an array of test vectors X. The predicted class C for each sample in X is returned.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]"}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "C : array, shape = [n_samples, n_classes]   Posterior log-probabilities of classification per class.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Return posterior probabilities of classification.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Array of samples/test vectors.  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "C : array, shape = [n_samples, n_classes]   Posterior probabilities of classification per class.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Return posterior probabilities of classification.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Array of samples/test vectors.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["priors", "reg_param", "store_covariance", "tol"], "notes": "", "funcName": "QuadraticDiscriminantAnalysis", "allFuncAttributes": ["covariance_", "means_", "priors_", "rotations_", "scalings_"], "funcDesc": "Quadratic Discriminant Analysis", "funcParamBody": "priors : array, optional, shape = [n_classes] Priors on classes reg_param : float, optional Regularizes the covariance estimate as (1-reg_param)*Sigma  +  reg_param*np.eye(n_features) store_covariance : boolean If True the covariance matrices are computed and stored in the self.covariance_ attribute. New in version 0.17. tol : float, optional, default 1.0e-4 Threshold used for rank estimation. New in version 0.17.", "funcAttrBody": "covariance_ : list of array-like, shape = [n_features, n_features] Covariance matrices of each class. means_ : array-like, shape = [n_classes, n_features] Class means. priors_ : array-like, shape = [n_classes] Class priors (sum to 1). rotations_ : list of arrays For each class k an array of shape [n_features, n_k], with n_k  =  min(n_features,  number  of  elements  in  class  k) scalings_ : list of arrays For each class k an array of shape [n_k]. It contains the scaling of the Gaussian distributions along its principal axes, i.e. the variance in the rotated coordinate system."},
{"libName": "sklearn.discriminant_analysis", "methods": [{"methodName": "__init__(solver=svd, shrinkage=None, priors=None, n_components=None, store_covariance=False, tol=0.0001)", "methodDesc": ""}, {"methodName": "decision_function(X)", "methodReturnsBody": "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) :   Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted.  ", "methodParams": ["X"], "methodReturns": ["array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"], "methodDesc": "Predict confidence scores for samples. The confidence score for a sample is the signed distance of that sample to the hyperplane.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = (n_samples, n_features)   Samples.  "}, {"methodParams": ["X", "y"], "methodName": "fit(X, y)", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data.   y : array, shape (n_samples,)   Target values.  ", "methodDesc": ""}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape = [n_samples]   Predicted class label per sample.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict class labels for samples in X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   Samples.  "}, {"methodName": "predict_log_proba(X)", "methodReturnsBody": "C : array, shape (n_samples, n_classes)   Estimated log probabilities.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Estimate log probability.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Input data.  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "C : array, shape (n_samples, n_classes)   Estimated probabilities.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Estimate probability.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Input data.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array, shape (n_samples, n_components)   Transformed data.  ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Project data to maximize class separation.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Input data.  "}], "allFuncParams": ["solver", "shrinkage", "priors", "n_components", "store_covariance", "tol"], "notes": "The default solver is svd. It can perform both classification and transform, and it does not rely on the calculation of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the svd solver cannot be used with shrinkage. The lsqr solver is an efficient algorithm that only works for classification. It supports shrinkage. The eigen solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the eigen solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features. The default solver is svd. It can perform both classification and transform, and it does not rely on the calculation of the covariance matrix. This can be an advantage in situations where the number of features is large. However, the svd solver cannot be used with shrinkage. The lsqr solver is an efficient algorithm that only works for classification. It supports shrinkage. The eigen solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the eigen solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features.", "funcName": "LinearDiscriminantAnalysis", "allFuncAttributes": ["coef_", "intercept_", "covariance_", "explained_variance_ratio_", "means_", "priors_", "scalings_", "xbar_", "classes_"], "funcDesc": "Linear Discriminant Analysis", "funcParamBody": "solver : string, optional Solver to use, possible values: svd: Singular value decomposition (default). Does not compute the covariance matrix, therefore this solver is recommended for data with a large number of features. lsqr: Least squares solution, can be combined with shrinkage. eigen: Eigenvalue decomposition, can be combined with shrinkage. shrinkage : string or float, optional Shrinkage parameter, possible values: None: no shrinkage (default). auto: automatic shrinkage using the Ledoit-Wolf lemma. float between 0 and 1: fixed shrinkage parameter. Note that shrinkage works only with lsqr and eigen solvers. priors : array, optional, shape (n_classes,) Class priors. n_components : int, optional Number of components (< n_classes - 1) for dimensionality reduction. store_covariance : bool, optional Additionally compute class covariance matrix (default False), used only in svd solver. New in version 0.17. tol : float, optional, (default 1.0e-4) Threshold used for rank estimation in SVD solver. New in version 0.17.", "funcAttrBody": "coef_ : array, shape (n_features,) or (n_classes, n_features) Weight vector(s). intercept_ : array, shape (n_features,) Intercept term. covariance_ : array-like, shape (n_features, n_features) Covariance matrix (shared by all classes). explained_variance_ratio_ : array, shape (n_components,) Percentage of variance explained by each of the selected components. If n_components is not set then all components are stored and the sum of explained variances is equal to 1.0. Only available when eigen or svd solver is used. means_ : array-like, shape (n_classes, n_features) Class means. priors_ : array-like, shape (n_classes,) Class priors (sum to 1). scalings_ : array-like, shape (rank, n_classes - 1) Scaling of the features in the space spanned by the class centroids. xbar_ : array-like, shape (n_features,) Overall mean. classes_ : array-like, shape (n_classes,) Unique class labels."},
{"allReturnParams": ["code"], "libName": "sklearn.decomposition", "methods": [], "notes": "", "funcName": "sparse_encode", "allFuncParams": ["X", "dictionary", "gram", "cov", "algorithm", "n_nonzero_coefs", "alpha", "copy_cov", "init", "max_iter", "n_jobs", "check_input", "verbose"], "funcDesc": "Sparse coding", "funcParamBody": "X : array of shape (n_samples, n_features) Data matrix dictionary : array of shape (n_components, n_features) The dictionary matrix against which to solve the sparse coding of the data. Some of the algorithms assume normalized rows for meaningful output. gram : array, shape=(n_components, n_components) Precomputed Gram matrix, dictionary * dictionary cov : array, shape=(n_components, n_samples) Precomputed covariance, dictionary * X algorithm : {lasso_lars, lasso_cd, lars, omp, threshold} lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection dictionary * X n_nonzero_coefs : int, 0.1 * n_features by default Number of nonzero coefficients to target in each column of the solution. This is only used by algorithm=lars and algorithm=omp alpha in the omp case. alpha : float, 1. by default If algorithm=lasso_lars or algorithm=lasso_cd , alpha is the penalty applied to the L1 norm. If algorithm=threshold , alpha is the absolute value of the threshold below which coefficients will be squashed to zero. If algorithm=omp , alpha is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides n_nonzero_coefs . copy_cov : boolean, optional Whether to copy the precomputed covariance matrix; if False, it may be overwritten. init : array of shape (n_samples, n_components) Initialization value of the sparse codes. Only used if algorithm=lasso_cd . max_iter : int, 1000 by default Maximum number of iterations to perform if algorithm=lasso_cd . n_jobs : int, optional Number of parallel jobs to run. check_input : boolean, optional If False, the input arrays X and dictionary will not be checked. verbose : int, optional Controls the verbosity; the higher, the more messages. Defaults to 0.", "funcReturnBody": "code : array of shape (n_samples, n_components) The sparse codes"},
{"allReturnParams": ["K", "W", "S", "X_mean", "n_iter"], "libName": "sklearn.decomposition", "methods": [], "notes": "The data matrix X is considered to be a linear combination of non-Gaussian (independent) components i.e. X = AS where columns of S contain the independent components and A is a linear mixing matrix. In short ICA attempts to un-mix the data by estimating an un-mixing matrix W where ``S = W K X.` This implementation was originally made for data of shape [n_features, n_samples]. Now the input is transposed before the algorithm is applied. This makes it slightly faster for Fortran-ordered input. Implemented using FastICA: A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430", "funcName": "fastica", "allFuncParams": ["X", "n_components", "algorithm", "whiten", "fun", "fun_args", "max_iter", "tol", "w_init", "random_state", "return_X_mean", "compute_sources", "return_n_iter"], "funcDesc": "Perform Fast Independent Component Analysis.", "funcParamBody": "X : array-like, shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. n_components : int, optional Number of components to extract. If None no dimension reduction is performed. algorithm : {parallel, deflation}, optional Apply a parallel or deflational FASTICA algorithm. whiten : boolean, optional If True perform an initial whitening of the data. If False, the data is assumed to have already been preprocessed: it should be centered, normed and white. Otherwise you will get incorrect results. In this case the parameter n_components will be ignored. fun : string or function, optional. Default: logcosh The functional form of the G function used in the approximation to neg-entropy. Could be either logcosh, exp, or cube. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. Example: def my_g(x): return x ** 3, 3 * x ** 2 fun_args : dictionary, optional Arguments to send to the functional form. If empty or None and if fun=logcosh, fun_args will take value {alpha : 1.0} max_iter : int, optional Maximum number of iterations to perform. tol : float, optional A positive scalar giving the tolerance at which the un-mixing matrix is considered to have converged. w_init : (n_components, n_components) array, optional Initial un-mixing array of dimension (n.comp,n.comp). If None (default) then an array of normal r.v.s is used. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . return_X_mean : bool, optional If True, X_mean is returned too. compute_sources : bool, optional If False, sources are not computed, but only the rotation matrix. This can save memory when working with big data. Defaults to True. return_n_iter : bool, optional Whether or not to return the number of iterations.", "funcReturnBody": "K : array, shape (n_components, n_features) | None. If whiten is True, K is the pre-whitening matrix that projects data onto the first n_components principal components. If whiten is False, K is None. W : array, shape (n_components, n_components) Estimated un-mixing matrix. The mixing matrix can be obtained by: w  =  np . dot ( W ,  K . T ) A  =  w . T  *  ( w  *  w . T ) . I S : array, shape (n_samples, n_components) | None Estimated source matrix X_mean : array, shape (n_features, ) The mean over features. Returned only if return_X_mean is True. n_iter : int If the algorithm is deflation, n_iter is the maximum number of iterations run across all components. Else they are just the number of iterations taken to converge. This is returned only when return_n_iter is set to True ."},
{"allReturnParams": ["code", "dictionary", "n_iter"], "libName": "sklearn.decomposition", "methods": [], "notes": "", "funcName": "dict_learning_online", "allFuncParams": ["X", "n_components", "alpha", "n_iter", "return_code", "dict_init", "callback", "batch_size", "verbose", "shuffle", "n_jobs", "method", "iter_offset", "random_state", "return_inner_stats", "inner_stats", "return_n_iter"], "funcDesc": "Solves a dictionary learning matrix factorization problem online.", "funcParamBody": "X : array of shape (n_samples, n_features) Data matrix. n_components : int, Number of dictionary atoms to extract. alpha : float, Sparsity controlling parameter. n_iter : int, Number of iterations to perform. return_code : boolean, Whether to also return the code U or just the dictionary V. dict_init : array of shape (n_components, n_features), Initial value for the dictionary for warm restart scenarios. callback : callable or None, optional (default: None) callable that gets invoked every five iterations batch_size : int, The number of samples to take in each batch. verbose : bool, optional (default: False) To control the verbosity of the procedure. shuffle : boolean, Whether to shuffle the data before splitting it in batches. n_jobs : int, Number of parallel jobs to run, or -1 to autodetect. method : {lars, cd} lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse. iter_offset : int, default 0 Number of previous iterations completed on the dictionary used for initialization. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . return_inner_stats : boolean, optional Return the inner statistics A (dictionary covariance) and B (data approximation). Useful to restart the algorithm in an online setting. If return_inner_stats is True, return_code is ignored inner_stats : tuple of (A, B) ndarrays Inner sufficient statistics that are kept by the algorithm. Passing them at initialization is useful in online settings, to avoid loosing the history of the evolution. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix return_n_iter : bool Whether or not to return the number of iterations.", "funcReturnBody": "code : array of shape (n_samples, n_components), the sparse code (only returned if return_code=True ) dictionary : array of shape (n_components, n_features), the solutions to the dictionary learning problem n_iter : int Number of iterations run. Returned only if return_n_iter is set to True ."},
{"allReturnParams": ["code", "dictionary", "errors", "n_iter"], "libName": "sklearn.decomposition", "methods": [], "notes": "", "funcName": "dict_learning", "allFuncParams": ["X", "n_components", "alpha", "max_iter", "tol", "method", "n_jobs", "dict_init", "code_init", "callback", "verbose", "random_state", "return_n_iter"], "funcDesc": "Solves a dictionary learning matrix factorization problem.", "funcParamBody": "X : array of shape (n_samples, n_features) Data matrix. n_components : int, Number of dictionary atoms to extract. alpha : int, Sparsity controlling parameter. max_iter : int, Maximum number of iterations to perform. tol : float, Tolerance for the stopping condition. method : {lars, cd} lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse. n_jobs : int, Number of parallel jobs to run, or -1 to autodetect. dict_init : array of shape (n_components, n_features), Initial value for the dictionary for warm restart scenarios. code_init : array of shape (n_samples, n_components), Initial value for the sparse code for warm restart scenarios. callback : callable or None, optional (default: None) Callable that gets invoked every five iterations verbose : bool, optional (default: False) To control the verbosity of the procedure. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . return_n_iter : bool Whether or not to return the number of iterations.", "funcReturnBody": "code : array of shape (n_samples, n_components) The sparse code factor in the matrix factorization. dictionary : array of shape (n_components, n_features), The dictionary factor in the matrix factorization. errors : array Vector of errors at each iteration. n_iter : int Number of iterations run. Returned only if return_n_iter is set to True."},
{"libName": "sklearn.decomposition", "methods": [{"methodName": "__init__(n_components=2, algorithm=randomized, n_iter=5, random_state=None, tol=0.0)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns the transformer object.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit LSI model on training data X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Training data.   y : Ignored. "}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "X_new : array, shape (n_samples, n_components)   Reduced version of X. This will always be a dense array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit LSI model to X and perform dimensionality reduction on X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Training data.   y : Ignored. "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_original : array, shape (n_samples, n_features)   Note that this is always a dense array.  ", "methodParams": ["X"], "methodReturns": ["X_original"], "methodDesc": "Transform X back to its original space. Returns an array X_original whose transform would be X.", "methodParamsBody": "X : array-like, shape (n_samples, n_components)   New data.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array, shape (n_samples, n_components)   Reduced version of X. This will always be a dense array.  ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Perform dimensionality reduction on X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   New data.  "}], "allFuncParams": ["n_components", "algorithm", "n_iter", "random_state", "tol"], "notes": "SVD suffers from a problem called sign indeterminancy, which means the sign of the components_ and the output from transform depend on the algorithm and random state. To work around this, fit instances of this class to data once, then keep the instance around to do transformations. Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061 SVD suffers from a problem called sign indeterminancy, which means the sign of the components_ and the output from transform depend on the algorithm and random state. To work around this, fit instances of this class to data once, then keep the instance around to do transformations. Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions Halko, et al., 2009 (arXiv:909) http://arxiv.org/pdf/0909.4061", "funcName": "TruncatedSVD", "allFuncAttributes": ["components_", "explained_variance_", "explained_variance_ratio_", "singular_values_"], "funcDesc": "Dimensionality reduction using truncated SVD (aka LSA).", "funcParamBody": "n_components : int, default = 2 Desired dimensionality of output data. Must be strictly less than the number of features. The default value is useful for visualisation. For LSA, a value of 100 is recommended. algorithm : string, default = randomized SVD solver to use. Either arpack for the ARPACK wrapper in SciPy (scipy.sparse.linalg.svds), or randomized for the randomized algorithm due to Halko (2009). n_iter : int, optional (default 5) Number of iterations for randomized SVD solver. Not used by ARPACK. The default is larger than the default in randomized_svd to handle sparse matrices that may have large slowly decaying spectrum. random_state : int, RandomState instance or None, optional, default = None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . tol : float, optional Tolerance for ARPACK. 0 means machine precision. Ignored by randomized SVD solver.", "funcAttrBody": "components_ : array, shape (n_components, n_features) explained_variance_ : array, shape (n_components,) The variance of the training samples transformed by a projection to each component. explained_variance_ratio_ : array, shape (n_components,) Percentage of variance explained by each of the selected components. singular_values_ : array, shape (n_components,) The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the n_components"},
{"libName": "sklearn.decomposition", "methods": [{"methodName": "__init__(dictionary, transform_algorithm=omp, transform_n_nonzero_coefs=None, transform_alpha=None, split_sign=False, n_jobs=1)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns the object itself  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Do nothing and return the estimator unchanged This method is just there to implement the usual API and hence work in pipelines.", "methodParamsBody": "X : Ignored.  y : Ignored. "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array, shape (n_samples, n_components)   Transformed data  ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Encode the data as a sparse combination of the dictionary atoms. Coding method is determined by the object parameter transform_algorithm .", "methodParamsBody": "X : array of shape (n_samples, n_features)   Test data to be transformed, must have the same number of features as the data used to train the model.  "}], "allFuncParams": ["dictionary", "transform_algorithm", "transform_n_nonzero_coefs", "transform_alpha", "split_sign", "n_jobs"], "notes": "", "funcName": "SparseCoder", "allFuncAttributes": ["components_"], "funcDesc": "Sparse coding", "funcParamBody": "dictionary : array, [n_components, n_features] The dictionary atoms used for sparse coding. Lines are assumed to be normalized to unit norm. transform_algorithm : {lasso_lars, lasso_cd, lars, omp,     threshold} Algorithm used to transform the data: lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection dictionary  *  X' transform_n_nonzero_coefs : int, 0.1  *  n_features by default Number of nonzero coefficients to target in each column of the solution. This is only used by algorithm=lars and algorithm=omp alpha in the omp case. transform_alpha : float, 1. by default If algorithm=lasso_lars or algorithm=lasso_cd , alpha is the penalty applied to the L1 norm. If algorithm=threshold , alpha is the absolute value of the threshold below which coefficients will be squashed to zero. If algorithm=omp , alpha is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides n_nonzero_coefs . split_sign : bool, False by default Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers. n_jobs : int, number of parallel jobs to run", "funcAttrBody": "components_ : array, [n_components, n_features] The unchanged dictionary atoms"},
{"libName": "sklearn.decomposition", "methods": [{"methodName": "__init__(n_components=None, alpha=1, ridge_alpha=0.01, max_iter=1000, tol=1e-08, method=lars, n_jobs=1, U_init=None, V_init=None, verbose=False, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns the instance itself.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model from data in X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training vector, where n_samples in the number of samples and n_features is the number of features.   y : Ignored. "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X, ridge_alpha=deprecated)", "methodReturnsBody": "X_new array, shape (n_samples, n_components) :   Transformed data.  ", "methodParams": ["X", "ridge_alpha"], "methodReturns": ["X_new array, shape (n_samples, n_components)"], "methodDesc": "Least Squares projection of the data onto the sparse components. To avoid instability issues in case the system is under-determined, regularization can be applied (Ridge regression) via the ridge_alpha parameter. Note that Sparse PCA components orthogonality is not enforced as in PCA hence one cannot use a simple linear projection.", "methodParamsBody": "X : array of shape (n_samples, n_features)   Test data to be transformed, must have the same number of features as the data used to train the model.   ridge_alpha : float, default: 0.01   Amount of ridge shrinkage to apply in order to improve conditioning.   Deprecated since version 0.19: This parameter will be removed in 0.21. Specify ridge_alpha in the SparsePCA constructor.   "}], "allFuncParams": ["n_components", "alpha", "ridge_alpha", "max_iter", "tol", "method", "n_jobs", "U_init", "V_init", "verbose", "random_state"], "notes": "", "funcName": "SparsePCA", "allFuncAttributes": ["components_", "error_", "n_iter_"], "funcDesc": "Sparse Principal Components Analysis (SparsePCA)", "funcParamBody": "n_components : int, Number of sparse atoms to extract. alpha : float, Sparsity controlling parameter. Higher values lead to sparser components. ridge_alpha : float, Amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method. max_iter : int, Maximum number of iterations to perform. tol : float, Tolerance for the stopping condition. method : {lars, cd} lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse. n_jobs : int, Number of parallel jobs to run. U_init : array of shape (n_samples, n_components), Initial values for the loadings for warm restart scenarios. V_init : array of shape (n_components, n_features), Initial values for the components for warm restart scenarios. verbose : int Controls the verbosity; the higher, the more messages. Defaults to 0. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcAttrBody": "components_ : array, [n_components, n_features] Sparse components extracted from the data. error_ : array Vector of errors at each iteration. n_iter_ : int Number of iterations run."},
{"libName": "sklearn.decomposition", "methods": [{"methodName": "__init__(n_components=None, copy=True, whiten=False, svd_solver=auto, tol=0.0, iterated_power=auto, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns the instance itself.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model with X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples in the number of samples and n_features is the number of features.   y : Ignored. "}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components) ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit the model with X and apply the dimensionality reduction on X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : Ignored. "}, {"methodName": "get_covariance()", "methodReturns": ["cov"], "methodDesc": "Compute data covariance with the generative model. cov  =  components_.T  *  S**2  *  components_  +  sigma2  *  eye(n_features) where  S**2 contains the explained variances, and sigma2 contains the noise variances.", "methodReturnsBody": "cov : array, shape=(n_features, n_features)   Estimated covariance of data.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_precision()", "methodReturns": ["precision"], "methodDesc": "Compute data precision matrix with the generative model. Equals the inverse of the covariance but computed with the matrix inversion lemma for efficiency.", "methodReturnsBody": "precision : array, shape=(n_features, n_features)   Estimated precision of data.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_original array-like, shape (n_samples, n_features) : ", "methodParams": ["X"], "methodReturns": ["X_original array-like, shape (n_samples, n_features)"], "methodDesc": "Transform data back to its original space. In other words, return an input X_original whose transform would be X. Notes If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening.", "methodParamsBody": "X : array-like, shape (n_samples, n_components)   New data, where n_samples is the number of samples and n_components is the number of components.  "}, {"methodName": "score(X, y=None)", "methodReturnsBody": "ll : float   Average log-likelihood of the samples under the current model  ", "methodParams": ["X", "y"], "methodReturns": ["ll"], "methodDesc": "Return the average log-likelihood of all samples. See. Pattern Recognition and Machine Learning by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf", "methodParamsBody": "X : array, shape(n_samples, n_features)   The data.   y : Ignored. "}, {"methodName": "score_samples(X)", "methodReturnsBody": "ll : array, shape (n_samples,)   Log-likelihood of each sample under the current model  ", "methodParams": ["X"], "methodReturns": ["ll"], "methodDesc": "Return the log-likelihood of each sample. See. Pattern Recognition and Machine Learning by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf", "methodParamsBody": "X : array, shape(n_samples, n_features)   The data.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components) ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Apply dimensionality reduction to X. X is projected on the first principal components previously extracted from a training set. Examples", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   New data, where n_samples is the number of samples and n_features is the number of features.  "}], "allFuncParams": ["n_components", "copy", "whiten", "svd_solver", "tol", "iterated_power", "random_state"], "notes": "If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening. If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening.", "funcName": "PCA", "allFuncAttributes": ["components_", "explained_variance_", "explained_variance_ratio_", "singular_values_", "mean_", "n_components_", "noise_variance_"], "funcDesc": "Principal component analysis (PCA)", "funcParamBody": "n_components : int, float, None or string Number of components to keep. if n_components is not set all components are kept: n_components  ==  min ( n_samples ,  n_features ) if n_components == mle and svd_solver == full, Minkas MLE is used to guess the dimension if 0  <  n_components  <  1 and svd_solver == full, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components n_components cannot be equal to n_features for svd_solver == arpack. copy : bool (default True) If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the expected results, use fit_transform(X) instead. whiten : bool, optional (default False) When True (False by default) the components_ vectors are multiplied by the square root of n_samples and then divided by the singular values to ensure uncorrelated outputs with unit component-wise variances. Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometime improve the predictive accuracy of the downstream estimators by making their data respect some hard-wired assumptions. svd_solver : string {auto, full, arpack, randomized} auto : the solver is selected by a default policy based on X.shape and n_components : if the input data is larger than 500x500 and the number of components to extract is lower than 80% of the smallest dimension of the data, then the more efficient randomized method is enabled. Otherwise the exact full SVD is computed and optionally truncated afterwards. full : run exact full SVD calling the standard LAPACK solver via scipy.linalg.svd and select the components by postprocessing arpack : run SVD truncated to n_components calling ARPACK solver via scipy.sparse.linalg.svds . It requires strictly 0 < n_components < X.shape[1] randomized : run randomized SVD by the method of Halko et al. New in version 0.18.0. tol : float >= 0, optional (default .0) Tolerance for singular values computed by svd_solver == arpack. New in version 0.18.0. iterated_power : int >= 0, or auto, (default auto) Number of iterations for the power method computed by svd_solver == randomized. New in version 0.18.0. random_state : int, RandomState instance or None, optional (default None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when svd_solver == arpack or randomized. New in version 0.18.0.", "funcAttrBody": "components_ : array, shape (n_components, n_features) Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explained_variance_ . explained_variance_ : array, shape (n_components,) The amount of variance explained by each of the selected components. Equal to n_components largest eigenvalues of the covariance matrix of X. New in version 0.18. explained_variance_ratio_ : array, shape (n_components,) Percentage of variance explained by each of the selected components. If n_components is not set then all components are stored and the sum of explained variances is equal to 1.0. singular_values_ : array, shape (n_components,) The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the n_components mean_ : array, shape (n_features,) Per-feature empirical mean, estimated from the training set. Equal to X.mean(axis=0) . n_components_ : int The estimated number of components. When n_components is set to mle or a number between 0 and 1 (with svd_solver == full) this number is estimated from input data. Otherwise it equals the parameter n_components, or n_features if n_components is None. noise_variance_ : float The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See Pattern Recognition and Machine Learning by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf . It is required to computed the estimated data covariance and score samples. Equal to the average of (min(n_features, n_samples) - n_components) smallest eigenvalues of the covariance matrix of X."},
{"libName": "sklearn.decomposition", "methods": [{"methodName": "__init__(n_components=None, init=None, solver=cd, beta_loss=frobenius, tol=0.0001, max_iter=200, random_state=None, alpha=0.0, l1_ratio=0.0, verbose=0, shuffle=False)", "methodDesc": ""}, {"methodName": "fit(X, y=None, **params)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Learn a NMF model for the data X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Data matrix to be decomposed   y : Ignored. "}, {"methodName": "fit_transform(X, y=None, W=None, H=None)", "methodReturnsBody": "W : array, shape (n_samples, n_components)   Transformed data.  ", "methodParams": ["X", "y", "W", "H"], "methodReturns": ["W"], "methodDesc": "Learn a NMF model for the data X and returns the transformed data. This is more efficient than calling fit followed by transform.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Data matrix to be decomposed   y : Ignored.  W : array-like, shape (n_samples, n_components)   If init=custom, it is used as initial guess for the solution.   H : array-like, shape (n_components, n_features)   If init=custom, it is used as initial guess for the solution.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "inverse_transform(W)", "methodReturnsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Data matrix of original shape   .. versionadded:: 0.18 : ", "methodParams": ["W"], "methodReturns": ["X", ".. versionadded:: 0.18"], "methodDesc": "Transform data back to its original space.", "methodParamsBody": "W : {array-like, sparse matrix}, shape (n_samples, n_components)   Transformed data matrix  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "W : array, shape (n_samples, n_components)   Transformed data  ", "methodParams": ["X"], "methodReturns": ["W"], "methodDesc": "Transform the data X according to the fitted NMF model", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Data matrix to be transformed by the model  "}], "allFuncParams": ["n_components", "init", "solver", "beta_loss", "tol", "max_iter", "random_state", "alpha", "l1_ratio", "verbose", "shuffle"], "notes": "", "funcName": "NMF", "allFuncAttributes": ["components_", "reconstruction_err_", "n_iter_"], "funcDesc": "Non-Negative Matrix Factorization (NMF)", "funcParamBody": "n_components : int or None Number of components, if n_components is not set all features are kept. init :  random | nndsvd |  nndsvda | nndsvdar | custom Method used to initialize the procedure. Default: nndsvd if n_components < n_features, otherwise random. Valid options: random: non-negative random matrices, scaled with: sqrt(X.mean() / n_components) nndsvd: Nonnegative Double Singular Value Decomposition (NNDSVD) initialization (better for sparseness) nndsvda: NNDSVD with zeros filled with the average of X (better when sparsity is not desired) nndsvdar: NNDSVD with zeros filled with small random values (generally faster, less accurate alternative to NNDSVDa for when sparsity is not desired) custom: use custom matrices W and H solver : cd | mu Numerical solver to use: cd is a Coordinate Descent solver. mu is a Multiplicative Update solver. New in version 0.17: Coordinate Descent solver. New in version 0.19: Multiplicative Update solver. beta_loss : float or string, default frobenius String must be in {frobenius, kullback-leibler, itakura-saito}. Beta divergence to be minimized, measuring the distance between X and the dot product WH. Note that values different from frobenius (or 2) and kullback-leibler (or 1) lead to significantly slower fits. Note that for beta_loss <= 0 (or itakura-saito), the input matrix X cannot contain zeros. Used only in mu solver. New in version 0.19. tol : float, default: 1e-4 Tolerance of the stopping condition. max_iter : integer, default: 200 Maximum number of iterations before timing out. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . alpha : double, default: 0. Constant that multiplies the regularization terms. Set it to zero to have no regularization. New in version 0.17: alpha used in the Coordinate Descent solver. l1_ratio : double, default: 0. The regularization mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2. New in version 0.17: Regularization parameter l1_ratio used in the Coordinate Descent solver. verbose : bool, default=False Whether to be verbose. shuffle : boolean, default: False If true, randomize the order of coordinates in the CD solver. New in version 0.17: shuffle parameter used in the Coordinate Descent solver.", "funcAttrBody": "components_ : array, [n_components, n_features] Factorization matrix, sometimes called dictionary. reconstruction_err_ : number Frobenius norm of the matrix difference, or beta-divergence, between the training data X and the reconstructed data WH from the fitted model. n_iter_ : int Actual number of iterations."},
{"libName": "sklearn.decomposition", "methods": [{"methodName": "__init__(n_components=None, alpha=1, ridge_alpha=0.01, n_iter=100, callback=None, batch_size=3, verbose=False, shuffle=True, n_jobs=1, method=lars, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns the instance itself.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model from data in X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training vector, where n_samples in the number of samples and n_features is the number of features.   y : Ignored. "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X, ridge_alpha=deprecated)", "methodReturnsBody": "X_new array, shape (n_samples, n_components) :   Transformed data.  ", "methodParams": ["X", "ridge_alpha"], "methodReturns": ["X_new array, shape (n_samples, n_components)"], "methodDesc": "Least Squares projection of the data onto the sparse components. To avoid instability issues in case the system is under-determined, regularization can be applied (Ridge regression) via the ridge_alpha parameter. Note that Sparse PCA components orthogonality is not enforced as in PCA hence one cannot use a simple linear projection.", "methodParamsBody": "X : array of shape (n_samples, n_features)   Test data to be transformed, must have the same number of features as the data used to train the model.   ridge_alpha : float, default: 0.01   Amount of ridge shrinkage to apply in order to improve conditioning.   Deprecated since version 0.19: This parameter will be removed in 0.21. Specify ridge_alpha in the SparsePCA constructor.   "}], "allFuncParams": ["n_components", "alpha", "ridge_alpha", "n_iter", "callback", "batch_size", "verbose", "shuffle", "n_jobs", "method", "random_state"], "notes": "", "funcName": "MiniBatchSparsePCA", "allFuncAttributes": ["components_", "error_", "n_iter_"], "funcDesc": "Mini-batch Sparse Principal Components Analysis", "funcParamBody": "n_components : int, number of sparse atoms to extract alpha : int, Sparsity controlling parameter. Higher values lead to sparser components. ridge_alpha : float, Amount of ridge shrinkage to apply in order to improve conditioning when calling the transform method. n_iter : int, number of iterations to perform for each mini batch callback : callable or None, optional (default: None) callable that gets invoked every five iterations batch_size : int, the number of features to take in each mini batch verbose : int Controls the verbosity; the higher, the more messages. Defaults to 0. shuffle : boolean, whether to shuffle the data before splitting it in batches n_jobs : int, number of parallel jobs to run, or -1 to autodetect. method : {lars, cd} lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcAttrBody": "components_ : array, [n_components, n_features] Sparse components extracted from the data. error_ : array Vector of errors at each iteration. n_iter_ : int Number of iterations run."},
{"libName": "sklearn.decomposition", "methods": [{"methodName": "__init__(n_components=None, alpha=1, n_iter=1000, fit_algorithm=lars, n_jobs=1, batch_size=3, shuffle=True, dict_init=None, transform_algorithm=omp, transform_n_nonzero_coefs=None, transform_alpha=None, verbose=False, split_sign=False, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns the instance itself.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model from data in X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training vector, where n_samples in the number of samples and n_features is the number of features.   y : Ignored. "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "partial_fit(X, y=None, iter_offset=None)", "methodReturnsBody": "self : object   Returns the instance itself.  ", "methodParams": ["X", "y", "iter_offset"], "methodReturns": ["self"], "methodDesc": "Updates the model using the data in X as a mini-batch.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training vector, where n_samples in the number of samples and n_features is the number of features.   y : Ignored.  iter_offset : integer, optional   The number of iteration on data batches that has been performed before this call to partial_fit. This is optional: if no number is passed, the memory of the object is used.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array, shape (n_samples, n_components)   Transformed data  ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Encode the data as a sparse combination of the dictionary atoms. Coding method is determined by the object parameter transform_algorithm .", "methodParamsBody": "X : array of shape (n_samples, n_features)   Test data to be transformed, must have the same number of features as the data used to train the model.  "}], "allFuncParams": ["n_components", "alpha", "n_iter", "fit_algorithm", "n_jobs", "batch_size", "shuffle", "dict_init", "transform_algorithm", "transform_n_nonzero_coefs", "transform_alpha", "verbose", "split_sign", "random_state"], "notes": "References: J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding ( http://www.di.ens.fr/sierra/pdfs/icml09.pdf ) References: J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding ( http://www.di.ens.fr/sierra/pdfs/icml09.pdf )", "funcName": "MiniBatchDictionaryLearning", "allFuncAttributes": ["components_", "inner_stats_", "n_iter_"], "funcDesc": "Mini-batch dictionary learning", "funcParamBody": "n_components : int, number of dictionary elements to extract alpha : float, sparsity controlling parameter n_iter : int, total number of iterations to perform fit_algorithm : {lars, cd} lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse. n_jobs : int, number of parallel jobs to run batch_size : int, number of samples in each mini-batch shuffle : bool, whether to shuffle the samples before forming batches dict_init : array of shape (n_components, n_features), initial value of the dictionary for warm restart scenarios transform_algorithm : {lasso_lars, lasso_cd, lars, omp,     threshold} Algorithm used to transform the data. lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection dictionary * X transform_n_nonzero_coefs : int, 0.1  *  n_features by default Number of nonzero coefficients to target in each column of the solution. This is only used by algorithm=lars and algorithm=omp alpha in the omp case. transform_alpha : float, 1. by default If algorithm=lasso_lars or algorithm=lasso_cd , alpha is the penalty applied to the L1 norm. If algorithm=threshold , alpha is the absolute value of the threshold below which coefficients will be squashed to zero. If algorithm=omp , alpha is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides n_nonzero_coefs . verbose : bool, optional (default: False) To control the verbosity of the procedure. split_sign : bool, False by default Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcAttrBody": "components_ : array, [n_components, n_features] components extracted from the data inner_stats_ : tuple of (A, B) ndarrays Internal sufficient statistics that are kept by the algorithm. Keeping them is useful in online settings, to avoid loosing the history of the evolution, but they shouldnt have any use for the end user. A (n_components, n_components) is the dictionary covariance matrix. B (n_features, n_components) is the data approximation matrix n_iter_ : int Number of iterations run."},
{"libName": "sklearn.decomposition", "methods": [{"methodName": "__init__(n_components=10, doc_topic_prior=None, topic_word_prior=None, learning_method=None, learning_decay=0.7, learning_offset=10.0, max_iter=10, batch_size=128, evaluate_every=-1, total_samples=1000000.0, perp_tol=0.1, mean_change_tol=0.001, max_doc_update_iter=100, n_jobs=1, verbose=0, random_state=None, n_topics=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Learn model for the data X with variational Bayes method. When learning_method is online, use mini-batch update. Otherwise, use batch update.", "methodParamsBody": "X : array-like or sparse matrix, shape=(n_samples, n_features)   Document word matrix.   y : Ignored. "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "partial_fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Online VB with Mini-Batch update.", "methodParamsBody": "X : array-like or sparse matrix, shape=(n_samples, n_features)   Document word matrix.   y : Ignored. "}, {"methodName": "perplexity(X, doc_topic_distr=deprecated, sub_sampling=False)", "methodReturnsBody": "score : float   Perplexity score.  ", "methodParams": ["X", "doc_topic_distr", "sub_sampling"], "methodReturns": ["score"], "methodDesc": "Calculate approximate perplexity for data X. Perplexity is defined as exp(-1. * log-likelihood per word)", "methodParamsBody": "X : array-like or sparse matrix, [n_samples, n_features]   Document word matrix.   doc_topic_distr : None or array, shape=(n_samples, n_components)   Document topic distribution. This argument is deprecated and is currently being ignored.   Deprecated since version 0.19.    sub_sampling : bool   Do sub-sampling or not.  "}, {"methodName": "score(X, y=None)", "methodReturnsBody": "score : float   Use approximate bound as score.  ", "methodParams": ["X", "y"], "methodReturns": ["score"], "methodDesc": "Calculate approximate log-likelihood as score.", "methodParamsBody": "X : array-like or sparse matrix, shape=(n_samples, n_features)   Document word matrix.   y : Ignored. "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "doc_topic_distr : shape=(n_samples, n_components)   Document topic distribution for X.  ", "methodParams": ["X"], "methodReturns": ["doc_topic_distr"], "methodDesc": "Transform data X according to the fitted model.", "methodParamsBody": "X : array-like or sparse matrix, shape=(n_samples, n_features)   Document word matrix.  "}], "allFuncParams": ["n_components", "doc_topic_prior", "topic_word_prior", "learning_method", "learning_decay", "learning_offset", "max_iter", "batch_size", "evaluate_every", "total_samples", "perp_tol", "mean_change_tol", "max_doc_update_iter", "n_jobs", "verbose", "random_state", "n_topics"], "notes": "", "funcName": "LatentDirichletAllocation", "allFuncAttributes": ["components_", "n_batch_iter_", "n_iter_"], "funcDesc": "Latent Dirichlet Allocation with online variational Bayes algorithm", "funcParamBody": "n_components : int, optional (default=10) Number of topics. doc_topic_prior : float, optional (default=None) Prior of document topic distribution theta . If the value is None, defaults to 1 / n_components . In the literature, this is called alpha . topic_word_prior : float, optional (default=None) Prior of topic word distribution beta . If the value is None, defaults to 1 / n_components . In the literature, this is called eta . learning_method : batch | online, default=online Method used to update _component . Only used in fit method. In general, if the data size is large, the online update will be much faster than the batch update. The default learning method is going to be changed to batch in the 0.20 release. Valid options: 'batch': Batch variational Bayes method. Use all training data in     each EM update.     Old `components_` will be overwritten in each iteration. 'online': Online variational Bayes method. In each EM update, use     mini-batch of training data to update the ``components_``     variable incrementally. The learning rate is controlled by the     ``learning_decay`` and the ``learning_offset`` parameters. learning_decay : float, optional (default=0.7) It is a parameter that control learning rate in the online learning method. The value should be set between (0.5, 1.0] to guarantee asymptotic convergence. When the value is 0.0 and batch_size is n_samples , the update method is same as batch learning. In the literature, this is called kappa. learning_offset : float, optional (default=10.) A (positive) parameter that downweights early iterations in online learning.  It should be greater than 1.0. In the literature, this is called tau_0. max_iter : integer, optional (default=10) The maximum number of iterations. batch_size : int, optional (default=128) Number of documents to use in each EM iteration. Only used in online learning. evaluate_every : int, optional (default=0) How often to evaluate perplexity. Only used in fit method. set it to 0 or negative number to not evalute perplexity in training at all. Evaluating perplexity can help you check convergence in training process, but it will also increase total training time. Evaluating perplexity in every iteration might increase training time up to two-fold. total_samples : int, optional (default=1e6) Total number of documents. Only used in the partial_fit method. perp_tol : float, optional (default=1e-1) Perplexity tolerance in batch learning. Only used when evaluate_every is greater than 0. mean_change_tol : float, optional (default=1e-3) Stopping tolerance for updating document topic distribution in E-step. max_doc_update_iter : int (default=100) Max number of iterations for updating document topic distribution in the E-step. n_jobs : int, optional (default=1) The number of jobs to use in the E-step. If -1, all CPUs are used. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. verbose : int, optional (default=0) Verbosity level. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_topics : int, optional (default=None) This parameter has been renamed to n_components and will be removed in version 0.21. .. deprecated:: 0.19", "funcAttrBody": "components_ : array, [n_components, n_features] Variational parameters for topic word distribution. Since the complete conditional for topic word distribution is a Dirichlet, components_[i,  j] can be viewed as pseudocount that represents the number of times word j was assigned to topic i . It can also be viewed as distribution over the words for each topic after normalization: model.components_  /  model.components_.sum(axis=1)[:,  np.newaxis] . n_batch_iter_ : int Number of iterations of the EM step. n_iter_ : int Number of passes over the dataset."},
{"libName": "sklearn.decomposition", "methods": [{"methodName": "__init__(n_components=None, kernel=linear, gamma=None, degree=3, coef0=1, kernel_params=None, alpha=1.0, fit_inverse_transform=False, eigen_solver=auto, tol=0, max_iter=None, remove_zero_eig=False, random_state=None, copy_X=True, n_jobs=1)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns the instance itself.  ", "methodParams": ["X"], "methodReturns": ["self"], "methodDesc": "Fit the model from data in X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training vector, where n_samples in the number of samples and n_features is the number of features.  "}, {"methodName": "fit_transform(X, y=None, **params)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components) ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Fit the model from data in X and transform X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training vector, where n_samples in the number of samples and n_features is the number of features.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_features)", "methodParams": [], "methodReturns": [], "methodDesc": "Transform X back to original space. References Learning to Find Pre-Images, G BakIr et al, 2004.", "methodParamsBody": "X : array-like, shape (n_samples, n_components)"}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components)", "methodParams": [], "methodReturns": [], "methodDesc": "Transform X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)"}], "allFuncParams": ["n_components", "kernel", "gamma", "degree", "coef0", "kernel_params", "alpha", "fit_inverse_transform", "eigen_solver", "tol", "max_iter", "remove_zero_eig", "random_state", "copy_X", "n_jobs"], "notes": "", "funcName": "KernelPCA", "allFuncAttributes": ["lambdas_", "alphas_", "dual_coef_", "X_transformed_fit_", "X_fit_"], "funcDesc": "Kernel Principal component analysis (KPCA)", "funcParamBody": "n_components : int, default=None Number of components. If None, all non-zero components are kept. kernel : linear | poly | rbf | sigmoid | cosine | precomputed Kernel. Default=linear. gamma : float, default=1/n_features Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels. degree : int, default=3 Degree for poly kernels. Ignored by other kernels. coef0 : float, default=1 Independent term in poly and sigmoid kernels. Ignored by other kernels. kernel_params : mapping of string to any, default=None Parameters (keyword arguments) and values for kernel passed as callable object. Ignored by other kernels. alpha : int, default=1.0 Hyperparameter of the ridge regression that learns the inverse transform (when fit_inverse_transform=True). fit_inverse_transform : bool, default=False Learn the inverse transform for non-precomputed kernels. (i.e. learn to find the pre-image of a point) eigen_solver : string [auto|dense|arpack], default=auto Select eigensolver to use. If n_components is much less than the number of training samples, arpack may be more efficient than the dense eigensolver. tol : float, default=0 Convergence tolerance for arpack. If 0, optimal value will be chosen by arpack. max_iter : int, default=None Maximum number of iterations for arpack. If None, optimal value will be chosen by arpack. remove_zero_eig : boolean, default=False If True, then all components with zero eigenvalues are removed, so that the number of components in the output may be < n_components (and sometimes even zero due to numerical instability). When n_components is None, this parameter is ignored and components with zero eigenvalues are removed regardless. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Used when eigen_solver == arpack. New in version 0.18. copy_X : boolean, default=True If True, input X is copied and stored by the model in the X_fit_ copy_X=False saves memory by storing a reference. New in version 0.18. n_jobs : int, default=1 The number of parallel jobs to run. If -1 , then the number of jobs is set to the number of CPU cores. New in version 0.18.", "funcAttrBody": "lambdas_ : array, (n_components,) Eigenvalues of the centered kernel matrix in decreasing order. If n_components and remove_zero_eig are not set, then all values are stored. alphas_ : array, (n_samples, n_components) Eigenvectors of the centered kernel matrix. If n_components and remove_zero_eig are not set, then all components are stored. dual_coef_ : array, (n_samples, n_features) Inverse transform matrix. Set if fit_inverse_transform is True. X_transformed_fit_ : array, (n_samples, n_components) Projection of the fitted data on the kernel principal components. X_fit_ : (n_samples, n_features) The data used to fit the model. If copy_X=False , then X_fit_ is a reference. This attribute is used for the calls to transform."},
{"libName": "sklearn.decomposition", "methods": [{"methodName": "__init__(n_components=None, whiten=False, copy=True, batch_size=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns the instance itself.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model with X, using minibatches of size batch_size.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : Ignored. "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_covariance()", "methodReturns": ["cov"], "methodDesc": "Compute data covariance with the generative model. cov  =  components_.T  *  S**2  *  components_  +  sigma2  *  eye(n_features) where  S**2 contains the explained variances, and sigma2 contains the noise variances.", "methodReturnsBody": "cov : array, shape=(n_features, n_features)   Estimated covariance of data.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_precision()", "methodReturns": ["precision"], "methodDesc": "Compute data precision matrix with the generative model. Equals the inverse of the covariance but computed with the matrix inversion lemma for efficiency.", "methodReturnsBody": "precision : array, shape=(n_features, n_features)   Estimated precision of data.  "}, {"methodName": "inverse_transform(X)", "methodReturnsBody": "X_original array-like, shape (n_samples, n_features) : ", "methodParams": ["X"], "methodReturns": ["X_original array-like, shape (n_samples, n_features)"], "methodDesc": "Transform data back to its original space. In other words, return an input X_original whose transform would be X. Notes If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening.", "methodParamsBody": "X : array-like, shape (n_samples, n_components)   New data, where n_samples is the number of samples and n_components is the number of components.  "}, {"methodName": "partial_fit(X, y=None, check_input=True)", "methodReturnsBody": "self : object   Returns the instance itself.  ", "methodParams": ["X", "check_input", "y"], "methodReturns": ["self"], "methodDesc": "Incremental fit with X. All of X is processed as a single batch.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   check_input : bool   Run check_array on X.   y : Ignored. "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components) ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Apply dimensionality reduction to X. X is projected on the first principal components previously extracted from a training set. Examples", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   New data, where n_samples is the number of samples and n_features is the number of features.  "}], "allFuncParams": ["n_components", "whiten", "copy", "batch_size"], "notes": "Implements the incremental PCA model from: D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008. See http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf This model is an extension of the Sequential Karhunen-Loeve Transform from: A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and its Application to Images, IEEE Transactions on Image Processing, Volume 9, Number 8, pp. 1371-1374, August 2000. See http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf We have specifically abstained from an optimization used by authors of both papers, a QR decomposition used in specific situations to reduce the algorithmic complexity of the SVD. The source for this technique is Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5, section 5.4.4, pp 252-253. . This technique has been omitted because it is advantageous only when decomposing a matrix with n_samples (rows) >= 5/3 * n_features (columns), and hurts the readability of the implemented algorithm. This would be a good opportunity for future optimization, if it is deemed necessary. If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening. Implements the incremental PCA model from: D. Ross, J. Lim, R. Lin, M. Yang, Incremental Learning for Robust Visual Tracking, International Journal of Computer Vision, Volume 77, Issue 1-3, pp. 125-141, May 2008. See http://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf This model is an extension of the Sequential Karhunen-Loeve Transform from: A. Levy and M. Lindenbaum, Sequential Karhunen-Loeve Basis Extraction and its Application to Images, IEEE Transactions on Image Processing, Volume 9, Number 8, pp. 1371-1374, August 2000. See http://www.cs.technion.ac.il/~mic/doc/skl-ip.pdf We have specifically abstained from an optimization used by authors of both papers, a QR decomposition used in specific situations to reduce the algorithmic complexity of the SVD. The source for this technique is Matrix Computations, Third Edition, G. Holub and C. Van Loan, Chapter 5, section 5.4.4, pp 252-253. . This technique has been omitted because it is advantageous only when decomposing a matrix with n_samples (rows) >= 5/3 * n_features (columns), and hurts the readability of the implemented algorithm. This would be a good opportunity for future optimization, if it is deemed necessary. If whitening is enabled, inverse_transform will compute the exact inverse operation, which includes reversing whitening.", "funcName": "IncrementalPCA", "allFuncAttributes": ["components_", "explained_variance_", "explained_variance_ratio_", "singular_values_", "mean_", "var_", "noise_variance_", "n_components_", "n_samples_seen_"], "funcDesc": "Incremental principal components analysis (IPCA).", "funcParamBody": "n_components : int or None, (default=None) Number of components to keep. If n_components  ``  is  ``None , then n_components is set to min(n_samples,  n_features) . whiten : bool, optional When True (False by default) the components_ vectors are divided by n_samples times components_ to ensure uncorrelated outputs with unit component-wise variances. Whitening will remove some information from the transformed signal (the relative variance scales of the components) but can sometimes improve the predictive accuracy of the downstream estimators by making data respect some hard-wired assumptions. copy : bool, (default=True) If False, X will be overwritten. copy=False can be used to save memory but is unsafe for general use. batch_size : int or None, (default=None) The number of samples to use for each batch. Only used when calling fit . If batch_size is None , then batch_size 5  *  n_features , to provide a balance between approximation accuracy and memory consumption.", "funcAttrBody": "components_ : array, shape (n_components, n_features) Components with maximum variance. explained_variance_ : array, shape (n_components,) Variance explained by each of the selected components. explained_variance_ratio_ : array, shape (n_components,) Percentage of variance explained by each of the selected components. If all components are stored, the sum of explained variances is equal to 1.0. singular_values_ : array, shape (n_components,) The singular values corresponding to each of the selected components. The singular values are equal to the 2-norms of the n_components mean_ : array, shape (n_features,) Per-feature empirical mean, aggregate over calls to partial_fit . var_ : array, shape (n_features,) Per-feature empirical variance, aggregate over calls to partial_fit . noise_variance_ : float The estimated noise covariance following the Probabilistic PCA model from Tipping and Bishop 1999. See Pattern Recognition and Machine Learning by C. Bishop, 12.2.1 p. 574 or http://www.miketipping.com/papers/met-mppca.pdf . n_components_ : int The estimated number of components. Relevant when n_components=None . n_samples_seen_ : int The number of samples processed by the estimator. Will be reset on new calls to fit, but increments across partial_fit calls."},
{"libName": "sklearn.decomposition", "methods": [{"methodName": "__init__(n_components=None, algorithm=parallel, whiten=True, fun=logcosh, fun_args=None, max_iter=200, tol=0.0001, w_init=None, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model to X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : Ignored. "}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components) ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit the model and recover the sources from X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data, where n_samples is the number of samples and n_features is the number of features.   y : Ignored. "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "inverse_transform(X, copy=True)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_features) ", "methodParams": ["X", "copy"], "methodReturns": ["X_new"], "methodDesc": "Transform the sources back to the mixed data (apply mixing matrix).", "methodParamsBody": "X : array-like, shape (n_samples, n_components)   Sources, where n_samples is the number of samples and n_components is the number of components.   copy : bool (optional)   If False, data passed to fit are overwritten. Defaults to True.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X, y=deprecated, copy=True)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components) ", "methodParams": ["X", "y", "copy"], "methodReturns": ["X_new"], "methodDesc": "Recover the sources from X (apply the unmixing matrix).", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Data to transform, where n_samples is the number of samples and n_features is the number of features.   y : (ignored)    Deprecated since version 0.19: This parameter will be removed in 0.21.    copy : bool (optional)   If False, data passed to fit are overwritten. Defaults to True.  "}], "allFuncParams": ["n_components", "algorithm", "whiten", "fun", "fun_args", "max_iter", "tol", "w_init", "random_state"], "notes": "Implementation based on A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430 Implementation based on A. Hyvarinen and E. Oja, Independent Component Analysis: Algorithms and Applications, Neural Networks, 13(4-5), 2000, pp. 411-430", "funcName": "FastICA", "allFuncAttributes": ["components_", "mixing_", "n_iter_"], "funcDesc": "FastICA: a fast algorithm for Independent Component Analysis.", "funcParamBody": "n_components : int, optional Number of components to use. If none is passed, all are used. algorithm : {parallel, deflation} Apply parallel or deflational algorithm for FastICA. whiten : boolean, optional If whiten is false, the data is already considered to be whitened, and no whitening is performed. fun : string or function, optional. Default: logcosh The functional form of the G function used in the approximation to neg-entropy. Could be either logcosh, exp, or cube. You can also provide your own function. It should return a tuple containing the value of the function, and of its derivative, in the point. Example: def my_g(x): return x ** 3, 3 * x ** 2 fun_args : dictionary, optional Arguments to send to the functional form. If empty and if fun=logcosh, fun_args will take value {alpha : 1.0}. max_iter : int, optional Maximum number of iterations during fit. tol : float, optional Tolerance on update at each iteration. w_init : None of an (n_components, n_components) ndarray The mixing matrix to be used to initialize the algorithm. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcAttrBody": "components_ : 2D array, shape (n_components, n_features) The unmixing matrix. mixing_ : array, shape (n_features, n_components) The mixing matrix. n_iter_ : int If the algorithm is deflation, n_iter is the maximum number of iterations run across all components. Else they are just the number of iterations taken to converge."},
{"libName": "sklearn.decomposition", "methods": [{"methodName": "__init__(n_components=None, tol=0.01, copy=True, max_iter=1000, noise_variance_init=None, svd_method=randomized, iterated_power=3, random_state=0)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the FactorAnalysis model to X using EM", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data.   y : Ignored. "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_covariance()", "methodReturns": ["cov"], "methodDesc": "Compute data covariance with the FactorAnalysis model. cov  =  components_.T  *  components_  +  diag(noise_variance)", "methodReturnsBody": "cov : array, shape (n_features, n_features)   Estimated covariance of data.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_precision()", "methodReturns": ["precision"], "methodDesc": "Compute data precision matrix with the FactorAnalysis model.", "methodReturnsBody": "precision : array, shape (n_features, n_features)   Estimated precision of data.  "}, {"methodName": "score(X, y=None)", "methodReturnsBody": "ll : float   Average log-likelihood of the samples under the current model  ", "methodParams": ["X", "y"], "methodReturns": ["ll"], "methodDesc": "Compute the average log-likelihood of the samples", "methodParamsBody": "X : array, shape (n_samples, n_features)   The data   y : Ignored. "}, {"methodName": "score_samples(X)", "methodReturnsBody": "ll : array, shape (n_samples,)   Log-likelihood of each sample under the current model  ", "methodParams": ["X"], "methodReturns": ["ll"], "methodDesc": "Compute the log-likelihood of each sample", "methodParamsBody": "X : array, shape (n_samples, n_features)   The data  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array-like, shape (n_samples, n_components)   The latent variables of X.  ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Apply dimensionality reduction to X using the model. Compute the expected mean of the latent variables. See Barber, 21.2.33 (or Bishop, 12.66).", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data.  "}], "allFuncParams": ["n_components", "tol", "copy", "max_iter", "noise_variance_init", "svd_method", "iterated_power", "random_state"], "notes": "", "funcName": "FactorAnalysis", "allFuncAttributes": ["components_", "loglike_", "noise_variance_", "n_iter_"], "funcDesc": "Factor Analysis (FA)", "funcParamBody": "n_components : int | None Dimensionality of latent space, the number of components of X that are obtained after transform . If None, n_components is set to the number of features. tol : float Stopping tolerance for EM algorithm. copy : bool Whether to make a copy of X. If False , the input X gets overwritten during fitting. max_iter : int Maximum number of iterations. noise_variance_init : None | array, shape=(n_features,) The initial guess of the noise variance for each feature. If None, it defaults to np.ones(n_features) svd_method : {lapack, randomized} Which SVD method to use. If lapack use standard SVD from scipy.linalg, if randomized use fast randomized_svd function. Defaults to randomized. For most applications randomized will be sufficiently precise while providing significant speed gains. Accuracy can also be improved by setting higher values for iterated_power . If this is not sufficient, for maximum precision you should choose lapack. iterated_power : int, optional Number of iterations for the power method. 3 by default. Only used if svd_method equals randomized random_state : int, RandomState instance or None, optional (default=0) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . Only used when svd_method equals randomized.", "funcAttrBody": "components_ : array, [n_components, n_features] Components with maximum variance. loglike_ : list, [n_iterations] The log likelihood at each iteration. noise_variance_ : array, shape=(n_features,) The estimated noise variance for each feature. n_iter_ : int Number of iterations run."},
{"libName": "sklearn.decomposition", "methods": [{"methodName": "__init__(n_components=None, alpha=1, max_iter=1000, tol=1e-08, fit_algorithm=lars, transform_algorithm=omp, transform_n_nonzero_coefs=None, transform_alpha=None, n_jobs=1, code_init=None, dict_init=None, verbose=False, split_sign=False, random_state=None)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns the object itself  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the model from data in X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training vector, where n_samples in the number of samples and n_features is the number of features.   y : Ignored. "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array, shape (n_samples, n_components)   Transformed data  ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Encode the data as a sparse combination of the dictionary atoms. Coding method is determined by the object parameter transform_algorithm .", "methodParamsBody": "X : array of shape (n_samples, n_features)   Test data to be transformed, must have the same number of features as the data used to train the model.  "}], "allFuncParams": ["n_components", "alpha", "max_iter", "tol", "fit_algorithm", "transform_algorithm", "transform_n_nonzero_coefs", "transform_alpha", "n_jobs", "code_init", "dict_init", "verbose", "split_sign", "random_state"], "notes": "References: J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009: Online dictionary learning for sparse coding ( http://www.di.ens.fr/sierra/pdfs/icml09.pdf )", "funcName": "DictionaryLearning", "allFuncAttributes": ["components_", "error_", "n_iter_"], "funcDesc": "Dictionary learning", "funcParamBody": "n_components : int, number of dictionary elements to extract alpha : float, sparsity controlling parameter max_iter : int, maximum number of iterations to perform tol : float, tolerance for numerical error fit_algorithm : {lars, cd} lars: uses the least angle regression method to solve the lasso problem (linear_model.lars_path) cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). Lars will be faster if the estimated components are sparse. New in version 0.17: cd coordinate descent method to improve speed. transform_algorithm : {lasso_lars, lasso_cd, lars, omp,     threshold} Algorithm used to transform the data lars: uses the least angle regression method (linear_model.lars_path) lasso_lars: uses Lars to compute the Lasso solution lasso_cd: uses the coordinate descent method to compute the Lasso solution (linear_model.Lasso). lasso_lars will be faster if the estimated components are sparse. omp: uses orthogonal matching pursuit to estimate the sparse solution threshold: squashes to zero all coefficients less than alpha from the projection dictionary  *  X' New in version 0.17: lasso_cd coordinate descent method to improve speed. transform_n_nonzero_coefs : int, 0.1  *  n_features by default Number of nonzero coefficients to target in each column of the solution. This is only used by algorithm=lars and algorithm=omp alpha in the omp case. transform_alpha : float, 1. by default If algorithm=lasso_lars or algorithm=lasso_cd , alpha is the penalty applied to the L1 norm. If algorithm=threshold , alpha is the absolute value of the threshold below which coefficients will be squashed to zero. If algorithm=omp , alpha is the tolerance parameter: the value of the reconstruction error targeted. In this case, it overrides n_nonzero_coefs . n_jobs : int, number of parallel jobs to run code_init : array of shape (n_samples, n_components), initial value for the code, for warm restart dict_init : array of shape (n_components, n_features), initial values for the dictionary, for warm restart verbose : bool, optional (default: False) To control the verbosity of the procedure. split_sign : bool, False by default Whether to split the sparse feature vector into the concatenation of its negative part and its positive part. This can improve the performance of downstream classifiers. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcAttrBody": "components_ : array, [n_components, n_features] dictionary atoms extracted from the data error_ : array vector of errors at each iteration n_iter_ : int Number of iterations run."},
{"allReturnParams": ["X", "t"], "libName": "sklearn.datasets", "methods": [], "notes": "The algorithm is from Marsland [1]. The algorithm is from Marsland [1].", "funcName": "make_swiss_roll", "allFuncParams": ["n_samples", "noise", "random_state"], "funcDesc": "Generate a swiss roll dataset.", "funcParamBody": "n_samples : int, optional (default=100) The number of sample points on the S curve. noise : float, optional (default=0.0) The standard deviation of the gaussian noise. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_samples, 3] The points. t : array of shape [n_samples] The univariate position of the sample according to the main dimension of the points in the manifold."},
{"allReturnParams": ["X"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_spd_matrix", "allFuncParams": ["n_dim", "random_state"], "funcDesc": "Generate a random symmetric, positive-definite matrix.", "funcParamBody": "n_dim : int The matrix dimension. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_dim, n_dim] The random symmetric, positive-definite matrix."},
{"allReturnParams": ["X", "y"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_sparse_uncorrelated", "allFuncParams": ["n_samples", "n_features", "random_state"], "funcDesc": "Generate a random regression problem with sparse uncorrelated design", "funcParamBody": "n_samples : int, optional (default=100) The number of samples. n_features : int, optional (default=10) The number of features. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_samples, n_features] The input samples. y : array of shape [n_samples] The output values."},
{"allReturnParams": ["prec"], "libName": "sklearn.datasets", "methods": [], "notes": "The sparsity is actually imposed on the cholesky factor of the matrix. Thus alpha does not translate directly into the filling fraction of the matrix itself. The sparsity is actually imposed on the cholesky factor of the matrix. Thus alpha does not translate directly into the filling fraction of the matrix itself.", "funcName": "make_sparse_spd_matrix", "allFuncParams": ["dim", "alpha", "norm_diag", "smallest_coef", "largest_coef", "random_state"], "funcDesc": "Generate a sparse symmetric definite positive matrix.", "funcParamBody": "dim : integer, optional (default=1) The size of the random matrix to generate. alpha : float between 0 and 1, optional (default=0.95) The probability that a coefficient is zero (see notes). Larger values enforce more sparsity. norm_diag : boolean, optional (default=False) Whether to normalize the output matrix to make the leading diagonal elements all 1 smallest_coef : float between 0 and 1, optional (default=0.1) The value of the smallest coefficient. largest_coef : float between 0 and 1, optional (default=0.9) The value of the largest coefficient. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "prec : sparse matrix of shape (dim, dim) The generated matrix."},
{"allReturnParams": ["data", "dictionary", "code"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_sparse_coded_signal", "allFuncParams": ["n_samples", "n_components", "n_features", "n_nonzero_coefs", "random_state"], "funcDesc": "Generate a signal as a sparse combination of dictionary elements.", "funcParamBody": "n_samples : int number of samples to generate n_components :  int, number of components in the dictionary n_features : int number of features of the dataset to generate n_nonzero_coefs : int number of active (non-zero) coefficients in each sample random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "data : array of shape [n_features, n_samples] The encoded signal (Y). dictionary : array of shape [n_features, n_components] The dictionary with normalized components (D). code : array of shape [n_components, n_samples] The sparse code such that each column of this matrix has exactly n_nonzero_coefs non-zero items (X)."},
{"allReturnParams": ["X", "t"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_s_curve", "allFuncParams": ["n_samples", "noise", "random_state"], "funcDesc": "Generate an S curve dataset.", "funcParamBody": "n_samples : int, optional (default=100) The number of sample points on the S curve. noise : float, optional (default=0.0) The standard deviation of the gaussian noise. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_samples, 3] The points. t : array of shape [n_samples] The univariate position of the sample according to the main dimension of the points in the manifold."},
{"allReturnParams": ["X", "y", "coef"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_regression", "allFuncParams": ["n_samples", "n_features", "n_informative", "n_targets", "bias", "effective_rank", "tail_strength", "noise", "shuffle", "coef", "random_state"], "funcDesc": "Generate a random regression problem.", "funcParamBody": "n_samples : int, optional (default=100) The number of samples. n_features : int, optional (default=100) The number of features. n_informative : int, optional (default=10) The number of informative features, i.e., the number of features used to build the linear model used to generate the output. n_targets : int, optional (default=1) The number of regression targets, i.e., the dimension of the y output vector associated with a sample. By default, the output is a scalar. bias : float, optional (default=0.0) The bias term in the underlying linear model. effective_rank : int or None, optional (default=None) if not None: The approximate number of singular vectors required to explain most of the input data by linear combinations. Using this kind of singular spectrum in the input allows the generator to reproduce the correlations often observed in practice. if None: The input set is well conditioned, centered and gaussian with unit variance. tail_strength : float between 0.0 and 1.0, optional (default=0.5) The relative importance of the fat noisy tail of the singular values profile if effective_rank is not None. noise : float, optional (default=0.0) The standard deviation of the gaussian noise applied to the output. shuffle : boolean, optional (default=True) Shuffle the samples and the features. coef : boolean, optional (default=False) If True, the coefficients of the underlying linear model are returned. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_samples, n_features] The input samples. y : array of shape [n_samples] or [n_samples, n_targets] The output values. coef : array of shape [n_features] or [n_features, n_targets], optional The coefficient of the underlying linear model. It is returned only if coef is True."},
{"allReturnParams": ["X", "Y", "p_c", "p_w_c"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_multilabel_classification", "allFuncParams": ["n_samples", "n_features", "n_classes", "n_labels", "length", "allow_unlabeled", "sparse", "return_indicator", "return_distributions", "random_state"], "funcDesc": "Generate a random multilabel classification problem.", "funcParamBody": "n_samples : int, optional (default=100) The number of samples. n_features : int, optional (default=20) The total number of features. n_classes : int, optional (default=5) The number of classes of the classification problem. n_labels : int, optional (default=2) The average number of labels per instance. More precisely, the number of labels per sample is drawn from a Poisson distribution with n_labels as its expected value, but samples are bounded (using rejection sampling) by n_classes , and must be nonzero if allow_unlabeled is False. length : int, optional (default=50) The sum of the features (number of words if documents) is drawn from a Poisson distribution with this expected value. allow_unlabeled : bool, optional (default=True) If True , some instances might not belong to any class. sparse : bool, optional (default=False) If True , return a sparse feature matrix New in version 0.17: parameter to allow sparse output. return_indicator : dense (default) | sparse | False If dense return Y in the dense binary indicator format. If 'sparse' return Y in the sparse binary indicator format. False returns a list of lists of labels. return_distributions : bool, optional (default=False) If True , return the prior class probability and conditional probabilities of features given classes, from which the data was drawn. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_samples, n_features] The generated samples. Y : array or sparse CSR matrix of shape [n_samples, n_classes] The label sets. p_c : array, shape [n_classes] The probability of each class being drawn. Only returned if return_distributions=True . p_w_c : array, shape [n_features, n_classes] The probability of each feature being drawn given each class. Only returned if return_distributions=True ."},
{"allReturnParams": ["X", "y"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_moons", "allFuncParams": ["n_samples", "shuffle", "noise", "random_state"], "funcDesc": "Make two interleaving half circles", "funcParamBody": "n_samples : int, optional (default=100) The total number of points generated. shuffle : bool, optional (default=True) Whether to shuffle the samples. noise : double or None (default=None) Standard deviation of Gaussian noise added to the data. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_samples, 2] The generated samples. y : array of shape [n_samples] The integer labels (0 or 1) for class membership of each sample."},
{"allReturnParams": ["X"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_low_rank_matrix", "allFuncParams": ["n_samples", "n_features", "effective_rank", "tail_strength", "random_state"], "funcDesc": "Generate a mostly low rank matrix with bell-shaped singular values", "funcParamBody": "n_samples : int, optional (default=100) The number of samples. n_features : int, optional (default=100) The number of features. effective_rank : int, optional (default=10) The approximate number of singular vectors required to explain most of the data by linear combinations. tail_strength : float between 0.0 and 1.0, optional (default=0.5) The relative importance of the fat noisy tail of the singular values profile. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_samples, n_features] The matrix."},
{"allReturnParams": ["X", "y"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_hastie_10_2", "allFuncParams": ["n_samples", "random_state"], "funcDesc": "Generates data for binary classification used in Hastie et al. 2009, Example 10.2.", "funcParamBody": "n_samples : int, optional (default=12000) The number of samples. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_samples, 10] The input samples. y : array of shape [n_samples] The output values."},
{"allReturnParams": ["X", "y"], "libName": "sklearn.datasets", "methods": [], "notes": "The dataset is from Zhu et al [1]. The dataset is from Zhu et al [1].", "funcName": "make_gaussian_quantiles", "allFuncParams": ["mean", "cov", "n_samples", "n_features", "n_classes", "shuffle", "random_state"], "funcDesc": "Generate isotropic Gaussian and label samples by quantile", "funcParamBody": "mean : array of shape [n_features], optional (default=None) The mean of the multi-dimensional normal distribution. If None then use the origin (0, 0, ...). cov : float, optional (default=1.) The covariance matrix will be this value times the unit matrix. This dataset only produces symmetric normal distributions. n_samples : int, optional (default=100) The total number of points equally divided among classes. n_features : int, optional (default=2) The number of features for each sample. n_classes : int, optional (default=3) The number of classes shuffle : boolean, optional (default=True) Shuffle the samples. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The integer labels for quantile membership of each sample."},
{"allReturnParams": ["X", "y"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_friedman3", "allFuncParams": ["n_samples", "noise", "random_state"], "funcDesc": "Generate the Friedman #3 regression problem", "funcParamBody": "n_samples : int, optional (default=100) The number of samples. noise : float, optional (default=0.0) The standard deviation of the gaussian noise applied to the output. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_samples, 4] The input samples. y : array of shape [n_samples] The output values."},
{"allReturnParams": ["X", "y"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_friedman2", "allFuncParams": ["n_samples", "noise", "random_state"], "funcDesc": "Generate the Friedman #2 regression problem", "funcParamBody": "n_samples : int, optional (default=100) The number of samples. noise : float, optional (default=0.0) The standard deviation of the gaussian noise applied to the output. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_samples, 4] The input samples. y : array of shape [n_samples] The output values."},
{"allReturnParams": ["X", "y"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_friedman1", "allFuncParams": ["n_samples", "n_features", "noise", "random_state"], "funcDesc": "Generate the Friedman #1 regression problem", "funcParamBody": "n_samples : int, optional (default=100) The number of samples. n_features : int, optional (default=10) The number of features. Should be at least 5. noise : float, optional (default=0.0) The standard deviation of the gaussian noise applied to the output. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_samples, n_features] The input samples. y : array of shape [n_samples] The output values."},
{"allReturnParams": ["X", "y"], "libName": "sklearn.datasets", "methods": [], "notes": "The algorithm is adapted from Guyon [1] and was designed to generate the Madelon dataset. The algorithm is adapted from Guyon [1] and was designed to generate the Madelon dataset.", "funcName": "make_classification", "allFuncParams": ["n_samples", "n_features", "n_informative", "n_redundant", "n_repeated", "n_classes", "n_clusters_per_class", "weights", "flip_y", "class_sep", "hypercube", "shift", "scale", "shuffle", "random_state"], "funcDesc": "Generate a random n-class classification problem.", "funcParamBody": "n_samples : int, optional (default=100) The number of samples. n_features : int, optional (default=20) The total number of features. These comprise n_informative n_redundant redundant features, n_repeated n_features-n_informative-n_redundant- n_repeated useless features drawn at random. n_informative : int, optional (default=2) The number of informative features. Each class is composed of a number of gaussian clusters each located around the vertices of a hypercube in a subspace of dimension n_informative . For each cluster, informative features are drawn independently from  N(0, 1) and then randomly linearly combined within each cluster in order to add covariance. The clusters are then placed on the vertices of the hypercube. n_redundant : int, optional (default=2) The number of redundant features. These features are generated as random linear combinations of the informative features. n_repeated : int, optional (default=0) The number of duplicated features, drawn randomly from the informative and the redundant features. n_classes : int, optional (default=2) The number of classes (or labels) of the classification problem. n_clusters_per_class : int, optional (default=2) The number of clusters per class. weights : list of floats or None (default=None) The proportions of samples assigned to each class. If None, then classes are balanced. Note that if len(weights) == n_classes - 1 , then the last class weight is automatically inferred. More than n_samples samples may be returned if the sum of weights flip_y : float, optional (default=0.01) The fraction of samples whose class are randomly exchanged. Larger values introduce noise in the labels and make the classification task harder. class_sep : float, optional (default=1.0) The factor multiplying the hypercube size.  Larger values spread out the clusters/classes and make the classification task easier. hypercube : boolean, optional (default=True) If True, the clusters are put on the vertices of a hypercube. If False, the clusters are put on the vertices of a random polytope. shift : float, array of shape [n_features] or None, optional (default=0.0) Shift features by the specified value. If None, then features are shifted by a random value drawn in [-class_sep, class_sep]. scale : float, array of shape [n_features] or None, optional (default=1.0) Multiply features by the specified value. If None, then features are scaled by a random value drawn in [1, 100]. Note that scaling happens after shifting. shuffle : boolean, optional (default=True) Shuffle the samples and the features. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The integer labels for class membership of each sample."},
{"allReturnParams": ["X", "y"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_circles", "allFuncParams": ["n_samples", "shuffle", "noise", "random_state", "factor"], "funcDesc": "Make a large circle containing a smaller circle in 2d.", "funcParamBody": "n_samples : int, optional (default=100) The total number of points generated. shuffle : bool, optional (default=True) Whether to shuffle the samples. noise : double or None (default=None) Standard deviation of Gaussian noise added to the data. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . factor : double < 1 (default=.8) Scale factor between inner and outer circle.", "funcReturnBody": "X : array of shape [n_samples, 2] The generated samples. y : array of shape [n_samples] The integer labels (0 or 1) for class membership of each sample."},
{"allReturnParams": ["X", "rows", "cols"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_checkerboard", "allFuncParams": ["shape", "n_clusters", "noise", "minval", "maxval", "shuffle", "random_state"], "funcDesc": "Generate an array with block checkerboard structure for biclustering.", "funcParamBody": "shape : iterable (n_rows, n_cols) The shape of the result. n_clusters : integer or iterable (n_row_clusters, n_column_clusters) The number of row and column clusters. noise : float, optional (default=0.0) The standard deviation of the gaussian noise. minval : int, optional (default=10) Minimum value of a bicluster. maxval : int, optional (default=100) Maximum value of a bicluster. shuffle : boolean, optional (default=True) Shuffle the samples. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape shape The generated array. rows : array of shape (n_clusters, X.shape[0],) The indicators for cluster membership of each row. cols : array of shape (n_clusters, X.shape[1],) The indicators for cluster membership of each column."},
{"allReturnParams": ["X", "y"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_blobs", "allFuncParams": ["n_samples", "n_features", "centers", "cluster_std", "center_box", "shuffle", "random_state"], "funcDesc": "Generate isotropic Gaussian blobs for clustering.", "funcParamBody": "n_samples : int, optional (default=100) The total number of points equally divided among clusters. n_features : int, optional (default=2) The number of features for each sample. centers : int or array of shape [n_centers, n_features], optional (default=3) The number of centers to generate, or the fixed center locations. cluster_std : float or sequence of floats, optional (default=1.0) The standard deviation of the clusters. center_box : pair of floats (min, max), optional (default=(-10.0, 10.0)) The bounding box for each cluster center when centers are generated at random. shuffle : boolean, optional (default=True) Shuffle the samples. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape [n_samples, n_features] The generated samples. y : array of shape [n_samples] The integer labels for cluster membership of each sample."},
{"allReturnParams": ["X", "rows", "cols"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "make_biclusters", "allFuncParams": ["shape", "n_clusters", "noise", "minval", "maxval", "shuffle", "random_state"], "funcDesc": "Generate an array with constant block diagonal structure for biclustering.", "funcParamBody": "shape : iterable (n_rows, n_cols) The shape of the result. n_clusters : integer The number of biclusters. noise : float, optional (default=0.0) The standard deviation of the gaussian noise. minval : int, optional (default=10) Minimum value of a bicluster. maxval : int, optional (default=100) Maximum value of a bicluster. shuffle : boolean, optional (default=True) Shuffle the samples. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "X : array of shape shape The generated array. rows : array of shape (n_clusters, X.shape[0],) The indicators for cluster membership of each row. cols : array of shape (n_clusters, X.shape[1],) The indicators for cluster membership of each column."},
{"allReturnParams": ["fname"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "mldata_filename", "allFuncParams": ["dataname"], "funcDesc": "Convert a raw name for a data set in a mldata.org filename.", "funcParamBody": "dataname : str Name of dataset", "funcReturnBody": "fname : str The converted dataname."},
{"allReturnParams": ["data", "(data, target)", "The copy of UCI ML Wine Data Set dataset is downloaded and modified to fit", "standard format from:", "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "load_wine", "allFuncParams": ["return_X_y"], "funcDesc": "Load and return the wine dataset (classification).", "funcParamBody": "return_X_y : boolean, default=False. If True, returns (data,  target) instead of a Bunch object. See below for more information about the data and target object.", "funcReturnBody": "data : Bunch Dictionary-like object, the interesting attributes are: data, the data to learn, target, the classification labels, target_names, the meaning of the labels, feature_names, the meaning of the features, and DESCR, the full description of the dataset. (data, target) : tuple if return_X_y is True The copy of UCI ML Wine Data Set dataset is downloaded and modified to fit : standard format from: : https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data :"},
{"allReturnParams": ["[X1, y1, \u2026, Xn, yn]", "where each (Xi, yi) pair is the result from load_svmlight_file(files[i]).", "If query_id is set to True, this will return instead [X1, y1, q1,", "\u2026, Xn, yn, qn] where (Xi, yi, qi) is the result from", "load_svmlight_file(files[i])"], "libName": "sklearn.datasets", "methods": [], "notes": "When fitting a model to a matrix X_train and evaluating it against a matrix X_test, it is essential that X_train and X_test have the same number of features (X_train.shape[1] == X_test.shape[1]). This may not be the case if you load the files individually with load_svmlight_file.", "funcName": "load_svmlight_files", "allFuncParams": ["files", "n_features", "dtype", "multilabel", "zero_based", "query_id", "offset", "length"], "funcDesc": "Load dataset from multiple files in SVMlight format", "funcParamBody": "files : iterable over {str, file-like, int} (Paths of) files to load. If a path ends in .gz or .bz2, it will be uncompressed on the fly. If an integer is passed, it is assumed to be a file descriptor. File-likes and file descriptors will not be closed by this function. File-like objects must be opened in binary mode. n_features : int or None The number of features to use. If None, it will be inferred from the maximum column index occurring in any of the files. This can be set to a higher value than the actual number of features in any of the input files, but setting it to a lower value will cause an exception to be raised. dtype : numpy data type, default np.float64 Data type of dataset to be loaded. This will be the data type of the output numpy arrays X and y . multilabel : boolean, optional Samples may have several labels each (see http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html ) zero_based : boolean or auto, optional Whether column indices in f are zero-based (True) or one-based (False). If column indices are one-based, they are transformed to zero-based to match Python/NumPy conventions. If set to auto, a heuristic check is applied to determine this from the file contents. Both kinds of files occur in the wild, but they are unfortunately not self-identifying. Using auto or True should always be safe when no offset or length is passed. If offset or length are passed, the auto mode falls back to zero_based=True to avoid having the heuristic check yield inconsistent results on different segments of the file. query_id : boolean, defaults to False If True, will return the query_id array for each file. offset : integer, optional, default 0 Ignore the offset first bytes by seeking forward, then discarding the following bytes up until the next new line character. length : integer, optional, default -1 If strictly positive, stop reading any new line of data once the position in the file has reached the (offset + length) bytes threshold.", "funcReturnBody": "[X1, y1, ..., Xn, yn] : where each (Xi, yi) pair is the result from load_svmlight_file(files[i]). : If query_id is set to True, this will return instead [X1, y1, q1, : ..., Xn, yn, qn] where (Xi, yi, qi) is the result from : load_svmlight_file(files[i]) :"},
{"allReturnParams": ["X", "y", "query_id"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "load_svmlight_file", "allFuncParams": ["f", "n_features", "dtype", "multilabel", "zero_based", "query_id", "offset", "length"], "funcDesc": "Load datasets in the svmlight / libsvm format into sparse CSR matrix", "funcParamBody": "f : {str, file-like, int} (Path to) a file to load. If a path ends in .gz or .bz2, it will be uncompressed on the fly. If an integer is passed, it is assumed to be a file descriptor. A file-like or file descriptor will not be closed by this function. A file-like object must be opened in binary mode. n_features : int or None The number of features to use. If None, it will be inferred. This argument is useful to load several files that are subsets of a bigger sliced dataset: each subset might not have examples of every feature, hence the inferred shape might vary from one slice to another. n_features is only required if offset or length are passed a non-default value. dtype : numpy data type, default np.float64 Data type of dataset to be loaded. This will be the data type of the output numpy arrays X and y . multilabel : boolean, optional, default False Samples may have several labels each (see http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html ) zero_based : boolean or auto, optional, default auto Whether column indices in f are zero-based (True) or one-based (False). If column indices are one-based, they are transformed to zero-based to match Python/NumPy conventions. If set to auto, a heuristic check is applied to determine this from the file contents. Both kinds of files occur in the wild, but they are unfortunately not self-identifying. Using auto or True should always be safe when no offset or length is passed. If offset or length are passed, the auto mode falls back to zero_based=True to avoid having the heuristic check yield inconsistent results on different segments of the file. query_id : boolean, default False If True, will return the query_id array for each file. offset : integer, optional, default 0 Ignore the offset first bytes by seeking forward, then discarding the following bytes up until the next new line character. length : integer, optional, default -1 If strictly positive, stop reading any new line of data once the position in the file has reached the (offset + length) bytes threshold.", "funcReturnBody": "X : scipy.sparse matrix of shape (n_samples, n_features) y : ndarray of shape (n_samples,), or, in the multilabel a list of tuples of length n_samples. query_id : array of shape (n_samples,) query_id for each sample. Only returned when query_id is set to True."},
{"allReturnParams": ["data"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "load_sample_images", "funcDesc": "Load sample images for image manipulation.", "funcReturnBody": "data : Bunch Dictionary-like object with the following attributes : images, the two sample images, filenames, the file names for the images, and DESCR the full description of the dataset."},
{"allReturnParams": ["img"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "load_sample_image", "allFuncParams": ["image_name"], "funcDesc": "Load the numpy array of a single sample image", "funcParamBody": "image_name : { china.jpg , flower.jpg } The name of the sample image loaded", "funcReturnBody": "img : 3D array The image as a numpy array: height x width x color"},
{"allReturnParams": ["data"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "load_mlcomp", "allFuncParams": ["name_or_id"], "funcDesc": "DEPRECATED: since the ", "funcParamBody": "name_or_id : the integer id or the string name metadata of the MLComp dataset to load set_ : select the portion to load: train, test or raw mlcomp_root :  the filesystem path to the root folder where MLComp datasets are stored, if mlcomp_root is None, the MLCOMP_DATASETS_HOME environment variable is looked up instead. ** kwargs : domain specific kwargs to be passed to the dataset loader. Read more in the User Guide .", "funcReturnBody": "data : Bunch Dictionary-like object, the interesting attributes are: filenames, the files holding the raw to learn, target, the classification labels (integer index), target_names, the meaning of the labels, and DESCR, the full description of the dataset. Note on the lookup process: depending on the type of name_or_id, will choose between integer id lookup or metadata name lookup by looking at the unzipped archives and metadata file. TODO: implement zip dataset loading too"},
{"allReturnParams": ["data", "(data, target)"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "load_linnerud", "allFuncParams": ["return_X_y"], "funcDesc": "Load and return the linnerud dataset (multivariate regression).", "funcParamBody": "return_X_y : boolean, default=False. If True, returns (data,  target) instead of a Bunch object. See below for more information about the data and target object. New in version 0.18.", "funcReturnBody": "data : Bunch Dictionary-like object, the interesting attributes are: data and targets, the two multivariate datasets, with data corresponding to the exercise and targets corresponding to the physiological measurements, as well as feature_names and target_names. (data, target) : tuple if return_X_y is True New in version 0.18."},
{"allReturnParams": ["data", "(data, target)"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "load_iris", "allFuncParams": ["return_X_y"], "funcDesc": "Load and return the iris dataset (classification).", "funcParamBody": "return_X_y : boolean, default=False. If True, returns (data,  target) instead of a Bunch object. See below for more information about the data and target object. New in version 0.18.", "funcReturnBody": "data : Bunch Dictionary-like object, the interesting attributes are: data, the data to learn, target, the classification labels, target_names, the meaning of the labels, feature_names, the meaning of the features, and DESCR, the full description of the dataset. (data, target) : tuple if return_X_y is True New in version 0.18."},
{"allReturnParams": ["data"], "libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "load_files", "allFuncParams": ["container_path", "description", "categories", "load_content", "shuffle", "encoding", "decode_error", "random_state"], "funcDesc": "Load text files with categories as subfolder names.", "funcParamBody": "container_path : string or unicode Path to the main folder holding one subfolder per category description : string or unicode, optional (default=None) A paragraph describing the characteristic of the dataset: its source, reference, etc. categories : A collection of strings or None, optional (default=None) If None (default), load all the categories. If not None, list of category names to load (other categories ignored). load_content : boolean, optional (default=True) Whether to load or not the content of the different files. If true a data attribute containing the text information is present in the data structure returned. If not, a filenames attribute gives the path to the files. shuffle : bool, optional (default=True) Whether or not to shuffle the data: might be important for models that make the assumption that the samples are independent and identically distributed (i.i.d.), such as stochastic gradient descent. encoding : string or None (default is None) If None, do not try to decode the content of the files (e.g. for images or other non-text content). If not None, encoding to use to decode text files to Unicode if load_content is True. decode_error : {strict, ignore, replace}, optional Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given encoding . Passed as keyword argument errors to bytes.decode. random_state : int, RandomState instance or None, optional (default=0) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcReturnBody": "data : Bunch Dictionary-like object, the interesting attributes are: either data, the raw text data to learn, or filenames, the files holding it, target, the classification labels (integer index), target_names, the meaning of the labels, and DESCR, the full description of the dataset."},
{"libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "fetch_20newsgroups", "allFuncParams": ["data_home", "subset", "categories", "shuffle", "random_state", "remove", "download_if_missing"], "funcDesc": "Load the filenames and data from the 20 newsgroups dataset.", "funcParamBody": "data_home : optional, default: None Specify a download and cache folder for the datasets. If None, all scikit-learn data is stored in ~/scikit_learn_data subfolders. subset : train or test, all, optional Select the dataset to load: train for the training set, test for the test set, all for both, with shuffled ordering. categories : None or collection of string or unicode If None (default), load all the categories. If not None, list of category names to load (other categories ignored). shuffle : bool, optional Whether or not to shuffle the data: might be important for models that make the assumption that the samples are independent and identically distributed (i.i.d.), such as stochastic gradient descent. random_state : numpy random number generator or seed integer Used to shuffle the dataset. remove : tuple May contain any subset of (headers, footers, quotes). Each of these are kinds of text that will be detected and removed from the newsgroup posts, preventing classifiers from overfitting on metadata. headers removes newsgroup headers, footers removes blocks at the ends of posts that look like signatures, and quotes removes lines that appear to be quoting another post. headers follows an exact standard; the other filters are not always correct. download_if_missing : optional, True by default If False, raise an IOError if the data is not locally available instead of trying to download the data from the source site."},
{"libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "dump_svmlight_file", "allFuncParams": ["X", "y", "f", "zero_based", "comment", "query_id", "multilabel"], "funcDesc": "Dump the dataset in svmlight / libsvm file format.", "funcParamBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features] Training vectors, where n_samples is the number of samples and n_features is the number of features. y : {array-like, sparse matrix}, shape = [n_samples (, n_labels)] Target values. Class labels must be an integer or float, or array-like objects of integer or float for multilabel classifications. f : string or file-like in binary mode If string, specifies the path that will contain the data. If file-like, data will be written to f. f should be opened in binary mode. zero_based : boolean, optional Whether column indices should be written zero-based (True) or one-based (False). comment : string, optional Comment to insert at the top of the file. This should be either a Unicode string, which will be encoded as UTF-8, or an ASCII byte string. If a comment is given, then it will be preceded by one that identifies the file as having been dumped by scikit-learn. Note that not all tools grok comments in SVMlight files. query_id : array-like, shape = [n_samples] Array containing pairwise preference constraints (qid in svmlight format). multilabel : boolean, optional Samples may have several labels each (see http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html ) New in version 0.17: parameter multilabel to support multilabel datasets."},
{"libName": "sklearn.datasets", "methods": [], "notes": "", "funcName": "clear_data_home", "allFuncParams": ["data_home"], "funcDesc": "Delete all the content of the data home cache.", "funcParamBody": "data_home : str | None The path to scikit-learn data dir."},
{"libName": "sklearn.cross_decomposition", "methods": [{"methodName": "__init__(n_components=2, scale=True, copy=True)", "methodDesc": ""}, {"methodParams": ["X", "Y"], "methodName": "fit(X, Y)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   Y : array-like, shape = [n_samples, n_targets]   Target vectors, where n_samples is the number of samples and n_targets is the number of response variables.  ", "methodDesc": "Fit model to data."}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "x_scores if Y is not given, (x_scores, y_scores) otherwise. : ", "methodParams": ["X", "y"], "methodReturns": ["x_scores if Y is not given, (x_scores, y_scores) otherwise."], "methodDesc": "Learn and apply the dimension reduction on the train data.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   y : array-like, shape = [n_samples, n_targets]   Target vectors, where n_samples is the number of samples and n_targets is the number of response variables.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodParams": ["X", "Y"], "methodName": "transform(X, Y=None)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   Y : array-like, shape = [n_samples, n_targets]   Target vectors, where n_samples is the number of samples and n_targets is the number of response variables.  ", "methodDesc": "Apply the dimension reduction learned on the train data."}], "allFuncParams": ["n_components", "scale", "copy"], "notes": "", "funcName": "PLSSVD", "allFuncAttributes": ["x_weights_", "y_weights_", "x_scores_", "y_scores_"], "funcDesc": "Partial Least Square SVD", "funcParamBody": "n_components : int, default 2 Number of components to keep. scale : boolean, default True Whether to scale X and Y. copy : boolean, default True Whether to copy X and Y, or perform in-place computations.", "funcAttrBody": "x_weights_ : array, [p, n_components] X block weights vectors. y_weights_ : array, [q, n_components] Y block weights vectors. x_scores_ : array, [n_samples, n_components] X scores. y_scores_ : array, [n_samples, n_components] Y scores."},
{"libName": "sklearn.cross_decomposition", "methods": [{"methodName": "__init__(n_components=2, scale=True, max_iter=500, tol=1e-06, copy=True)", "methodDesc": ""}, {"methodParams": ["X", "Y"], "methodName": "fit(X, Y)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   Y : array-like, shape = [n_samples, n_targets]   Target vectors, where n_samples is the number of samples and n_targets is the number of response variables.  ", "methodDesc": "Fit model to data."}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "x_scores if Y is not given, (x_scores, y_scores) otherwise. : ", "methodParams": ["X", "y"], "methodReturns": ["x_scores if Y is not given, (x_scores, y_scores) otherwise."], "methodDesc": "Learn and apply the dimension reduction on the train data.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   y : array-like, shape = [n_samples, n_targets]   Target vectors, where n_samples is the number of samples and n_targets is the number of response variables.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodParams": ["X", "copy"], "methodName": "predict(X, copy=True)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   copy : boolean, default True   Whether to copy X and Y, or perform in-place normalization.  ", "methodDesc": "Apply the dimension reduction learned on the train data. Notes This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space."}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X, Y=None, copy=True)", "methodReturnsBody": "x_scores if Y is not given, (x_scores, y_scores) otherwise. : ", "methodParams": ["X", "Y", "copy"], "methodReturns": ["x_scores if Y is not given, (x_scores, y_scores) otherwise."], "methodDesc": "Apply the dimension reduction learned on the train data.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   Y : array-like, shape = [n_samples, n_targets]   Target vectors, where n_samples is the number of samples and n_targets is the number of response variables.   copy : boolean, default True   Whether to copy X and Y, or perform in-place normalization.  "}], "allFuncParams": ["n_components", "scale", "max_iter", "tol", "copy"], "notes": "Matrices: Are computed such that: where Xk and Yk are residual matrices at iteration k. Slides explaining PLS For each component k, find weights u, v that optimizes: max  corr(Xk  u,  Yk  v)  *  std(Xk  u)  std(Yk  u) , such that |u|  =  1 Note that it maximizes both the correlations between the scores and the intra-block variances. The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score. The residual matrix of Y (Yk+1) block is obtained by deflation on the current X score. This performs the PLS regression known as PLS2. This mode is prediction oriented. This implementation provides the same results that 3 PLS packages provided in the R language (R-project): Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000. In french but still a reference: Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic. This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space. Matrices: Are computed such that: where Xk and Yk are residual matrices at iteration k. Slides explaining PLS For each component k, find weights u, v that optimizes: max  corr(Xk  u,  Yk  v)  *  std(Xk  u)  std(Yk  u) , such that |u|  =  1 Note that it maximizes both the correlations between the scores and the intra-block variances. The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score. The residual matrix of Y (Yk+1) block is obtained by deflation on the current X score. This performs the PLS regression known as PLS2. This mode is prediction oriented. This implementation provides the same results that 3 PLS packages provided in the R language (R-project): Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000. In french but still a reference: Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic. This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space.", "funcName": "PLSRegression", "allFuncAttributes": ["x_weights_", "y_weights_", "x_loadings_", "y_loadings_", "x_scores_", "y_scores_", "x_rotations_", "y_rotations_", "coef_", "n_iter_"], "funcDesc": "PLS regression", "funcParamBody": "n_components : int, (default 2) Number of components to keep. scale : boolean, (default True) whether to scale the data max_iter : an integer, (default 500) the maximum number of iterations of the NIPALS inner loop (used only if algorithm=nipals) tol : non-negative real Tolerance used in the iterative algorithm default 1e-06. copy : boolean, default True Whether the deflation should be done on a copy. Let the default value to True unless you dont care about side effect", "funcAttrBody": "x_weights_ : array, [p, n_components] X block weights vectors. y_weights_ : array, [q, n_components] Y block weights vectors. x_loadings_ : array, [p, n_components] X block loadings vectors. y_loadings_ : array, [q, n_components] Y block loadings vectors. x_scores_ : array, [n_samples, n_components] X scores. y_scores_ : array, [n_samples, n_components] Y scores. x_rotations_ : array, [p, n_components] X block to latents rotations. y_rotations_ : array, [q, n_components] Y block to latents rotations. coef_ : array, [p, q] The coefficients of the linear model: Y  =  X  coef_  +  Err n_iter_ : array-like Number of iterations of the NIPALS inner loop for each component."},
{"libName": "sklearn.cross_decomposition", "methods": [{"methodName": "__init__(n_components=2, scale=True, algorithm=nipals, max_iter=500, tol=1e-06, copy=True)", "methodDesc": ""}, {"methodParams": ["X", "Y"], "methodName": "fit(X, Y)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   Y : array-like, shape = [n_samples, n_targets]   Target vectors, where n_samples is the number of samples and n_targets is the number of response variables.  ", "methodDesc": "Fit model to data."}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "x_scores if Y is not given, (x_scores, y_scores) otherwise. : ", "methodParams": ["X", "y"], "methodReturns": ["x_scores if Y is not given, (x_scores, y_scores) otherwise."], "methodDesc": "Learn and apply the dimension reduction on the train data.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   y : array-like, shape = [n_samples, n_targets]   Target vectors, where n_samples is the number of samples and n_targets is the number of response variables.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodParams": ["X", "copy"], "methodName": "predict(X, copy=True)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   copy : boolean, default True   Whether to copy X and Y, or perform in-place normalization.  ", "methodDesc": "Apply the dimension reduction learned on the train data. Notes This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space."}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X, Y=None, copy=True)", "methodReturnsBody": "x_scores if Y is not given, (x_scores, y_scores) otherwise. : ", "methodParams": ["X", "Y", "copy"], "methodReturns": ["x_scores if Y is not given, (x_scores, y_scores) otherwise."], "methodDesc": "Apply the dimension reduction learned on the train data.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   Y : array-like, shape = [n_samples, n_targets]   Target vectors, where n_samples is the number of samples and n_targets is the number of response variables.   copy : boolean, default True   Whether to copy X and Y, or perform in-place normalization.  "}], "allFuncParams": ["n_components", "scale", "algorithm", "max_iter", "tol", "copy"], "notes": "Matrices: Are computed such that: where Xk and Yk are residual matrices at iteration k. Slides explaining PLS For each component k, find weights u, v that optimize: Note that it maximizes both the correlations between the scores and the intra-block variances. The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score. The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score. This performs a canonical symmetric version of the PLS regression. But slightly different than the CCA. This is mostly used for modeling. This implementation provides the same results that the plspm package provided in the R language (R-project), using the function plsca(X, Y). Results are equal or collinear with the function pls(...,  mode  =  \"canonical\") of the mixOmics package. The difference relies in the fact that mixOmics implementation does not exactly implement the Wold algorithm since it does not normalize y_weights to one. Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000. Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic. This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space. Matrices: Are computed such that: where Xk and Yk are residual matrices at iteration k. Slides explaining PLS For each component k, find weights u, v that optimize: Note that it maximizes both the correlations between the scores and the intra-block variances. The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score. The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score. This performs a canonical symmetric version of the PLS regression. But slightly different than the CCA. This is mostly used for modeling. This implementation provides the same results that the plspm package provided in the R language (R-project), using the function plsca(X, Y). Results are equal or collinear with the function pls(...,  mode  =  \"canonical\") of the mixOmics package. The difference relies in the fact that mixOmics implementation does not exactly implement the Wold algorithm since it does not normalize y_weights to one. Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000. Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic. This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space.", "funcName": "PLSCanonical", "allFuncAttributes": ["x_weights_", "y_weights_", "x_loadings_", "y_loadings_", "x_scores_", "y_scores_", "x_rotations_", "y_rotations_", "n_iter_"], "funcDesc": "PLSCanonical implements the 2 blocks canonical PLS of the original Wold algorithm [Tenenhaus 1998] p.204, referred as PLS-C2A in [Wegelin 2000].", "funcParamBody": "n_components : int, (default 2). Number of components to keep scale : boolean, (default True) Option to scale data algorithm : string, nipals or svd The algorithm used to estimate the weights. It will be called n_components times, i.e. once for each iteration of the outer loop. max_iter : an integer, (default 500) the maximum number of iterations of the NIPALS inner loop (used only if algorithm=nipals) tol : non-negative real, default 1e-06 the tolerance used in the iterative algorithm copy : boolean, default True Whether the deflation should be done on a copy. Let the default value to True unless you dont care about side effect", "funcAttrBody": "x_weights_ : array, shape = [p, n_components] X block weights vectors. y_weights_ : array, shape = [q, n_components] Y block weights vectors. x_loadings_ : array, shape = [p, n_components] X block loadings vectors. y_loadings_ : array, shape = [q, n_components] Y block loadings vectors. x_scores_ : array, shape = [n_samples, n_components] X scores. y_scores_ : array, shape = [n_samples, n_components] Y scores. x_rotations_ : array, shape = [p, n_components] X block to latents rotations. y_rotations_ : array, shape = [q, n_components] Y block to latents rotations. n_iter_ : array-like Number of iterations of the NIPALS inner loop for each component. Not useful if the algorithm provided is svd."},
{"libName": "sklearn.cross_decomposition", "methods": [{"methodName": "__init__(n_components=2, scale=True, max_iter=500, tol=1e-06, copy=True)", "methodDesc": ""}, {"methodParams": ["X", "Y"], "methodName": "fit(X, Y)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   Y : array-like, shape = [n_samples, n_targets]   Target vectors, where n_samples is the number of samples and n_targets is the number of response variables.  ", "methodDesc": "Fit model to data."}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "x_scores if Y is not given, (x_scores, y_scores) otherwise. : ", "methodParams": ["X", "y"], "methodReturns": ["x_scores if Y is not given, (x_scores, y_scores) otherwise."], "methodDesc": "Learn and apply the dimension reduction on the train data.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   y : array-like, shape = [n_samples, n_targets]   Target vectors, where n_samples is the number of samples and n_targets is the number of response variables.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodParams": ["X", "copy"], "methodName": "predict(X, copy=True)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   copy : boolean, default True   Whether to copy X and Y, or perform in-place normalization.  ", "methodDesc": "Apply the dimension reduction learned on the train data. Notes This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space."}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X, Y=None, copy=True)", "methodReturnsBody": "x_scores if Y is not given, (x_scores, y_scores) otherwise. : ", "methodParams": ["X", "Y", "copy"], "methodReturns": ["x_scores if Y is not given, (x_scores, y_scores) otherwise."], "methodDesc": "Apply the dimension reduction learned on the train data.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training vectors, where n_samples is the number of samples and n_features is the number of predictors.   Y : array-like, shape = [n_samples, n_targets]   Target vectors, where n_samples is the number of samples and n_targets is the number of response variables.   copy : boolean, default True   Whether to copy X and Y, or perform in-place normalization.  "}], "allFuncParams": ["n_components", "scale", "max_iter", "tol", "copy"], "notes": "For each component k, find the weights u, v that maximizes max corr(Xk u, Yk v), such that |u|  =  |v|  =  1 Note that it maximizes only the correlations between the scores. The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score. The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score. Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000. In french but still a reference: Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic. This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space. For each component k, find the weights u, v that maximizes max corr(Xk u, Yk v), such that |u|  =  |v|  =  1 Note that it maximizes only the correlations between the scores. The residual matrix of X (Xk+1) block is obtained by the deflation on the current X score: x_score. The residual matrix of Y (Yk+1) block is obtained by deflation on the current Y score. Jacob A. Wegelin. A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case. Technical Report 371, Department of Statistics, University of Washington, Seattle, 2000. In french but still a reference: Tenenhaus, M. (1998). La regression PLS: theorie et pratique. Paris: Editions Technic. This call requires the estimation of a p x q matrix, which may be an issue in high dimensional space.", "funcName": "CCA", "allFuncAttributes": ["x_weights_", "y_weights_", "x_loadings_", "y_loadings_", "x_scores_", "y_scores_", "x_rotations_", "y_rotations_", "n_iter_"], "funcDesc": "CCA Canonical Correlation Analysis.", "funcParamBody": "n_components : int, (default 2). number of components to keep. scale : boolean, (default True) whether to scale the data? max_iter : an integer, (default 500) the maximum number of iterations of the NIPALS inner loop tol : non-negative real, default 1e-06. the tolerance used in the iterative algorithm copy : boolean Whether the deflation be done on a copy. Let the default value to True unless you dont care about side effects", "funcAttrBody": "x_weights_ : array, [p, n_components] X block weights vectors. y_weights_ : array, [q, n_components] Y block weights vectors. x_loadings_ : array, [p, n_components] X block loadings vectors. y_loadings_ : array, [q, n_components] Y block loadings vectors. x_scores_ : array, [n_samples, n_components] X scores. y_scores_ : array, [n_samples, n_components] Y scores. x_rotations_ : array, [p, n_components] X block to latents rotations. y_rotations_ : array, [q, n_components] Y block to latents rotations. n_iter_ : array-like Number of iterations of the NIPALS inner loop for each component."},
{"allReturnParams": ["shrunk_cov"], "libName": "sklearn.covariance", "methods": [], "notes": "The regularized (shrunk) covariance is given by where mu = trace(cov) / n_features", "funcName": "shrunk_covariance", "allFuncParams": ["emp_cov", "shrinkage"], "funcDesc": "Calculates a covariance matrix shrunk on the diagonal", "funcParamBody": "emp_cov : array-like, shape (n_features, n_features) Covariance matrix to be shrunk shrinkage : float, 0 <= shrinkage <= 1 Coefficient in the convex combination used for the computation of the shrunk estimate.", "funcReturnBody": "shrunk_cov : array-like Shrunk covariance."},
{"allReturnParams": ["shrunk_cov", "shrinkage"], "libName": "sklearn.covariance", "methods": [], "notes": "The regularised (shrunk) covariance is: where mu = trace(cov) / n_features The formula we used to implement the OAS does not correspond to the one given in the article. It has been taken from the MATLAB program available from the authors webpage ( http://tbayes.eecs.umich.edu/yilun/covestimation ).", "funcName": "oas", "allFuncParams": ["X", "assume_centered"], "funcDesc": "Estimate covariance with the Oracle Approximating Shrinkage algorithm.", "funcParamBody": "X : array-like, shape (n_samples, n_features) Data from which to compute the covariance estimate. assume_centered : boolean If True, data are not centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data are centered before computation.", "funcReturnBody": "shrunk_cov : array-like, shape (n_features, n_features) Shrunk covariance. shrinkage : float Coefficient in the convex combination used for the computation of the shrunk estimate."},
{"allReturnParams": ["shrunk_cov", "shrinkage"], "libName": "sklearn.covariance", "methods": [], "notes": "The regularized (shrunk) covariance is: where mu = trace(cov) / n_features The regularized (shrunk) covariance is: where mu = trace(cov) / n_features", "funcName": "ledoit_wolf", "allFuncParams": ["X", "assume_centered", "block_size"], "funcDesc": "Estimates the shrunk Ledoit-Wolf covariance matrix.", "funcParamBody": "X : array-like, shape (n_samples, n_features) Data from which to compute the covariance estimate assume_centered : boolean, default=False If True, data are not centered before computation. Useful to work with data whose mean is significantly equal to zero but is not exactly zero. If False, data are centered before computation. block_size : int, default=1000 Size of the blocks into which the covariance matrix will be split. This is purely a memory optimization and does not affect results.", "funcReturnBody": "shrunk_cov : array-like, shape (n_features, n_features) Shrunk covariance. shrinkage : float Coefficient in the convex combination used for the computation of the shrunk estimate."},
{"allReturnParams": ["covariance", "precision", "costs", "n_iter"], "libName": "sklearn.covariance", "methods": [], "notes": "The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R glasso package. One possible difference with the glasso R package is that the diagonal coefficients are not penalized.", "funcName": "graph_lasso", "allFuncParams": ["emp_cov", "alpha", "cov_init", "mode", "tol", "enet_tol", "max_iter", "verbose", "return_costs", "eps", "return_n_iter"], "funcDesc": "l1-penalized covariance estimator", "funcParamBody": "emp_cov : 2D ndarray, shape (n_features, n_features) Empirical covariance from which to compute the covariance estimate. alpha : positive float The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance. cov_init : 2D array (n_features, n_features), optional The initial guess for the covariance. mode : {cd, lars} The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p > n. Elsewhere prefer cd which is more numerically stable. tol : positive float, optional The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. enet_tol : positive float, optional The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=cd. max_iter : integer, optional The maximum number of iterations. verbose : boolean, optional If verbose is True, the objective function and dual gap are printed at each iteration. return_costs : boolean, optional If return_costs is True, the objective function and dual gap at each iteration are returned. eps : float, optional The machine-precision regularization in the computation of the Cholesky diagonal factors. Increase this for very ill-conditioned systems. return_n_iter : bool, optional Whether or not to return the number of iterations.", "funcReturnBody": "covariance : 2D ndarray, shape (n_features, n_features) The estimated covariance matrix. precision : 2D ndarray, shape (n_features, n_features) The estimated (sparse) precision matrix. costs : list of (objective, dual_gap) pairs The list of values of the objective function and the dual gap at each iteration. Returned only if return_costs is True. n_iter : int Number of iterations. Returned only if return_n_iter is set to True."},
{"allReturnParams": ["covariance"], "libName": "sklearn.covariance", "methods": [], "notes": "", "funcName": "empirical_covariance", "allFuncParams": ["X", "assume_centered"], "funcDesc": "Computes the Maximum likelihood covariance estimator", "funcParamBody": "X : ndarray, shape (n_samples, n_features) Data from which to compute the covariance estimate assume_centered : Boolean If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation.", "funcReturnBody": "covariance : 2D ndarray, shape (n_features, n_features) Empirical covariance (Maximum Likelihood Estimator)."},
{"libName": "sklearn.covariance", "methods": [{"methodName": "__init__(store_precision=True, assume_centered=False, shrinkage=0.1)", "methodDesc": ""}, {"methodName": "error_norm(comp_cov, norm=frobenius, scaling=True, squared=True)", "methodReturnsBody": "The Mean Squared Error (in the sense of the Frobenius norm) between :  `self` and `comp_cov` covariance estimators. : ", "methodParams": ["comp_cov", "norm", "scaling", "squared"], "methodReturns": ["The Mean Squared Error (in the sense of the Frobenius norm) between", "`self` and `comp_cov` covariance estimators."], "methodDesc": "Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius norm).", "methodParamsBody": "comp_cov : array-like, shape = [n_features, n_features]   The covariance to compare with.   norm : str   The type of norm used to compute the error. Available error types: - frobenius (default): sqrt(tr(A^t.A)) - spectral: sqrt(max(eigenvalues(A^t.A)) where A is the error (comp_cov  -  self.covariance_) .   scaling : bool   If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled.   squared : bool   Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned.  "}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fits the shrunk covariance model according to the given training data and parameters.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training data, where n_samples is the number of samples and n_features is the number of features.   y : not used, present for API consistence purpose. "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_precision()", "methodReturns": ["precision_"], "methodDesc": "Getter for the precision matrix.", "methodReturnsBody": "precision_ : array-like,   The precision matrix associated to the current covariance object.  "}, {"methodName": "mahalanobis(observations)", "methodReturnsBody": "mahalanobis_distance : array, shape = [n_observations,]   Squared Mahalanobis distances of the observations.  ", "methodParams": ["observations"], "methodReturns": ["mahalanobis_distance"], "methodDesc": "Computes the squared Mahalanobis distances of given observations.", "methodParamsBody": "observations : array-like, shape = [n_observations, n_features]   The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.  "}, {"methodName": "score(X_test, y=None)", "methodReturnsBody": "res : float   The likelihood of the data set with self.covariance_ as an estimator of its covariance matrix.  ", "methodParams": ["X_test", "y"], "methodReturns": ["res"], "methodDesc": "Computes the log-likelihood of a Gaussian data set with self.covariance_ as an estimator of its covariance matrix.", "methodParamsBody": "X_test : array-like, shape = [n_samples, n_features]   Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering).   y : not used, present for API consistence purpose. "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["store_precision", "assume_centered", "shrinkage"], "notes": "The regularized covariance is given by where mu = trace(cov) / n_features The regularized covariance is given by where mu = trace(cov) / n_features", "funcName": "ShrunkCovariance", "allFuncAttributes": ["covariance_", "precision_", "shrinkage"], "funcDesc": "Covariance estimator with shrinkage", "funcParamBody": "store_precision : boolean, default True Specify if the estimated precision is stored assume_centered : boolean, default False If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation. shrinkage : float, 0 <= shrinkage <= 1, default 0.1 Coefficient in the convex combination used for the computation of the shrunk estimate.", "funcAttrBody": "covariance_ : array-like, shape (n_features, n_features) Estimated covariance matrix precision_ : array-like, shape (n_features, n_features) Estimated pseudo inverse matrix. (stored only if store_precision is True) shrinkage : float, 0 <= shrinkage <= 1 Coefficient in the convex combination used for the computation of the shrunk estimate."},
{"libName": "sklearn.covariance", "methods": [{"methodName": "__init__(store_precision=True, assume_centered=False)", "methodDesc": ""}, {"methodName": "error_norm(comp_cov, norm=frobenius, scaling=True, squared=True)", "methodReturnsBody": "The Mean Squared Error (in the sense of the Frobenius norm) between :  `self` and `comp_cov` covariance estimators. : ", "methodParams": ["comp_cov", "norm", "scaling", "squared"], "methodReturns": ["The Mean Squared Error (in the sense of the Frobenius norm) between", "`self` and `comp_cov` covariance estimators."], "methodDesc": "Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius norm).", "methodParamsBody": "comp_cov : array-like, shape = [n_features, n_features]   The covariance to compare with.   norm : str   The type of norm used to compute the error. Available error types: - frobenius (default): sqrt(tr(A^t.A)) - spectral: sqrt(max(eigenvalues(A^t.A)) where A is the error (comp_cov  -  self.covariance_) .   scaling : bool   If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled.   squared : bool   Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned.  "}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fits the Oracle Approximating Shrinkage covariance model according to the given training data and parameters.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training data, where n_samples is the number of samples and n_features is the number of features.   y : not used, present for API consistence purpose. "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_precision()", "methodReturns": ["precision_"], "methodDesc": "Getter for the precision matrix.", "methodReturnsBody": "precision_ : array-like,   The precision matrix associated to the current covariance object.  "}, {"methodName": "mahalanobis(observations)", "methodReturnsBody": "mahalanobis_distance : array, shape = [n_observations,]   Squared Mahalanobis distances of the observations.  ", "methodParams": ["observations"], "methodReturns": ["mahalanobis_distance"], "methodDesc": "Computes the squared Mahalanobis distances of given observations.", "methodParamsBody": "observations : array-like, shape = [n_observations, n_features]   The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.  "}, {"methodName": "score(X_test, y=None)", "methodReturnsBody": "res : float   The likelihood of the data set with self.covariance_ as an estimator of its covariance matrix.  ", "methodParams": ["X_test", "y"], "methodReturns": ["res"], "methodDesc": "Computes the log-likelihood of a Gaussian data set with self.covariance_ as an estimator of its covariance matrix.", "methodParamsBody": "X_test : array-like, shape = [n_samples, n_features]   Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering).   y : not used, present for API consistence purpose. "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["store_precision", "assume_centered"], "notes": "The regularised covariance is: where mu = trace(cov) / n_features and shrinkage is given by the OAS formula (see References) Shrinkage Algorithms for MMSE Covariance Estimation Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010. The regularised covariance is: where mu = trace(cov) / n_features and shrinkage is given by the OAS formula (see References) Shrinkage Algorithms for MMSE Covariance Estimation Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.", "funcName": "OAS", "allFuncAttributes": ["covariance_", "precision_", "shrinkage_"], "funcDesc": "Oracle Approximating Shrinkage Estimator", "funcParamBody": "store_precision : bool, default=True Specify if the estimated precision is stored. assume_centered : bool, default=False If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data are centered before computation.", "funcAttrBody": "covariance_ : array-like, shape (n_features, n_features) Estimated covariance matrix. precision_ : array-like, shape (n_features, n_features) Estimated pseudo inverse matrix. (stored only if store_precision is True) shrinkage_ : float, 0 <= shrinkage <= 1 coefficient in the convex combination used for the computation of the shrunk estimate."},
{"funcName": "MinCovDet", "notes": "", "libName": "sklearn.covariance", "methods": [{"methodName": "__init__(store_precision=True, assume_centered=False, support_fraction=None, random_state=None)", "methodDesc": ""}, {"methodName": "correct_covariance(data)", "methodReturnsBody": "covariance_corrected : array-like, shape (n_features, n_features)   Corrected robust covariance estimate.  ", "methodParams": ["data"], "methodReturns": ["covariance_corrected"], "methodDesc": "Apply a correction to raw Minimum Covariance Determinant estimates. Correction using the empirical correction factor suggested by Rousseeuw and Van Driessen in [RVD] . References", "methodParamsBody": "data : array-like, shape (n_samples, n_features)   The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates.  "}, {"methodName": "error_norm(comp_cov, norm=frobenius, scaling=True, squared=True)", "methodReturnsBody": "The Mean Squared Error (in the sense of the Frobenius norm) between :  `self` and `comp_cov` covariance estimators. : ", "methodParams": ["comp_cov", "norm", "scaling", "squared"], "methodReturns": ["The Mean Squared Error (in the sense of the Frobenius norm) between", "`self` and `comp_cov` covariance estimators."], "methodDesc": "Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius norm).", "methodParamsBody": "comp_cov : array-like, shape = [n_features, n_features]   The covariance to compare with.   norm : str   The type of norm used to compute the error. Available error types: - frobenius (default): sqrt(tr(A^t.A)) - spectral: sqrt(max(eigenvalues(A^t.A)) where A is the error (comp_cov  -  self.covariance_) .   scaling : bool   If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled.   squared : bool   Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned.  "}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fits a Minimum Covariance Determinant with the FastMCD algorithm.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training data, where n_samples is the number of samples and n_features is the number of features.   y : not used, present for API consistence purpose. "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_precision()", "methodReturns": ["precision_"], "methodDesc": "Getter for the precision matrix.", "methodReturnsBody": "precision_ : array-like,   The precision matrix associated to the current covariance object.  "}, {"methodName": "mahalanobis(observations)", "methodReturnsBody": "mahalanobis_distance : array, shape = [n_observations,]   Squared Mahalanobis distances of the observations.  ", "methodParams": ["observations"], "methodReturns": ["mahalanobis_distance"], "methodDesc": "Computes the squared Mahalanobis distances of given observations.", "methodParamsBody": "observations : array-like, shape = [n_observations, n_features]   The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.  "}, {"methodName": "reweight_covariance(data)", "methodReturnsBody": "location_reweighted : array-like, shape (n_features, )   Re-weighted robust location estimate.   covariance_reweighted : array-like, shape (n_features, n_features)   Re-weighted robust covariance estimate.   support_reweighted : array-like, type boolean, shape (n_samples,)   A mask of the observations that have been used to compute the re-weighted robust location and covariance estimates.  ", "methodParams": ["data"], "methodReturns": ["location_reweighted", "covariance_reweighted", "support_reweighted"], "methodDesc": "Re-weight raw Minimum Covariance Determinant estimates. Re-weight observations using Rousseeuws method (equivalent to deleting outlying observations from the data set before computing location and covariance estimates) described in [RVDriessen] . References", "methodParamsBody": "data : array-like, shape (n_samples, n_features)   The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates.  "}, {"methodName": "score(X_test, y=None)", "methodReturnsBody": "res : float   The likelihood of the data set with self.covariance_ as an estimator of its covariance matrix.  ", "methodParams": ["X_test", "y"], "methodReturns": ["res"], "methodDesc": "Computes the log-likelihood of a Gaussian data set with self.covariance_ as an estimator of its covariance matrix.", "methodParamsBody": "X_test : array-like, shape = [n_samples, n_features]   Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering).   y : not used, present for API consistence purpose. "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "Minimum Covariance Determinant (MCD): robust estimator of covariance."},
{"libName": "sklearn.covariance", "methods": [{"methodName": "__init__(store_precision=True, assume_centered=False, block_size=1000)", "methodDesc": ""}, {"methodName": "error_norm(comp_cov, norm=frobenius, scaling=True, squared=True)", "methodReturnsBody": "The Mean Squared Error (in the sense of the Frobenius norm) between :  `self` and `comp_cov` covariance estimators. : ", "methodParams": ["comp_cov", "norm", "scaling", "squared"], "methodReturns": ["The Mean Squared Error (in the sense of the Frobenius norm) between", "`self` and `comp_cov` covariance estimators."], "methodDesc": "Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius norm).", "methodParamsBody": "comp_cov : array-like, shape = [n_features, n_features]   The covariance to compare with.   norm : str   The type of norm used to compute the error. Available error types: - frobenius (default): sqrt(tr(A^t.A)) - spectral: sqrt(max(eigenvalues(A^t.A)) where A is the error (comp_cov  -  self.covariance_) .   scaling : bool   If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled.   squared : bool   Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned.  "}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fits the Ledoit-Wolf shrunk covariance model according to the given training data and parameters.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training data, where n_samples is the number of samples and n_features is the number of features.   y : not used, present for API consistence purpose. "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_precision()", "methodReturns": ["precision_"], "methodDesc": "Getter for the precision matrix.", "methodReturnsBody": "precision_ : array-like,   The precision matrix associated to the current covariance object.  "}, {"methodName": "mahalanobis(observations)", "methodReturnsBody": "mahalanobis_distance : array, shape = [n_observations,]   Squared Mahalanobis distances of the observations.  ", "methodParams": ["observations"], "methodReturns": ["mahalanobis_distance"], "methodDesc": "Computes the squared Mahalanobis distances of given observations.", "methodParamsBody": "observations : array-like, shape = [n_observations, n_features]   The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.  "}, {"methodName": "score(X_test, y=None)", "methodReturnsBody": "res : float   The likelihood of the data set with self.covariance_ as an estimator of its covariance matrix.  ", "methodParams": ["X_test", "y"], "methodReturns": ["res"], "methodDesc": "Computes the log-likelihood of a Gaussian data set with self.covariance_ as an estimator of its covariance matrix.", "methodParamsBody": "X_test : array-like, shape = [n_samples, n_features]   Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering).   y : not used, present for API consistence purpose. "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["store_precision", "assume_centered", "block_size"], "notes": "The regularised covariance is: where mu = trace(cov) / n_features and shrinkage is given by the Ledoit and Wolf formula (see References) A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices, Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411. The regularised covariance is: where mu = trace(cov) / n_features and shrinkage is given by the Ledoit and Wolf formula (see References) A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices, Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.", "funcName": "LedoitWolf", "allFuncAttributes": ["covariance_", "precision_", "shrinkage_"], "funcDesc": "LedoitWolf Estimator", "funcParamBody": "store_precision : bool, default=True Specify if the estimated precision is stored. assume_centered : bool, default=False If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data are centered before computation. block_size : int, default=1000 Size of the blocks into which the covariance matrix will be split during its Ledoit-Wolf estimation. This is purely a memory optimization and does not affect results.", "funcAttrBody": "covariance_ : array-like, shape (n_features, n_features) Estimated covariance matrix precision_ : array-like, shape (n_features, n_features) Estimated pseudo inverse matrix. (stored only if store_precision is True) shrinkage_ : float, 0 <= shrinkage <= 1 Coefficient in the convex combination used for the computation of the shrunk estimate."},
{"libName": "sklearn.covariance", "methods": [{"methodName": "__init__(alphas=4, n_refinements=4, cv=None, tol=0.0001, enet_tol=0.0001, max_iter=100, mode=cd, n_jobs=1, verbose=False, assume_centered=False)", "methodDesc": ""}, {"methodName": "error_norm(comp_cov, norm=frobenius, scaling=True, squared=True)", "methodReturnsBody": "The Mean Squared Error (in the sense of the Frobenius norm) between :  `self` and `comp_cov` covariance estimators. : ", "methodParams": ["comp_cov", "norm", "scaling", "squared"], "methodReturns": ["The Mean Squared Error (in the sense of the Frobenius norm) between", "`self` and `comp_cov` covariance estimators."], "methodDesc": "Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius norm).", "methodParamsBody": "comp_cov : array-like, shape = [n_features, n_features]   The covariance to compare with.   norm : str   The type of norm used to compute the error. Available error types: - frobenius (default): sqrt(tr(A^t.A)) - spectral: sqrt(max(eigenvalues(A^t.A)) where A is the error (comp_cov  -  self.covariance_) .   scaling : bool   If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled.   squared : bool   Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned.  "}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Data from which to compute the covariance estimate   y : (ignored) ", "methodDesc": "Fits the GraphLasso covariance model to X."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_precision()", "methodReturns": ["precision_"], "methodDesc": "Getter for the precision matrix.", "methodReturnsBody": "precision_ : array-like,   The precision matrix associated to the current covariance object.  "}, {"methodName": "mahalanobis(observations)", "methodReturnsBody": "mahalanobis_distance : array, shape = [n_observations,]   Squared Mahalanobis distances of the observations.  ", "methodParams": ["observations"], "methodReturns": ["mahalanobis_distance"], "methodDesc": "Computes the squared Mahalanobis distances of given observations.", "methodParamsBody": "observations : array-like, shape = [n_observations, n_features]   The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.  "}, {"methodName": "score(X_test, y=None)", "methodReturnsBody": "res : float   The likelihood of the data set with self.covariance_ as an estimator of its covariance matrix.  ", "methodParams": ["X_test", "y"], "methodReturns": ["res"], "methodDesc": "Computes the log-likelihood of a Gaussian data set with self.covariance_ as an estimator of its covariance matrix.", "methodParamsBody": "X_test : array-like, shape = [n_samples, n_features]   Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering).   y : not used, present for API consistence purpose. "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["alphas", "n_refinements", "cv", "tol", "enet_tol", "max_iter", "mode", "n_jobs", "verbose", "assume_centered"], "notes": "The search for the optimal penalization parameter (alpha) is done on an iteratively refined grid: first the cross-validated scores on a grid are computed, then a new refined grid is centered around the maximum, and so on. One of the challenges which is faced here is that the solvers can fail to converge to a well-conditioned estimate. The corresponding values of alpha then come out as missing values, but the optimum may be close to these missing values. The search for the optimal penalization parameter (alpha) is done on an iteratively refined grid: first the cross-validated scores on a grid are computed, then a new refined grid is centered around the maximum, and so on. One of the challenges which is faced here is that the solvers can fail to converge to a well-conditioned estimate. The corresponding values of alpha then come out as missing values, but the optimum may be close to these missing values.", "funcName": "GraphLassoCV", "allFuncAttributes": ["covariance_", "precision_", "alpha_", "cv_alphas_", "grid_scores_", "n_iter_"], "funcDesc": "Sparse inverse covariance w/ cross-validated choice of the l1 penalty", "funcParamBody": "alphas : integer, or list positive float, optional If an integer is given, it fixes the number of points on the grids of alpha to be used. If a list is given, it gives the grid to be used. See the notes in the class docstring for more details. n_refinements : strictly positive integer The number of times the grid is refined. Not used if explicit values of alphas are passed. cv : int, cross-validation generator or an iterable, optional Determines the cross-validation splitting strategy. Possible inputs for cv are: None, to use the default 3-fold cross-validation, integer, to specify the number of folds. An object to be used as a cross-validation generator. An iterable yielding train/test splits. For integer/None inputs KFold is used. Refer User Guide for the various cross-validation strategies that can be used here. tol : positive float, optional The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. enet_tol : positive float, optional The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=cd. max_iter : integer, optional Maximum number of iterations. mode : {cd, lars} The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where number of features is greater than number of samples. Elsewhere prefer cd which is more numerically stable. n_jobs : int, optional number of jobs to run in parallel (default 1). verbose : boolean, optional If verbose is True, the objective function and duality gap are printed at each iteration. assume_centered : Boolean If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation.", "funcAttrBody": "covariance_ : numpy.ndarray, shape (n_features, n_features) Estimated covariance matrix. precision_ : numpy.ndarray, shape (n_features, n_features) Estimated precision matrix (inverse covariance). alpha_ : float Penalization parameter selected. cv_alphas_ : list of float All penalization parameters explored. grid_scores_ : 2D numpy.ndarray (n_alphas, n_folds) Log-likelihood score on left-out data across folds. n_iter_ : int Number of iterations run for the optimal alpha."},
{"libName": "sklearn.covariance", "methods": [{"methodName": "__init__(alpha=0.01, mode=cd, tol=0.0001, enet_tol=0.0001, max_iter=100, verbose=False, assume_centered=False)", "methodDesc": ""}, {"methodName": "error_norm(comp_cov, norm=frobenius, scaling=True, squared=True)", "methodReturnsBody": "The Mean Squared Error (in the sense of the Frobenius norm) between :  `self` and `comp_cov` covariance estimators. : ", "methodParams": ["comp_cov", "norm", "scaling", "squared"], "methodReturns": ["The Mean Squared Error (in the sense of the Frobenius norm) between", "`self` and `comp_cov` covariance estimators."], "methodDesc": "Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius norm).", "methodParamsBody": "comp_cov : array-like, shape = [n_features, n_features]   The covariance to compare with.   norm : str   The type of norm used to compute the error. Available error types: - frobenius (default): sqrt(tr(A^t.A)) - spectral: sqrt(max(eigenvalues(A^t.A)) where A is the error (comp_cov  -  self.covariance_) .   scaling : bool   If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled.   squared : bool   Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned.  "}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Data from which to compute the covariance estimate   y : (ignored) ", "methodDesc": "Fits the GraphLasso model to X."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_precision()", "methodReturns": ["precision_"], "methodDesc": "Getter for the precision matrix.", "methodReturnsBody": "precision_ : array-like,   The precision matrix associated to the current covariance object.  "}, {"methodName": "mahalanobis(observations)", "methodReturnsBody": "mahalanobis_distance : array, shape = [n_observations,]   Squared Mahalanobis distances of the observations.  ", "methodParams": ["observations"], "methodReturns": ["mahalanobis_distance"], "methodDesc": "Computes the squared Mahalanobis distances of given observations.", "methodParamsBody": "observations : array-like, shape = [n_observations, n_features]   The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.  "}, {"methodName": "score(X_test, y=None)", "methodReturnsBody": "res : float   The likelihood of the data set with self.covariance_ as an estimator of its covariance matrix.  ", "methodParams": ["X_test", "y"], "methodReturns": ["res"], "methodDesc": "Computes the log-likelihood of a Gaussian data set with self.covariance_ as an estimator of its covariance matrix.", "methodParamsBody": "X_test : array-like, shape = [n_samples, n_features]   Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering).   y : not used, present for API consistence purpose. "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["alpha", "mode", "tol", "enet_tol", "max_iter", "verbose", "assume_centered"], "notes": "", "funcName": "GraphLasso", "allFuncAttributes": ["covariance_", "precision_", "n_iter_"], "funcDesc": "Sparse inverse covariance estimation with an l1-penalized estimator.", "funcParamBody": "alpha : positive float, default 0.01 The regularization parameter: the higher alpha, the more regularization, the sparser the inverse covariance. mode : {cd, lars}, default cd The Lasso solver to use: coordinate descent or LARS. Use LARS for very sparse underlying graphs, where p > n. Elsewhere prefer cd which is more numerically stable. tol : positive float, default 1e-4 The tolerance to declare convergence: if the dual gap goes below this value, iterations are stopped. enet_tol : positive float, optional The tolerance for the elastic net solver used to calculate the descent direction. This parameter controls the accuracy of the search direction for a given column update, not of the overall parameter estimate. Only used for mode=cd. max_iter : integer, default 100 The maximum number of iterations. verbose : boolean, default False If verbose is True, the objective function and dual gap are plotted at each iteration. assume_centered : boolean, default False If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False, data are centered before computation.", "funcAttrBody": "covariance_ : array-like, shape (n_features, n_features) Estimated covariance matrix precision_ : array-like, shape (n_features, n_features) Estimated pseudo inverse matrix. n_iter_ : int Number of iterations run."},
{"funcName": "EllipticEnvelope", "notes": "Outlier detection from covariance estimation may break or not perform well in high-dimensional settings. In particular, one will always take care to work with n_samples  >  n_features  **  2 . Outlier detection from covariance estimation may break or not perform well in high-dimensional settings. In particular, one will always take care to work with n_samples  >  n_features  **  2 .", "libName": "sklearn.covariance", "methods": [{"methodName": "__init__(store_precision=True, assume_centered=False, support_fraction=None, contamination=0.1, random_state=None)", "methodDesc": ""}, {"methodName": "correct_covariance(data)", "methodReturnsBody": "covariance_corrected : array-like, shape (n_features, n_features)   Corrected robust covariance estimate.  ", "methodParams": ["data"], "methodReturns": ["covariance_corrected"], "methodDesc": "Apply a correction to raw Minimum Covariance Determinant estimates. Correction using the empirical correction factor suggested by Rousseeuw and Van Driessen in [RVD] . References", "methodParamsBody": "data : array-like, shape (n_samples, n_features)   The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates.  "}, {"methodName": "decision_function(X, raw_values=False)", "methodReturnsBody": "decision : array-like, shape (n_samples, )   Decision function of the samples. It is equal to the Mahalanobis distances if raw_values is True. By default ( raw_values=False ), it is equal to the cubic root of the shifted Mahalanobis distances. In that case, the threshold for being an outlier is 0, which ensures a compatibility with other outlier detection tools such as the One-Class SVM.  ", "methodParams": ["X", "raw_values"], "methodReturns": ["decision"], "methodDesc": "Compute the decision function of the given observations.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)  raw_values : bool   Whether or not to consider raw Mahalanobis distances as the decision function. Must be False (default) for compatibility with the others outlier detection tools.  "}, {"methodName": "error_norm(comp_cov, norm=frobenius, scaling=True, squared=True)", "methodReturnsBody": "The Mean Squared Error (in the sense of the Frobenius norm) between :  `self` and `comp_cov` covariance estimators. : ", "methodParams": ["comp_cov", "norm", "scaling", "squared"], "methodReturns": ["The Mean Squared Error (in the sense of the Frobenius norm) between", "`self` and `comp_cov` covariance estimators."], "methodDesc": "Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius norm).", "methodParamsBody": "comp_cov : array-like, shape = [n_features, n_features]   The covariance to compare with.   norm : str   The type of norm used to compute the error. Available error types: - frobenius (default): sqrt(tr(A^t.A)) - spectral: sqrt(max(eigenvalues(A^t.A)) where A is the error (comp_cov  -  self.covariance_) .   scaling : bool   If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled.   squared : bool   Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned.  "}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : numpy array or sparse matrix of shape [n_samples, n_features]   Training data   y : (ignored) ", "methodDesc": "Fit the EllipticEnvelope model with X."}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_precision()", "methodReturns": ["precision_"], "methodDesc": "Getter for the precision matrix.", "methodReturnsBody": "precision_ : array-like,   The precision matrix associated to the current covariance object.  "}, {"methodName": "mahalanobis(observations)", "methodReturnsBody": "mahalanobis_distance : array, shape = [n_observations,]   Squared Mahalanobis distances of the observations.  ", "methodParams": ["observations"], "methodReturns": ["mahalanobis_distance"], "methodDesc": "Computes the squared Mahalanobis distances of given observations.", "methodParamsBody": "observations : array-like, shape = [n_observations, n_features]   The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.  "}, {"methodName": "predict(X)", "methodReturnsBody": "is_outliers : array, shape = (n_samples, ), dtype = bool   For each observation, tells whether or not it should be considered as an outlier according to the fitted model.   threshold : float,   The values of the less outlying points decision function.  ", "methodParams": ["X"], "methodReturns": ["is_outliers", "threshold"], "methodDesc": "Outlyingness of observations in X according to the fitted model.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features) "}, {"methodName": "reweight_covariance(data)", "methodReturnsBody": "location_reweighted : array-like, shape (n_features, )   Re-weighted robust location estimate.   covariance_reweighted : array-like, shape (n_features, n_features)   Re-weighted robust covariance estimate.   support_reweighted : array-like, type boolean, shape (n_samples,)   A mask of the observations that have been used to compute the re-weighted robust location and covariance estimates.  ", "methodParams": ["data"], "methodReturns": ["location_reweighted", "covariance_reweighted", "support_reweighted"], "methodDesc": "Re-weight raw Minimum Covariance Determinant estimates. Re-weight observations using Rousseeuws method (equivalent to deleting outlying observations from the data set before computing location and covariance estimates) described in [RVDriessen] . References", "methodParamsBody": "data : array-like, shape (n_samples, n_features)   The data matrix, with p features and n samples. The data set must be the one which was used to compute the raw estimates.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples,) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = (n_samples,), optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "An object for detecting outliers in a Gaussian distributed dataset."},
{"libName": "sklearn.covariance", "methods": [{"methodName": "__init__(store_precision=True, assume_centered=False)", "methodDesc": ""}, {"methodName": "error_norm(comp_cov, norm=frobenius, scaling=True, squared=True)", "methodReturnsBody": "The Mean Squared Error (in the sense of the Frobenius norm) between :  `self` and `comp_cov` covariance estimators. : ", "methodParams": ["comp_cov", "norm", "scaling", "squared"], "methodReturns": ["The Mean Squared Error (in the sense of the Frobenius norm) between", "`self` and `comp_cov` covariance estimators."], "methodDesc": "Computes the Mean Squared Error between two covariance estimators. (In the sense of the Frobenius norm).", "methodParamsBody": "comp_cov : array-like, shape = [n_features, n_features]   The covariance to compare with.   norm : str   The type of norm used to compute the error. Available error types: - frobenius (default): sqrt(tr(A^t.A)) - spectral: sqrt(max(eigenvalues(A^t.A)) where A is the error (comp_cov  -  self.covariance_) .   scaling : bool   If True (default), the squared error norm is divided by n_features. If False, the squared error norm is not rescaled.   squared : bool   Whether to compute the squared error norm or the error norm. If True (default), the squared error norm is returned. If False, the error norm is returned.  "}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : object   Returns self.  ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fits the Maximum Likelihood Estimator covariance model according to the given training data and parameters.", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Training data, where n_samples is the number of samples and n_features is the number of features.   y : not used, present for API consistence purpose. "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_precision()", "methodReturns": ["precision_"], "methodDesc": "Getter for the precision matrix.", "methodReturnsBody": "precision_ : array-like,   The precision matrix associated to the current covariance object.  "}, {"methodName": "mahalanobis(observations)", "methodReturnsBody": "mahalanobis_distance : array, shape = [n_observations,]   Squared Mahalanobis distances of the observations.  ", "methodParams": ["observations"], "methodReturns": ["mahalanobis_distance"], "methodDesc": "Computes the squared Mahalanobis distances of given observations.", "methodParamsBody": "observations : array-like, shape = [n_observations, n_features]   The observations, the Mahalanobis distances of the which we compute. Observations are assumed to be drawn from the same distribution than the data used in fit.  "}, {"methodName": "score(X_test, y=None)", "methodReturnsBody": "res : float   The likelihood of the data set with self.covariance_ as an estimator of its covariance matrix.  ", "methodParams": ["X_test", "y"], "methodReturns": ["res"], "methodDesc": "Computes the log-likelihood of a Gaussian data set with self.covariance_ as an estimator of its covariance matrix.", "methodParamsBody": "X_test : array-like, shape = [n_samples, n_features]   Test data of which we compute the likelihood, where n_samples is the number of samples and n_features is the number of features. X_test is assumed to be drawn from the same distribution than the data used in fit (including centering).   y : not used, present for API consistence purpose. "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["store_precision", "assume_centered"], "notes": "", "funcName": "EmpiricalCovariance", "allFuncAttributes": ["covariance_", "precision_"], "funcDesc": "Maximum likelihood covariance estimator", "funcParamBody": "store_precision : bool Specifies if the estimated precision is stored. assume_centered : bool If True, data are not centered before computation. Useful when working with data whose mean is almost, but not exactly zero. If False (default), data are centered before computation.", "funcAttrBody": "covariance_ : 2D ndarray, shape (n_features, n_features) Estimated covariance matrix precision_ : 2D ndarray, shape (n_features, n_features) Estimated pseudo-inverse matrix. (stored only if store_precision is True)"},
{"libName": "sklearn.cluster.bicluster", "methods": [{"methodName": "__init__(n_clusters=3, svd_method=randomized, n_svd_vecs=None, mini_batch=False, init=k-means++, n_init=10, n_jobs=1, random_state=None)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like, shape (n_samples, n_features)  y : Ignored ", "methodDesc": "Creates a biclustering for X."}, {"methodName": "get_indices(i)", "methodReturnsBody": "row_ind : np.array, dtype=np.intp   Indices of rows in the dataset that belong to the bicluster.   col_ind : np.array, dtype=np.intp   Indices of columns in the dataset that belong to the bicluster.  ", "methodParams": ["i"], "methodReturns": ["row_ind", "col_ind"], "methodDesc": "Row and column indices of the ith bicluster. Only works if rows_ and columns_ attributes exist.", "methodParamsBody": "i : int   The index of the cluster.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_shape(i)", "methodReturnsBody": "shape : (int, int)   Number of rows and columns (resp.) in the bicluster.  ", "methodParams": ["i"], "methodReturns": ["shape"], "methodDesc": "Shape of the ith bicluster.", "methodParamsBody": "i : int   The index of the cluster.  "}, {"methodName": "get_submatrix(i, data)", "methodReturnsBody": "submatrix : array   The submatrix corresponding to bicluster i.  ", "methodParams": ["i", "data"], "methodReturns": ["submatrix"], "methodDesc": "Returns the submatrix corresponding to bicluster i . Notes Works with sparse matrices. Only works if rows_ and columns_ attributes exist.", "methodParamsBody": "i : int   The index of the cluster.   data : array   The data.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["n_clusters", "svd_method", "n_svd_vecs", "mini_batch", "init", "n_init", "n_jobs", "random_state"], "notes": "Works with sparse matrices. Only works if rows_ and columns_ attributes exist.", "funcName": "SpectralCoclustering", "allFuncAttributes": ["rows_", "columns_", "row_labels_", "column_labels_"], "funcDesc": "Spectral Co-Clustering algorithm (Dhillon, 2001).", "funcParamBody": "n_clusters : integer, optional, default: 3 The number of biclusters to find. svd_method : string, optional, default: randomized Selects the algorithm for finding singular vectors. May be randomized or arpack. If randomized, use sklearn.utils.extmath.randomized_svd , which may be faster for large matrices. If arpack, use scipy.sparse.linalg.svds , which is more accurate, but possibly slower in some cases. n_svd_vecs : int, optional, default: None Number of vectors to use in calculating the SVD. Corresponds to ncv when svd_method=arpack and n_oversamples when svd_method is randomized`. mini_batch : bool, optional, default: False Whether to use mini-batch k-means, which is faster but may get different results. init : {k-means++, random or an ndarray} Method for initialization of k-means algorithm; defaults to k-means++. n_init : int, optional, default: 10 Number of random initializations that are tried with the k-means algorithm. If mini-batch k-means is used, the best initialization is chosen and the algorithm runs once. Otherwise, the algorithm is run for each initialization and the best solution chosen. n_jobs : int, optional, default: 1 The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcAttrBody": "rows_ : array-like, shape (n_row_clusters, n_rows) Results of the clustering. rows[i, r] is True if cluster i contains row r . Available only after calling fit . columns_ : array-like, shape (n_column_clusters, n_columns) Results of the clustering, like rows . row_labels_ : array-like, shape (n_rows,) The bicluster label of each row. column_labels_ : array-like, shape (n_cols,) The bicluster label of each column."},
{"libName": "sklearn.cluster.bicluster", "methods": [{"methodName": "__init__(n_clusters=3, method=bistochastic, n_components=6, n_best=3, svd_method=randomized, n_svd_vecs=None, mini_batch=False, init=k-means++, n_init=10, n_jobs=1, random_state=None)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like, shape (n_samples, n_features)  y : Ignored ", "methodDesc": "Creates a biclustering for X."}, {"methodName": "get_indices(i)", "methodReturnsBody": "row_ind : np.array, dtype=np.intp   Indices of rows in the dataset that belong to the bicluster.   col_ind : np.array, dtype=np.intp   Indices of columns in the dataset that belong to the bicluster.  ", "methodParams": ["i"], "methodReturns": ["row_ind", "col_ind"], "methodDesc": "Row and column indices of the ith bicluster. Only works if rows_ and columns_ attributes exist.", "methodParamsBody": "i : int   The index of the cluster.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "get_shape(i)", "methodReturnsBody": "shape : (int, int)   Number of rows and columns (resp.) in the bicluster.  ", "methodParams": ["i"], "methodReturns": ["shape"], "methodDesc": "Shape of the ith bicluster.", "methodParamsBody": "i : int   The index of the cluster.  "}, {"methodName": "get_submatrix(i, data)", "methodReturnsBody": "submatrix : array   The submatrix corresponding to bicluster i.  ", "methodParams": ["i", "data"], "methodReturns": ["submatrix"], "methodDesc": "Returns the submatrix corresponding to bicluster i . Notes Works with sparse matrices. Only works if rows_ and columns_ attributes exist.", "methodParamsBody": "i : int   The index of the cluster.   data : array   The data.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["n_clusters", "method", "n_components", "n_best", "svd_method", "n_svd_vecs", "mini_batch", "init", "n_init", "n_jobs", "random_state"], "notes": "Works with sparse matrices. Only works if rows_ and columns_ attributes exist.", "funcName": "SpectralBiclustering", "allFuncAttributes": ["rows_", "columns_", "row_labels_", "column_labels_"], "funcDesc": "Spectral biclustering (Kluger, 2003).", "funcParamBody": "n_clusters : integer or tuple (n_row_clusters, n_column_clusters) The number of row and column clusters in the checkerboard structure. method : string, optional, default: bistochastic Method of normalizing and converting singular vectors into biclusters. May be one of scale, bistochastic, or log. The authors recommend using log. If the data is sparse, however, log normalization will not work, which is why the default is bistochastic. CAUTION: if method=log , the data must not be sparse. n_components : integer, optional, default: 6 Number of singular vectors to check. n_best : integer, optional, default: 3 Number of best singular vectors to which to project the data for clustering. svd_method : string, optional, default: randomized Selects the algorithm for finding singular vectors. May be randomized or arpack. If randomized, uses sklearn.utils.extmath.randomized_svd , which may be faster for large matrices. If arpack, uses scipy.sparse.linalg.svds , which is more accurate, but possibly slower in some cases. n_svd_vecs : int, optional, default: None Number of vectors to use in calculating the SVD. Corresponds to ncv when svd_method=arpack and n_oversamples when svd_method is randomized`. mini_batch : bool, optional, default: False Whether to use mini-batch k-means, which is faster but may get different results. init : {k-means++, random or an ndarray} Method for initialization of k-means algorithm; defaults to k-means++. n_init : int, optional, default: 10 Number of random initializations that are tried with the k-means algorithm. If mini-batch k-means is used, the best initialization is chosen and the algorithm runs once. Otherwise, the algorithm is run for each initialization and the best solution chosen. n_jobs : int, optional, default: 1 The number of jobs to use for the computation. This works by breaking down the pairwise matrix into n_jobs even slices and computing them in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random .", "funcAttrBody": "rows_ : array-like, shape (n_row_clusters, n_rows) Results of the clustering. rows[i, r] is True if cluster i contains row r . Available only after calling fit . columns_ : array-like, shape (n_column_clusters, n_columns) Results of the clustering, like rows . row_labels_ : array-like, shape (n_rows,) Row partition labels. column_labels_ : array-like, shape (n_cols,) Column partition labels."},
{"allReturnParams": ["children", "n_components", "n_leaves", "parents", "distances"], "libName": "sklearn.cluster", "methods": [], "notes": "", "funcName": "ward_tree", "allFuncParams": ["X", "connectivity", "n_clusters", "return_distance"], "funcDesc": "Ward clustering based on a Feature matrix.", "funcParamBody": "X : array, shape (n_samples, n_features) feature matrix  representing n_samples samples to be clustered connectivity : sparse matrix (optional). connectivity matrix. Defines for each sample the neighboring samples following a given structure of the data. The matrix is assumed to be symmetric and only the upper triangular half is used. Default is None, i.e, the Ward algorithm is unstructured. n_clusters : int (optional) Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. In this case, the complete tree is not computed, thus the children output is of limited use, and the parents output should rather be used. This option is valid only when specifying a connectivity matrix. return_distance : bool (optional) If True, return the distance between the clusters.", "funcReturnBody": "children : 2D array, shape (n_nodes-1, 2) The children of each non-leaf node. Values less than n_samples i greater than or equal to n_samples is a non-leaf node and has children children_[i - n_samples] . Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i n_components : int The number of connected components in the graph. n_leaves : int The number of leaves in the tree parents : 1D array, shape (n_nodes, ) or None The parent of each node. Only returned when a connectivity matrix is specified, elsewhere None is returned. distances : 1D array, shape (n_nodes-1, ) Only returned if return_distance is set to True (for compatibility). The distances between the centers of the nodes. distances[i] children[i, 1] and children[i, 2] . If the nodes refer to leaves of the tree, then distances[i] is their unweighted euclidean distance. Distances are updated in the following way (from scipy.hierarchy.linkage): The new entry is computed as follows, where is the newly joined cluster consisting of clusters and , is an unused cluster in the forest, , and is the cardinality of its argument. This is also known as the incremental algorithm."},
{"allReturnParams": ["labels"], "libName": "sklearn.cluster", "methods": [], "notes": "The graph should contain only one connect component, elsewhere the results make little sense. This algorithm solves the normalized cut for k=2: it is a normalized spectral clustering. The graph should contain only one connect component, elsewhere the results make little sense. This algorithm solves the normalized cut for k=2: it is a normalized spectral clustering.", "funcName": "spectral_clustering", "allFuncParams": ["affinity", "Must be symmetric", "n_clusters", "n_components", "eigen_solver", "random_state", "n_init", "eigen_tol", "assign_labels"], "funcDesc": "Apply clustering to a projection to the normalized laplacian.", "funcParamBody": "affinity : array-like or sparse matrix, shape: (n_samples, n_samples) The affinity matrix describing the relationship of the samples to embed. Must be symmetric . Possible examples: adjacency matrix of a graph, heat kernel of the pairwise distance matrix of the samples, symmetric k-nearest neighbours connectivity matrix of the samples. n_clusters : integer, optional Number of clusters to extract. n_components : integer, optional, default is n_clusters Number of eigen vectors to use for the spectral embedding eigen_solver : {None, arpack, lobpcg, or amg} The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities random_state : int, RandomState instance or None, optional, default: None A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == amg and by the K-Means initialization. If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_init : int, optional, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. eigen_tol : float, optional, default: 0.0 Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack eigen_solver. assign_labels : {kmeans, discretize}, default: kmeans The strategy to use to assign labels in the embedding space.  There are two ways to assign labels after the laplacian embedding.  k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization. See the Multiclass spectral clustering paper referenced below for more details on the discretization approach.", "funcReturnBody": "labels : array of integers, shape: n_samples The labels of the clusters."},
{"allReturnParams": ["cluster_centers", "labels"], "libName": "sklearn.cluster", "methods": [], "notes": "For an example, see examples/cluster/plot_mean_shift.py .", "funcName": "mean_shift", "allFuncParams": ["X", "bandwidth", "seeds", "bin_seeding", "min_bin_freq", "cluster_all", "max_iter", "n_jobs"], "funcDesc": "Perform mean shift clustering of data using a flat kernel.", "funcParamBody": "X : array-like, shape=[n_samples, n_features] Input data. bandwidth : float, optional Kernel bandwidth. If bandwidth is not given, it is determined using a heuristic based on the median of all pairwise distances. This will take quadratic time in the number of samples. The sklearn.cluster.estimate_bandwidth function can be used to do this more efficiently. seeds : array-like, shape=[n_seeds, n_features] or None Point used as initial kernel locations. If None and bin_seeding=False, each data point is used as a seed. If None and bin_seeding=True, see bin_seeding. bin_seeding : boolean, default=False If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. Ignored if seeds argument is not None. min_bin_freq : int, default=1 To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds. cluster_all : boolean, default True If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1. max_iter : int, default 300 Maximum number of iterations, per seed point before the clustering operation terminates (for that seed point), if has not converged yet. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. New in version 0.17: Parallel Execution using n_jobs .", "funcReturnBody": "cluster_centers : array, shape=[n_clusters, n_features] Coordinates of cluster centers. labels : array, shape=[n_samples] Cluster labels for each point."},
{"allReturnParams": ["centroid", "label", "inertia", "best_n_iter"], "libName": "sklearn.cluster", "methods": [], "notes": "", "funcName": "k_means", "allFuncParams": ["X", "n_clusters", "init", "precompute_distances", "n_init", "max_iter", "verbose", "tol", "random_state", "copy_x", "n_jobs", "algorithm", "return_n_iter"], "funcDesc": "K-means clustering algorithm.", "funcParamBody": "X : array-like or sparse matrix, shape (n_samples, n_features) The observations to cluster. n_clusters : int The number of clusters to form as well as the number of centroids to generate. init : {k-means++, random, or ndarray, or a callable}, optional Method for initialization, default to k-means++: k-means++ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. random: generate k centroids from a Gaussian with mean and variance estimated from the data. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. If a callable is passed, it should take arguments X, k and and a random state and return an initialization. precompute_distances : {auto, True, False} Precompute distances (faster but takes more memory). auto : do not precompute distances if n_samples * n_clusters > 12 million. This corresponds to about 100MB overhead per job using double precision. True : always precompute distances False : never precompute distances n_init : int, optional, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. max_iter : int, optional, default 300 Maximum number of iterations of the k-means algorithm to run. verbose : boolean, optional Verbosity mode. tol : float, optional The relative increment in the results before declaring convergence. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . copy_x : boolean, optional When pre-computing distances it is more numerically accurate to center the data first.  If copy_x is True, then the original data is not modified.  If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. algorithm : auto, full or elkan, default=auto K-means algorithm to use. The classical EM-style algorithm is full. The elkan variation is more efficient by using the triangle inequality, but currently doesnt support sparse data. auto chooses elkan for dense data and full for sparse data. return_n_iter : bool, optional Whether or not to return the number of iterations.", "funcReturnBody": "centroid : float ndarray with shape (k, n_features) Centroids found at the last iteration of k-means. label : integer ndarray with shape (n_samples,) label[i] is the code or index of the centroid the ith observation is closest to. inertia : float The final value of the inertia criterion (sum of squared distances to the closest centroid for all observations in the training set). best_n_iter : int Number of iterations corresponding to the best results. Returned only if return_n_iter is set to True."},
{"allReturnParams": ["bandwidth"], "libName": "sklearn.cluster", "methods": [], "notes": "", "funcName": "estimate_bandwidth", "allFuncParams": ["X", "quantile", "n_samples", "random_state", "n_jobs"], "funcDesc": "Estimate the bandwidth to use with the mean-shift algorithm.", "funcParamBody": "X : array-like, shape=[n_samples, n_features] Input points. quantile : float, default 0.3 should be between [0, 1] 0.5 means that the median of all pairwise distances is used. n_samples : int, optional The number of samples to use. If not given, all samples are used. random_state : int, RandomState instance or None, optional (default=None) If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_jobs : int, optional (default = 1) The number of parallel jobs to run for neighbors search. If -1 , then the number of jobs is set to the number of CPU cores.", "funcReturnBody": "bandwidth : float The bandwidth parameter."},
{"allReturnParams": ["core_samples", "labels"], "libName": "sklearn.cluster", "methods": [], "notes": "For an example, see examples/cluster/plot_dbscan.py . This implementation bulk-computes all neighborhood queries, which increases the memory complexity to O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n). Sparse neighborhoods can be precomputed using NearestNeighbors.radius_neighbors_graph with mode='distance' . Ester, M., H. P. Kriegel, J. Sander, and X. Xu, A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In: Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996", "funcName": "dbscan", "allFuncParams": ["X", "eps", "min_samples", "metric", "metric_params", "algorithm", "leaf_size", "p", "sample_weight", "n_jobs"], "funcDesc": "Perform DBSCAN clustering from vector array or distance matrix.", "funcParamBody": "X : array or sparse (CSR) matrix of shape (n_samples, n_features), or             array of shape (n_samples, n_samples) A feature array, or array of distances between samples if metric='precomputed' . eps : float, optional The maximum distance between two samples for them to be considered as in the same neighborhood. min_samples : int, optional The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself. metric : string, or callable The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.pairwise_distances for its metric parameter. If metric is precomputed, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only nonzero elements may be considered neighbors for DBSCAN. metric_params : dict, optional Additional keyword arguments for the metric function. New in version 0.19. algorithm : {auto, ball_tree, kd_tree, brute}, optional The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details. leaf_size : int, optional (default = 30) Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. p : float, optional The power of the Minkowski metric to be used to calculate distance between points. sample_weight : array, shape (n_samples,), optional Weight of each sample, such that a sample with a weight of at least min_samples is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor from being core. Note that weights are absolute, and default to 1. n_jobs : int, optional (default = 1) The number of parallel jobs to run for neighbors search. If -1 , then the number of jobs is set to the number of CPU cores.", "funcReturnBody": "core_samples : array [n_core_samples] Indices of core samples. labels : array [n_samples] Cluster labels for each point.  Noisy samples are given the label -1."},
{"allReturnParams": ["cluster_centers_indices", "labels", "n_iter"], "libName": "sklearn.cluster", "methods": [], "notes": "For an example, see examples/cluster/plot_affinity_propagation.py . Brendan J. Frey and Delbert Dueck, Clustering by Passing Messages Between Data Points, Science Feb. 2007 For an example, see examples/cluster/plot_affinity_propagation.py . Brendan J. Frey and Delbert Dueck, Clustering by Passing Messages Between Data Points, Science Feb. 2007", "funcName": "affinity_propagation", "allFuncParams": ["S", "preference", "convergence_iter", "max_iter", "damping", "copy", "verbose", "return_n_iter"], "funcDesc": "Perform Affinity Propagation Clustering of data", "funcParamBody": "S : array-like, shape (n_samples, n_samples) Matrix of similarities between points preference : array-like, shape (n_samples,) or float, optional Preferences for each point - points with larger values of preferences are more likely to be chosen as exemplars. The number of exemplars, i.e. of clusters, is influenced by the input preferences value. If the preferences are not passed as arguments, they will be set to the median of the input similarities (resulting in a moderate number of clusters). For a smaller amount of clusters, this can be set to the minimum value of the similarities. convergence_iter : int, optional, default: 15 Number of iterations with no change in the number of estimated clusters that stops the convergence. max_iter : int, optional, default: 200 Maximum number of iterations damping : float, optional, default: 0.5 Damping factor between 0.5 and 1. copy : boolean, optional, default: True If copy is False, the affinity matrix is modified inplace by the algorithm, for memory efficiency verbose : boolean, optional, default: False The verbosity level return_n_iter : bool, default False Whether or not to return the number of iterations.", "funcReturnBody": "cluster_centers_indices : array, shape (n_clusters,) index of clusters centers labels : array, shape (n_samples,) cluster labels for each point n_iter : int number of iterations run. Returned only if return_n_iter is set to True."},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(n_clusters=8, eigen_solver=None, random_state=None, n_init=10, gamma=1.0, affinity=rbf, n_neighbors=10, eigen_tol=0.0, assign_labels=kmeans, degree=3, coef0=1, kernel_params=None, n_jobs=1)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like or sparse matrix, shape (n_samples, n_features)   OR, if affinity==`precomputed`, a precomputed affinity matrix of shape (n_samples, n_samples)   y : Ignored ", "methodDesc": "Creates an affinity matrix for X using the selected affinity, then applies spectral clustering to this affinity matrix."}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "y : ndarray, shape (n_samples,)   cluster labels  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Performs clustering on X and returns cluster labels.", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Input data.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["n_clusters", "eigen_solver", "random_state", "n_init", "gamma", "affinity", "n_neighbors", "eigen_tol", "assign_labels", "degree", "coef0", "kernel_params", "n_jobs"], "notes": "If you have an affinity matrix, such as a distance matrix, for which 0 means identical elements, and high values means very dissimilar elements, it can be transformed in a similarity matrix that is well suited for the algorithm by applying the Gaussian (RBF, heat) kernel: Where delta is a free parameter representing the width of the Gaussian kernel. Another alternative is to take a symmetric version of the k nearest neighbors connectivity matrix of the points. If the pyamg package is installed, it is used: this greatly speeds up computation. If you have an affinity matrix, such as a distance matrix, for which 0 means identical elements, and high values means very dissimilar elements, it can be transformed in a similarity matrix that is well suited for the algorithm by applying the Gaussian (RBF, heat) kernel: Where delta is a free parameter representing the width of the Gaussian kernel. Another alternative is to take a symmetric version of the k nearest neighbors connectivity matrix of the points. If the pyamg package is installed, it is used: this greatly speeds up computation.", "funcName": "SpectralClustering", "allFuncAttributes": ["affinity_matrix_", "labels_ :"], "funcDesc": "Apply clustering to a projection to the normalized laplacian.", "funcParamBody": "n_clusters : integer, optional The dimension of the projection subspace. eigen_solver : {None, arpack, lobpcg, or amg} The eigenvalue decomposition strategy to use. AMG requires pyamg to be installed. It can be faster on very large, sparse problems, but may also lead to instabilities random_state : int, RandomState instance or None, optional, default: None A pseudo random number generator used for the initialization of the lobpcg eigen vectors decomposition when eigen_solver == amg and by the K-Means initialization.  If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . n_init : int, optional, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. gamma : float, default=1.0 Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels. Ignored for affinity='nearest_neighbors' . affinity : string, array-like or callable, default rbf If a string, this may be one of nearest_neighbors, precomputed, rbf or one of the kernels supported by sklearn.metrics.pairwise_kernels . Only kernels that produce similarity scores (non-negative values that increase with similarity) should be used. This property is not checked by the clustering algorithm. n_neighbors : integer Number of neighbors to use when constructing the affinity matrix using the nearest neighbors method. Ignored for affinity='rbf' . eigen_tol : float, optional, default: 0.0 Stopping criterion for eigendecomposition of the Laplacian matrix when using arpack eigen_solver. assign_labels : {kmeans, discretize}, default: kmeans The strategy to use to assign labels in the embedding space. There are two ways to assign labels after the laplacian embedding. k-means can be applied and is a popular choice. But it can also be sensitive to initialization. Discretization is another approach which is less sensitive to random initialization. degree : float, default=3 Degree of the polynomial kernel. Ignored by other kernels. coef0 : float, default=1 Zero coefficient for polynomial and sigmoid kernels. Ignored by other kernels. kernel_params : dictionary of string to any, optional Parameters (keyword arguments) and values for kernel passed as callable object. Ignored by other kernels. n_jobs : int, optional (default = 1) The number of parallel jobs to run. If -1 , then the number of jobs is set to the number of CPU cores.", "funcAttrBody": "affinity_matrix_ : array-like, shape (n_samples, n_samples) Affinity matrix used for clustering. Available only if after calling fit . labels_ : : Labels of each point"},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=1)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like, shape=[n_samples, n_features]   Samples to cluster.   y : Ignored ", "methodDesc": "Perform clustering."}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "y : ndarray, shape (n_samples,)   cluster labels  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Performs clustering on X and returns cluster labels.", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Input data.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "labels : array, shape [n_samples,]   Index of the cluster each sample belongs to.  ", "methodParams": ["X"], "methodReturns": ["labels"], "methodDesc": "Predict the closest cluster each sample in X belongs to.", "methodParamsBody": "X : {array-like, sparse matrix}, shape=[n_samples, n_features]   New data to predict.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["bandwidth", "seeds", "bin_seeding", "min_bin_freq", "cluster_all", "n_jobs"], "notes": "Scalability: Because this implementation uses a flat kernel and a Ball Tree to look up members of each kernel, the complexity will tend towards O(T*n*log(n)) in lower dimensions, with n the number of samples and T the number of points. In higher dimensions the complexity will tend towards O(T*n^2). Scalability can be boosted by using fewer seeds, for example by using a higher value of min_bin_freq in the get_bin_seeds function. Note that the estimate_bandwidth function is much less scalable than the mean shift algorithm and will be the bottleneck if it is used. Dorin Comaniciu and Peter Meer, Mean Shift: A robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2002. pp. 603-619. Scalability: Because this implementation uses a flat kernel and a Ball Tree to look up members of each kernel, the complexity will tend towards O(T*n*log(n)) in lower dimensions, with n the number of samples and T the number of points. In higher dimensions the complexity will tend towards O(T*n^2). Scalability can be boosted by using fewer seeds, for example by using a higher value of min_bin_freq in the get_bin_seeds function. Note that the estimate_bandwidth function is much less scalable than the mean shift algorithm and will be the bottleneck if it is used. Dorin Comaniciu and Peter Meer, Mean Shift: A robust approach toward feature space analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence. 2002. pp. 603-619.", "funcName": "MeanShift", "allFuncAttributes": ["cluster_centers_", "labels_ :"], "funcDesc": "Mean shift clustering using a flat kernel.", "funcParamBody": "bandwidth : float, optional Bandwidth used in the RBF kernel. If not given, the bandwidth is estimated using sklearn.cluster.estimate_bandwidth; see the documentation for that function for hints on scalability (see also the Notes, below). seeds : array, shape=[n_samples, n_features], optional Seeds used to initialize kernels. If not set, the seeds are calculated by clustering.get_bin_seeds with bandwidth as the grid size and default values for other parameters. bin_seeding : boolean, optional If true, initial kernel locations are not locations of all points, but rather the location of the discretized version of points, where points are binned onto a grid whose coarseness corresponds to the bandwidth. Setting this option to True will speed up the algorithm because fewer seeds will be initialized. default value: False Ignored if seeds argument is not None. min_bin_freq : int, optional To speed up the algorithm, accept only those bins with at least min_bin_freq points as seeds. If not defined, set to 1. cluster_all : boolean, default True If true, then all points are clustered, even those orphans that are not within any kernel. Orphans are assigned to the nearest kernel. If false, then orphans are given cluster label -1. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used.", "funcAttrBody": "cluster_centers_ : array, [n_clusters, n_features] Coordinates of cluster centers. labels_ : : Labels of each point."},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(n_clusters=8, init=k-means++, max_iter=100, batch_size=100, verbose=0, compute_labels=True, random_state=None, tol=0.0, max_no_improvement=10, init_size=None, n_init=3, reassignment_ratio=0.01)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like or sparse matrix, shape=(n_samples, n_features)   Training instances to cluster.   y : Ignored ", "methodDesc": "Compute the centroids on X by chunking it into mini-batches."}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "labels : array, shape [n_samples,]   Index of the cluster each sample belongs to.  ", "methodParams": ["X", "u"], "methodReturns": ["labels"], "methodDesc": "Compute cluster centers and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X).", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to transform.   u : Ignored "}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "X_new : array, shape [n_samples, k]   X transformed in the new space.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Compute clustering and transform X to cluster-distance space. Equivalent to fit(X).transform(X), but more efficiently implemented.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to transform.   y : Ignored "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodParams": ["X", "y"], "methodName": "partial_fit(X, y=None)", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   Coordinates of the data points to cluster.   y : Ignored ", "methodDesc": "Update k means estimate on a single mini-batch X."}, {"methodName": "predict(X)", "methodReturnsBody": "labels : array, shape [n_samples,]   Index of the cluster each sample belongs to.  ", "methodParams": ["X"], "methodReturns": ["labels"], "methodDesc": "Predict the closest cluster each sample in X belongs to. In the vector quantization literature, cluster_centers_ is called the code book and each value returned by predict is the index of the closest code in the code book.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to predict.  "}, {"methodName": "score(X, y=None)", "methodReturnsBody": "score : float   Opposite of the value of X on the K-means objective.  ", "methodParams": ["X", "y"], "methodReturns": ["score"], "methodDesc": "Opposite of the value of X on the K-means objective.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data.   y : Ignored "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array, shape [n_samples, k]   X transformed in the new space.  ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Transform X to a cluster-distance space. In the new space, each dimension is the distance to the cluster centers.  Note that even if X is sparse, the array returned by transform will typically be dense.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to transform.  "}], "allFuncParams": ["n_clusters", "init", "max_iter", "batch_size", "verbose", "compute_labels", "random_state", "tol", "max_no_improvement", "init_size", "n_init", "reassignment_ratio"], "notes": "See http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf See http://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf", "funcName": "MiniBatchKMeans", "allFuncAttributes": ["cluster_centers_", "labels_ :", "inertia_"], "funcDesc": "Mini-Batch K-Means clustering", "funcParamBody": "n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. init : {k-means++, random or an ndarray}, default: k-means++ Method for initialization, defaults to k-means++: k-means++ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. random: choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. max_iter : int, optional Maximum number of iterations over the complete dataset before stopping independently of any early stopping criterion heuristics. batch_size : int, optional, default: 100 Size of the mini batches. verbose : boolean, optional Verbosity mode. compute_labels : boolean, default=True Compute label assignment and inertia for the complete dataset once the minibatch optimization has converged in fit. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . tol : float, default: 0.0 Control early stopping based on the relative center changes as measured by a smoothed, variance-normalized of the mean center squared position changes. This early stopping heuristics is closer to the one used for the batch variant of the algorithms but induces a slight computational and memory overhead over the inertia heuristic. To disable convergence detection based on normalized center change, set tol to 0.0 (default). max_no_improvement : int, default: 10 Control early stopping based on the consecutive number of mini batches that does not yield an improvement on the smoothed inertia. To disable convergence detection based on inertia, set max_no_improvement to None. init_size : int, optional, default: 3 * batch_size Number of samples to randomly sample for speeding up the initialization (sometimes at the expense of accuracy): the only algorithm is initialized by running a batch KMeans on a random subset of the data. This needs to be larger than n_clusters. n_init : int, default=3 Number of random initializations that are tried. In contrast to KMeans, the algorithm is only run once, using the best of the n_init initializations as measured by inertia. reassignment_ratio : float, default: 0.01 Control the fraction of the maximum number of counts for a center to be reassigned. A higher value means that low count centers are more easily reassigned, which means that the model will take longer to converge, but should converge in a better clustering.", "funcAttrBody": "cluster_centers_ : array, [n_clusters, n_features] Coordinates of cluster centers labels_ : : Labels of each point (if compute_labels is set to True). inertia_ : float The value of the inertia criterion associated with the chosen partition (if compute_labels is set to True). The inertia is defined as the sum of square distances of samples to their nearest neighbor."},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(n_clusters=8, init=k-means++, n_init=10, max_iter=300, tol=0.0001, precompute_distances=auto, verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm=auto)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like or sparse matrix, shape=(n_samples, n_features)   Training instances to cluster.   y : Ignored ", "methodDesc": "Compute k-means clustering."}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "labels : array, shape [n_samples,]   Index of the cluster each sample belongs to.  ", "methodParams": ["X", "u"], "methodReturns": ["labels"], "methodDesc": "Compute cluster centers and predict cluster index for each sample. Convenience method; equivalent to calling fit(X) followed by predict(X).", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to transform.   u : Ignored "}, {"methodName": "fit_transform(X, y=None)", "methodReturnsBody": "X_new : array, shape [n_samples, k]   X transformed in the new space.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Compute clustering and transform X to cluster-distance space. Equivalent to fit(X).transform(X), but more efficiently implemented.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to transform.   y : Ignored "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "labels : array, shape [n_samples,]   Index of the cluster each sample belongs to.  ", "methodParams": ["X"], "methodReturns": ["labels"], "methodDesc": "Predict the closest cluster each sample in X belongs to. In the vector quantization literature, cluster_centers_ is called the code book and each value returned by predict is the index of the closest code in the code book.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to predict.  "}, {"methodName": "score(X, y=None)", "methodReturnsBody": "score : float   Opposite of the value of X on the K-means objective.  ", "methodParams": ["X", "y"], "methodReturns": ["score"], "methodDesc": "Opposite of the value of X on the K-means objective.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data.   y : Ignored "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_new : array, shape [n_samples, k]   X transformed in the new space.  ", "methodParams": ["X"], "methodReturns": ["X_new"], "methodDesc": "Transform X to a cluster-distance space. In the new space, each dimension is the distance to the cluster centers.  Note that even if X is sparse, the array returned by transform will typically be dense.", "methodParamsBody": "X : {array-like, sparse matrix}, shape = [n_samples, n_features]   New data to transform.  "}], "allFuncParams": ["n_clusters", "init", "n_init", "max_iter", "tol", "precompute_distances", "verbose", "random_state", "copy_x", "n_jobs", "algorithm"], "notes": "The k-means problem is solved using Lloyds algorithm. The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration. The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, How slow is the k-means method? SoCG2006) In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. Thats why it can be useful to restart it several times. The k-means problem is solved using Lloyds algorithm. The average complexity is given by O(k n T), were n is the number of samples and T is the number of iteration. The worst case complexity is given by O(n^(k+2/p)) with n = n_samples, p = n_features. (D. Arthur and S. Vassilvitskii, How slow is the k-means method? SoCG2006) In practice, the k-means algorithm is very fast (one of the fastest clustering algorithms available), but it falls in local minima. Thats why it can be useful to restart it several times.", "funcName": "KMeans", "allFuncAttributes": ["cluster_centers_", "labels_ :", "inertia_"], "funcDesc": "K-Means clustering", "funcParamBody": "n_clusters : int, optional, default: 8 The number of clusters to form as well as the number of centroids to generate. init : {k-means++, random or an ndarray} Method for initialization, defaults to k-means++: k-means++ : selects initial cluster centers for k-mean clustering in a smart way to speed up convergence. See section Notes in k_init for more details. random: choose k observations (rows) at random from data for the initial centroids. If an ndarray is passed, it should be of shape (n_clusters, n_features) and gives the initial centers. n_init : int, default: 10 Number of time the k-means algorithm will be run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. max_iter : int, default: 300 Maximum number of iterations of the k-means algorithm for a single run. tol : float, default: 1e-4 Relative tolerance with regards to inertia to declare convergence precompute_distances : {auto, True, False} Precompute distances (faster but takes more memory). auto : do not precompute distances if n_samples * n_clusters > 12 million. This corresponds to about 100MB overhead per job using double precision. True : always precompute distances False : never precompute distances verbose : int, default 0 Verbosity mode. random_state : int, RandomState instance or None, optional, default: None If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random . copy_x : boolean, default True When pre-computing distances it is more numerically accurate to center the data first.  If copy_x is True, then the original data is not modified.  If False, the original data is modified, and put back before the function returns, but small numerical differences may be introduced by subtracting and then adding the data mean. n_jobs : int The number of jobs to use for the computation. This works by computing each of the n_init runs in parallel. If -1 all CPUs are used. If 1 is given, no parallel computing code is used at all, which is useful for debugging. For n_jobs below -1, (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one are used. algorithm : auto, full or elkan, default=auto K-means algorithm to use. The classical EM-style algorithm is full. The elkan variation is more efficient by using the triangle inequality, but currently doesnt support sparse data. auto chooses elkan for dense data and full for sparse data.", "funcAttrBody": "cluster_centers_ : array, [n_clusters, n_features] Coordinates of cluster centers labels_ : : Labels of each point inertia_ : float Sum of squared distances of samples to their closest cluster center."},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(n_clusters=2, affinity=euclidean, memory=None, connectivity=None, compute_full_tree=auto, linkage=ward, pooling_func=<function mean>)", "methodDesc": ""}, {"methodName": "fit(X, y=None, **params)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the hierarchical clustering on the data", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   The data   y : Ignored "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "inverse_transform(Xred)", "methodReturnsBody": "X : array, shape=[n_samples, n_features] or [n_features]   A vector of size n_samples with the values of Xred assigned to each of the cluster of samples.  ", "methodParams": ["Xred"], "methodReturns": ["X"], "methodDesc": "Inverse the transformation. Return a vector of size nb_features with the values of Xred assigned to each group of features", "methodParamsBody": "Xred : array-like, shape=[n_samples, n_clusters] or [n_clusters,]   The values to be assigned to each cluster of samples  "}, {"methodName": "pooling_func(a, axis=None, dtype=None, out=None, keepdims=<class numpy._globals._NoValue>)", "methodReturnsBody": "m : ndarray, see dtype parameter above   If out=None , returns a new array containing the mean values, otherwise a reference to the output array is returned.  ", "methodParams": ["a", "axis", "dtype", "out", "keepdims"], "methodReturns": ["m"], "methodDesc": "Compute the arithmetic mean along the specified axis. Returns the average of the array elements.  The average is taken over the flattened array by default, otherwise over the specified axis. float64 intermediate and return values are used for integer inputs. Notes The arithmetic mean is the sum of the elements along the axis divided by the number of elements. Note that for floating-point input, the mean is computed using the same precision the input has.  Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below).  Specifying a higher-precision accumulator using the dtype keyword can alleviate this issue. By default, float16 results are computed using float32 intermediates for extra precision. Examples In single precision, mean can be inaccurate: Computing the mean in float64 is more accurate:", "methodParamsBody": "a : array_like   Array containing numbers whose mean is desired. If a is not an array, a conversion is attempted.   axis : None or int or tuple of ints, optional   Axis or axes along which the means are computed. The default is to compute the mean of the flattened array.   New in version 1.7.0.   If this is a tuple of ints, a mean is performed over multiple axes, instead of a single axis or all the axes as before.   dtype : data-type, optional   Type to use in computing the mean.  For integer inputs, the default is float64 ; for floating point inputs, it is the same as the input dtype.   out : ndarray, optional   Alternate output array in which to place the result.  The default is None ; if provided, it must have the same shape as the expected output, but the type will be cast if necessary. See doc.ufuncs for details.   keepdims : bool, optional   If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.  If the default value is passed, then keepdims will not be passed through to the mean method of sub-classes of ndarray , however any non-default value will be.  If the sub-classes sum method does not implement keepdims any exceptions will be raised.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "Y : array, shape = [n_samples, n_clusters] or [n_clusters]   The pooled values for each feature cluster.  ", "methodParams": ["X"], "methodReturns": ["Y"], "methodDesc": "Transform a new matrix using the built clustering", "methodParamsBody": "X : array-like, shape = [n_samples, n_features] or [n_features]   A M by N array of M observations in N dimensions or a length M array of M one-dimensional observations.  "}], "allFuncParams": ["n_clusters", "affinity", "memory", "connectivity", "compute_full_tree", "linkage", "pooling_func"], "notes": "The arithmetic mean is the sum of the elements along the axis divided by the number of elements. Note that for floating-point input, the mean is computed using the same precision the input has.  Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below).  Specifying a higher-precision accumulator using the dtype keyword can alleviate this issue. By default, float16 results are computed using float32 intermediates for extra precision. In single precision, mean can be inaccurate: Computing the mean in float64 is more accurate: The arithmetic mean is the sum of the elements along the axis divided by the number of elements. Note that for floating-point input, the mean is computed using the same precision the input has.  Depending on the input data, this can cause the results to be inaccurate, especially for float32 (see example below).  Specifying a higher-precision accumulator using the dtype keyword can alleviate this issue. By default, float16 results are computed using float32 intermediates for extra precision. In single precision, mean can be inaccurate: Computing the mean in float64 is more accurate:", "funcName": "FeatureAgglomeration", "allFuncAttributes": ["labels_", "n_leaves_", "n_components_", "children_"], "funcDesc": "Agglomerate features.", "funcParamBody": "n_clusters : int, default 2 The number of clusters to find. affinity : string or callable, default euclidean Metric used to compute the linkage. Can be euclidean, l1, l2, manhattan, cosine, or precomputed. If linkage is ward, only euclidean is accepted. memory : None, str or object with the joblib.Memory interface, optional Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory. connectivity : array-like or callable, optional Connectivity matrix. Defines for each feature the neighboring features following a given structure of the data. This can be a connectivity matrix itself or a callable that transforms the data into a connectivity matrix, such as derived from kneighbors_graph. Default is None, i.e, the hierarchical clustering algorithm is unstructured. compute_full_tree : bool or auto, optional, default auto Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of features. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree. linkage : {ward, complete, average}, optional, default ward Which linkage criterion to use. The linkage criterion determines which distance to use between sets of features. The algorithm will merge the pairs of cluster that minimize this criterion. ward minimizes the variance of the clusters being merged. average uses the average of the distances of each feature of the two sets. complete or maximum linkage uses the maximum distances between all features of the two sets. pooling_func : callable, default np.mean This combines the values of agglomerated features into a single value, and should accept an array of shape [M, N] and the keyword argument axis=1 , and reduce it to an array of size [M].", "funcAttrBody": "labels_ : array-like, (n_features,) cluster labels for each feature. n_leaves_ : int Number of leaves in the hierarchical tree. n_components_ : int The estimated number of connected components in the graph. children_ : array-like, shape (n_nodes-1, 2) The children of each non-leaf node. Values less than n_features i greater than or equal to n_features is a non-leaf node and has children children_[i - n_features] . Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node n_features + i"},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(eps=0.5, min_samples=5, metric=euclidean, metric_params=None, algorithm=auto, leaf_size=30, p=None, n_jobs=1)", "methodDesc": ""}, {"methodParams": ["X", "sample_weight", "y"], "methodName": "fit(X, y=None, sample_weight=None)", "methodParamsBody": "X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)   A feature array, or array of distances between samples if metric='precomputed' .   sample_weight : array, shape (n_samples,), optional   Weight of each sample, such that a sample with a weight of at least min_samples is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor from being core. Note that weights are absolute, and default to 1.   y : Ignored ", "methodDesc": "Perform DBSCAN clustering from features or distance matrix."}, {"methodName": "fit_predict(X, y=None, sample_weight=None)", "methodReturnsBody": "y : ndarray, shape (n_samples,)   cluster labels  ", "methodParams": ["X", "sample_weight", "y"], "methodReturns": ["y"], "methodDesc": "Performs clustering on X and returns cluster labels.", "methodParamsBody": "X : array or sparse (CSR) matrix of shape (n_samples, n_features), or                 array of shape (n_samples, n_samples)   A feature array, or array of distances between samples if metric='precomputed' .   sample_weight : array, shape (n_samples,), optional   Weight of each sample, such that a sample with a weight of at least min_samples is by itself a core sample; a sample with negative weight may inhibit its eps-neighbor from being core. Note that weights are absolute, and default to 1.   y : Ignored "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["eps", "min_samples", "metric", "metric_params", "algorithm", "leaf_size", "p", "n_jobs"], "notes": "For an example, see examples/cluster/plot_dbscan.py . This implementation bulk-computes all neighborhood queries, which increases the memory complexity to O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n). Sparse neighborhoods can be precomputed using NearestNeighbors.radius_neighbors_graph with mode='distance' . Ester, M., H. P. Kriegel, J. Sander, and X. Xu, A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In: Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996 For an example, see examples/cluster/plot_dbscan.py . This implementation bulk-computes all neighborhood queries, which increases the memory complexity to O(n.d) where d is the average number of neighbors, while original DBSCAN had memory complexity O(n). Sparse neighborhoods can be precomputed using NearestNeighbors.radius_neighbors_graph with mode='distance' . Ester, M., H. P. Kriegel, J. Sander, and X. Xu, A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise. In: Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226-231. 1996", "funcName": "DBSCAN", "allFuncAttributes": ["core_sample_indices_", "components_", "labels_"], "funcDesc": "Perform DBSCAN clustering from vector array or distance matrix.", "funcParamBody": "eps : float, optional The maximum distance between two samples for them to be considered as in the same neighborhood. min_samples : int, optional The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself. metric : string, or callable The metric to use when calculating distance between instances in a feature array. If metric is a string or callable, it must be one of the options allowed by metrics.pairwise.calculate_distance for its metric parameter. If metric is precomputed, X is assumed to be a distance matrix and must be square. X may be a sparse matrix, in which case only nonzero elements may be considered neighbors for DBSCAN. New in version 0.17: metric precomputed to accept precomputed sparse matrix. metric_params : dict, optional Additional keyword arguments for the metric function. New in version 0.19. algorithm : {auto, ball_tree, kd_tree, brute}, optional The algorithm to be used by the NearestNeighbors module to compute pointwise distances and find nearest neighbors. See NearestNeighbors module documentation for details. leaf_size : int, optional (default = 30) Leaf size passed to BallTree or cKDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem. p : float, optional The power of the Minkowski metric to be used to calculate distance between points. n_jobs : int, optional (default = 1) The number of parallel jobs to run. If -1 , then the number of jobs is set to the number of CPU cores.", "funcAttrBody": "core_sample_indices_ : array, shape = [n_core_samples] Indices of core samples. components_ : array, shape = [n_core_samples, n_features] Copy of each core sample found by training. labels_ : array, shape = [n_samples] Cluster labels for each point in the dataset given to fit(). Noisy samples are given the label -1."},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(threshold=0.5, branching_factor=50, n_clusters=3, compute_labels=True, copy=True)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Input data.   y : Ignored ", "methodDesc": "Build a CF Tree for the input data."}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "y : ndarray, shape (n_samples,)   cluster labels  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Performs clustering on X and returns cluster labels.", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Input data.  "}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodParams": ["X", "y"], "methodName": "partial_fit(X=None, y=None)", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features), None   Input data. If X is not provided, only the global clustering step is done.   y : Ignored ", "methodDesc": "Online learning. Prevents rebuilding of CFTree from scratch."}, {"methodName": "predict(X)", "methodReturnsBody": "labels : ndarray, shape(n_samples)   Labelled data.  ", "methodParams": ["X"], "methodReturns": ["labels"], "methodDesc": "Predict data using the centroids_ of subclusters. Avoid computation of the row norms of X.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Input data.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}, {"methodName": "transform(X)", "methodReturnsBody": "X_trans : {array-like, sparse matrix}, shape (n_samples, n_clusters)   Transformed data.  ", "methodParams": ["X"], "methodReturns": ["X_trans"], "methodDesc": "Transform X into subcluster centroids dimension. Each dimension represents the distance from the sample point to each cluster centroid.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   Input data.  "}], "allFuncParams": ["threshold", "branching_factor", "n_clusters", "compute_labels", "copy"], "notes": "The tree data structure consists of nodes with each node consisting of a number of subclusters. The maximum number of subclusters in a node is determined by the branching factor. Each subcluster maintains a linear sum, squared sum and the number of samples in that subcluster. In addition, each subcluster can also have a node as its child, if the subcluster is not a member of a leaf node. For a new point entering the root, it is merged with the subcluster closest to it and the linear sum, squared sum and the number of samples of that subcluster are updated. This is done recursively till the properties of the leaf node are updated. The tree data structure consists of nodes with each node consisting of a number of subclusters. The maximum number of subclusters in a node is determined by the branching factor. Each subcluster maintains a linear sum, squared sum and the number of samples in that subcluster. In addition, each subcluster can also have a node as its child, if the subcluster is not a member of a leaf node. For a new point entering the root, it is merged with the subcluster closest to it and the linear sum, squared sum and the number of samples of that subcluster are updated. This is done recursively till the properties of the leaf node are updated.", "funcName": "Birch", "allFuncAttributes": ["root_", "dummy_leaf_", "subcluster_centers_", "subcluster_labels_", "labels_"], "funcDesc": "Implements the Birch clustering algorithm.", "funcParamBody": "threshold : float, default 0.5 The radius of the subcluster obtained by merging a new sample and the closest subcluster should be lesser than the threshold. Otherwise a new subcluster is started. Setting this value to be very low promotes splitting and vice-versa. branching_factor : int, default 50 Maximum number of CF subclusters in each node. If a new samples enters such that the number of subclusters exceed the branching_factor then that node is split into two nodes with the subclusters redistributed in each. The parent subcluster of that node is removed and two new subclusters are added as parents of the 2 split nodes. n_clusters : int, instance of sklearn.cluster model, default 3 Number of clusters after the final clustering step, which treats the subclusters from the leaves as new samples. None : the final clustering step is not performed and the subclusters are returned as they are. sklearn.cluster Estimator : If a model is provided, the model is fit treating the subclusters as new samples and the initial data is mapped to the label of the closest subcluster. int : the model fit is AgglomerativeClustering with n_clusters set to be equal to the int. compute_labels : bool, default True Whether or not to compute labels for each fit. copy : bool, default True Whether or not to make a copy of the given data. If set to False, the initial data will be overwritten.", "funcAttrBody": "root_ : _CFNode Root of the CFTree. dummy_leaf_ : _CFNode Start pointer to all the leaves. subcluster_centers_ : ndarray, Centroids of all subclusters read directly from the leaves. subcluster_labels_ : ndarray, Labels assigned to the centroids of the subclusters after they are clustered globally. labels_ : ndarray, shape (n_samples,) Array of labels assigned to the input data. if partial_fit is used instead of fit, they are assigned to the last batch of data."},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(n_clusters=2, affinity=euclidean, memory=None, connectivity=None, compute_full_tree=auto, linkage=ward, pooling_func=<function mean>)", "methodDesc": ""}, {"methodName": "fit(X, y=None)", "methodReturnsBody": "self : ", "methodParams": ["X", "y"], "methodReturns": ["self"], "methodDesc": "Fit the hierarchical clustering on the data", "methodParamsBody": "X : array-like, shape = [n_samples, n_features]   The samples a.k.a. observations.   y : Ignored "}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "y : ndarray, shape (n_samples,)   cluster labels  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Performs clustering on X and returns cluster labels.", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Input data.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["n_clusters", "affinity", "memory", "connectivity", "compute_full_tree", "linkage", "pooling_func"], "notes": "", "funcName": "AgglomerativeClustering", "allFuncAttributes": ["labels_", "n_leaves_", "n_components_", "children_"], "funcDesc": "Agglomerative Clustering", "funcParamBody": "n_clusters : int, default=2 The number of clusters to find. affinity : string or callable, default: euclidean Metric used to compute the linkage. Can be euclidean, l1, l2, manhattan, cosine, or precomputed. If linkage is ward, only euclidean is accepted. memory : None, str or object with the joblib.Memory interface, optional Used to cache the output of the computation of the tree. By default, no caching is done. If a string is given, it is the path to the caching directory. connectivity : array-like or callable, optional Connectivity matrix. Defines for each sample the neighboring samples following a given structure of the data. This can be a connectivity matrix itself or a callable that transforms the data into a connectivity matrix, such as derived from kneighbors_graph. Default is None, i.e, the hierarchical clustering algorithm is unstructured. compute_full_tree : bool or auto (optional) Stop early the construction of the tree at n_clusters. This is useful to decrease computation time if the number of clusters is not small compared to the number of samples. This option is useful only when specifying a connectivity matrix. Note also that when varying the number of clusters and using caching, it may be advantageous to compute the full tree. linkage : {ward, complete, average}, optional, default: ward Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion. ward minimizes the variance of the clusters being merged. average uses the average of the distances of each observation of the two sets. complete or maximum linkage uses the maximum distances between all observations of the two sets. pooling_func : callable, default=np.mean This combines the values of agglomerated features into a single value, and should accept an array of shape [M, N] and the keyword argument axis=1 , and reduce it to an array of size [M].", "funcAttrBody": "labels_ : array [n_samples] cluster labels for each point n_leaves_ : int Number of leaves in the hierarchical tree. n_components_ : int The estimated number of connected components in the graph. children_ : array-like, shape (n_nodes-1, 2) The children of each non-leaf node. Values less than n_samples i greater than or equal to n_samples is a non-leaf node and has children children_[i - n_samples] . Alternatively at the i-th iteration, children[i][0] and children[i][1] are merged to form node n_samples + i"},
{"libName": "sklearn.cluster", "methods": [{"methodName": "__init__(damping=0.5, max_iter=200, convergence_iter=15, copy=True, preference=None, affinity=euclidean, verbose=False)", "methodDesc": ""}, {"methodParams": ["X", "y"], "methodName": "fit(X, y=None)", "methodParamsBody": "X : array-like, shape (n_samples, n_features) or (n_samples, n_samples)   Data matrix or, if affinity is precomputed , matrix of similarities / affinities.   y : Ignored ", "methodDesc": "Create affinity matrix from negative euclidean distances, then apply affinity propagation clustering."}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "y : ndarray, shape (n_samples,)   cluster labels  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Performs clustering on X and returns cluster labels.", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Input data.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "labels : array, shape (n_samples,)   Index of the cluster each sample belongs to.  ", "methodParams": ["X"], "methodReturns": ["labels"], "methodDesc": "Predict the closest cluster each sample in X belongs to.", "methodParamsBody": "X : {array-like, sparse matrix}, shape (n_samples, n_features)   New data to predict.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "allFuncParams": ["damping", "max_iter", "convergence_iter", "copy", "preference", "affinity", "verbose"], "notes": "For an example, see examples/cluster/plot_affinity_propagation.py . The algorithmic complexity of affinity propagation is quadratic in the number of points. Brendan J. Frey and Delbert Dueck, Clustering by Passing Messages Between Data Points, Science Feb. 2007 For an example, see examples/cluster/plot_affinity_propagation.py . The algorithmic complexity of affinity propagation is quadratic in the number of points. Brendan J. Frey and Delbert Dueck, Clustering by Passing Messages Between Data Points, Science Feb. 2007", "funcName": "AffinityPropagation", "allFuncAttributes": ["cluster_centers_indices_", "cluster_centers_", "labels_", "affinity_matrix_", "n_iter_"], "funcDesc": "Perform Affinity Propagation Clustering of data.", "funcParamBody": "damping : float, optional, default: 0.5 Damping factor (between 0.5 and 1) is the extent to which the current value is maintained relative to incoming values (weighted 1 - damping). This in order to avoid numerical oscillations when updating these values (messages). max_iter : int, optional, default: 200 Maximum number of iterations. convergence_iter : int, optional, default: 15 Number of iterations with no change in the number of estimated clusters that stops the convergence. copy : boolean, optional, default: True Make a copy of input data. preference : array-like, shape (n_samples,) or float, optional Preferences for each point - points with larger values of preferences are more likely to be chosen as exemplars. The number of exemplars, ie of clusters, is influenced by the input preferences value. If the preferences are not passed as arguments, they will be set to the median of the input similarities. affinity : string, optional, default=``euclidean`` Which affinity to use. At the moment precomputed and euclidean are supported. euclidean uses the negative squared euclidean distance between points. verbose : boolean, optional, default: False Whether to be verbose.", "funcAttrBody": "cluster_centers_indices_ : array, shape (n_clusters,) Indices of cluster centers cluster_centers_ : array, shape (n_clusters, n_features) Cluster centers (if affinity != precomputed ). labels_ : array, shape (n_samples,) Labels of each point affinity_matrix_ : array, shape (n_samples, n_samples) Stores the affinity matrix used in fit . n_iter_ : int Number of iterations taken to converge."},
{"allReturnParams": ["prob_true", "prob_pred"], "libName": "sklearn.calibration", "methods": [], "notes": "", "funcName": "calibration_curve", "allFuncParams": ["y_true", "y_prob", "normalize", "n_bins"], "funcDesc": "Compute true and predicted probabilities for a calibration curve.", "funcParamBody": "y_true : array, shape (n_samples,) True targets. y_prob : array, shape (n_samples,) Probabilities of the positive class. normalize : bool, optional, default=False Whether y_prob needs to be normalized into the bin [0, 1], i.e. is not a proper probability. If True, the smallest value in y_prob is mapped onto 0 and the largest one onto 1. n_bins : int Number of bins. A bigger number requires more data.", "funcReturnBody": "prob_true : array, shape (n_bins,) The true probability in each bin (fraction of positives). prob_pred : array, shape (n_bins,) The mean predicted probability in each bin."},
{"funcName": "CalibratedClassifierCV", "notes": "", "libName": "sklearn.calibration", "methods": [{"methodName": "__init__(base_estimator=None, method=sigmoid, cv=3)", "methodDesc": ""}, {"methodName": "fit(X, y, sample_weight=None)", "methodReturnsBody": "self : object   Returns an instance of self.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["self"], "methodDesc": "Fit the calibrated model", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   Training data.   y : array-like, shape (n_samples,)   Target values.   sample_weight : array-like, shape = [n_samples] or None   Sample weights. If None, then samples are equally weighted.  "}, {"methodName": "get_params(deep=True)", "methodReturnsBody": "params : mapping of string to any   Parameter names mapped to their values.  ", "methodParams": ["deep"], "methodReturns": ["params"], "methodDesc": "Get parameters for this estimator.", "methodParamsBody": "deep : boolean, optional   If True, will return the parameters for this estimator and contained subobjects that are estimators.  "}, {"methodName": "predict(X)", "methodReturnsBody": "C : array, shape (n_samples,)   The predicted class.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Predict the target of new samples. Can be different from the prediction of the uncalibrated classifier.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   The samples.  "}, {"methodName": "predict_proba(X)", "methodReturnsBody": "C : array, shape (n_samples, n_classes)   The predicted probas.  ", "methodParams": ["X"], "methodReturns": ["C"], "methodDesc": "Posterior probabilities of classification This function returns posterior probabilities of classification according to each class on an array of test vectors X.", "methodParamsBody": "X : array-like, shape (n_samples, n_features)   The samples.  "}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}, {"methodName": "set_params(**params)", "methodReturns": [], "methodDesc": "Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as pipelines). The latter have parameters of the form <component>__<parameter> so that its possible to update each component of a nested object.", "methodReturnsBody": "self :"}], "funcDesc": "Probability calibration with isotonic regression or sigmoid."},
{"libName": "sklearn", "methods": [], "notes": "", "funcName": "set_config", "allFuncParams": ["assume_finite"], "funcDesc": "Set global scikit-learn configuration", "funcParamBody": "assume_finite : bool, optional If True, validation for finiteness will be skipped, saving time, but leading to potential crashes. If False, validation for finiteness will be performed, avoiding error."},
{"allReturnParams": ["config"], "libName": "sklearn", "methods": [], "notes": "", "funcName": "get_config", "funcDesc": "Retrieve current values for configuration set by ", "funcReturnBody": "config : dict Keys are parameter names that can be passed to set_config ."},
{"libName": "sklearn", "methods": [], "notes": "All settings, not just those presently modified, will be returned to their previous values when the context manager is exited. This is not thread-safe.", "funcName": "config_context", "allFuncParams": ["assume_finite"], "funcDesc": "Context manager for global scikit-learn configuration", "funcParamBody": "assume_finite : bool, optional If True, validation for finiteness will be skipped, saving time, but leading to potential crashes. If False, validation for finiteness will be performed, avoiding error."},
{"libName": "sklearn.base", "methods": [], "notes": "", "funcName": "clone", "allFuncParams": ["estimator", "safe"], "funcDesc": "Constructs a new estimator with the same parameters.", "funcParamBody": "estimator : estimator object, or list, tuple or set of objects The estimator or group of estimators to be cloned safe : boolean, optional If safe is false, clone will fall back to a deep copy on objects that are not estimators."},
{"funcName": "TransformerMixin", "notes": "", "libName": "sklearn.base", "methods": [{"methodName": "__init__()", "methodDesc": "Initialize self.  See help(type(self)) for accurate signature."}, {"methodName": "fit_transform(X, y=None, **fit_params)", "methodReturnsBody": "X_new : numpy array of shape [n_samples, n_features_new]   Transformed array.  ", "methodParams": ["X", "y"], "methodReturns": ["X_new"], "methodDesc": "Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X.", "methodParamsBody": "X : numpy array of shape [n_samples, n_features]   Training set.   y : numpy array of shape [n_samples]   Target values.  "}], "funcDesc": "Mixin class for all transformers in scikit-learn."},
{"funcName": "RegressorMixin", "notes": "", "libName": "sklearn.base", "methods": [{"methodName": "__init__()", "methodDesc": "Initialize self.  See help(type(self)) for accurate signature."}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   R^2 of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the coefficient of determination R^2 of the prediction. The coefficient R^2 is defined as (1 - u/v), where u is the residual sum of squares ((y_true - y_pred) ** 2).sum() and v is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True values for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}], "funcDesc": "Mixin class for all regression estimators in scikit-learn."},
{"funcName": "DensityMixin", "notes": "", "libName": "sklearn.base", "methods": [{"methodName": "__init__()", "methodDesc": "Initialize self.  See help(type(self)) for accurate signature."}, {"methodName": "score(X, y=None)", "methodReturnsBody": "score : float", "methodParams": [], "methodReturns": [], "methodDesc": "Returns the score of the model on the data X", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)"}], "funcDesc": "Mixin class for all density estimators in scikit-learn."},
{"funcName": "ClusterMixin", "notes": "", "libName": "sklearn.base", "methods": [{"methodName": "__init__()", "methodDesc": "Initialize self.  See help(type(self)) for accurate signature."}, {"methodName": "fit_predict(X, y=None)", "methodReturnsBody": "y : ndarray, shape (n_samples,)   cluster labels  ", "methodParams": ["X"], "methodReturns": ["y"], "methodDesc": "Performs clustering on X and returns cluster labels.", "methodParamsBody": "X : ndarray, shape (n_samples, n_features)   Input data.  "}], "funcDesc": "Mixin class for all cluster estimators in scikit-learn."},
{"funcName": "ClassifierMixin", "notes": "", "libName": "sklearn.base", "methods": [{"methodName": "__init__()", "methodDesc": "Initialize self.  See help(type(self)) for accurate signature."}, {"methodName": "score(X, y, sample_weight=None)", "methodReturnsBody": "score : float   Mean accuracy of self.predict(X) wrt. y.  ", "methodParams": ["X", "y", "sample_weight"], "methodReturns": ["score"], "methodDesc": "Returns the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.", "methodParamsBody": "X : array-like, shape = (n_samples, n_features)   Test samples.   y : array-like, shape = (n_samples) or (n_samples, n_outputs)   True labels for X.   sample_weight : array-like, shape = [n_samples], optional   Sample weights.  "}], "funcDesc": "Mixin class for all classifiers in scikit-learn."},
{"funcName": "BiclusterMixin", "notes": "Works with sparse matrices. Only works if rows_ and columns_ attributes exist.", "libName": "sklearn.base", "methods": [{"methodName": "__init__()", "methodDesc": "Initialize self.  See help(type(self)) for accurate signature."}, {"methodName": "get_indices(i)", "methodReturnsBody": "row_ind : np.array, dtype=np.intp   Indices of rows in the dataset that belong to the bicluster.   col_ind : np.array, dtype=np.intp   Indices of columns in the dataset that belong to the bicluster.  ", "methodParams": ["i"], "methodReturns": ["row_ind", "col_ind"], "methodDesc": "Row and column indices of the ith bicluster. Only works if rows_ and columns_ attributes exist.", "methodParamsBody": "i : int   The index of the cluster.  "}, {"methodName": "get_shape(i)", "methodReturnsBody": "shape : (int, int)   Number of rows and columns (resp.) in the bicluster.  ", "methodParams": ["i"], "methodReturns": ["shape"], "methodDesc": "Shape of the ith bicluster.", "methodParamsBody": "i : int   The index of the cluster.  "}, {"methodName": "get_submatrix(i, data)", "methodReturnsBody": "submatrix : array   The submatrix corresponding to bicluster i.  ", "methodParams": ["i", "data"], "methodReturns": ["submatrix"], "methodDesc": "Returns the submatrix corresponding to bicluster i . Notes Works with sparse matrices. Only works if rows_ and columns_ attributes exist.", "methodParamsBody": "i : int   The index of the cluster.   data : array   The data.  "}], "funcDesc": "Mixin class for all bicluster estimators in scikit-learn"}
]